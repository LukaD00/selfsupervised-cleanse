{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from datasets import prepare_poison_dataset\n",
    "from simclr import SimClrBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "epochs = 250\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"sig0\"\n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "CHECKPOINT_NAME = f\"{DATASET}-SimCLR-NEW.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poison dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "poison_dataset, poison_indices, target_class, dataset = prepare_poison_dataset(DATASET, train=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimCLR data augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrastiveDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, s: int = 0.5):\n",
    "        self.original_dataset = original_dataset\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomResizedCrop(32,(0.8,1.0),antialias=False),\n",
    "            transforms.Compose([transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)], p = 0.8),\n",
    "                transforms.RandomGrayscale(p=0.2)]),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = self.original_dataset[index][0]\n",
    "        augmented_img_1 = self.transforms(img)\n",
    "        augmented_img_2 = self.transforms(img)\n",
    "        return augmented_img_1, augmented_img_2\n",
    "    \n",
    "constrastive_dataset = ConstrastiveDataset(poison_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimCLR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Luka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = SimClrBackbone()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LARS optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "import re\n",
    "\n",
    "EETA_DEFAULT = 0.001\n",
    "\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    \"\"\"\n",
    "    Layer-wise Adaptive Rate Scaling for large batch training.\n",
    "    Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
    "    I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=required,\n",
    "        momentum=0.9,\n",
    "        use_nesterov=False,\n",
    "        weight_decay=0.0,\n",
    "        exclude_from_weight_decay=None,\n",
    "        exclude_from_layer_adaptation=None,\n",
    "        classic_momentum=True,\n",
    "        eeta=EETA_DEFAULT,\n",
    "    ):\n",
    "        \"\"\"Constructs a LARSOptimizer.\n",
    "        Args:\n",
    "        lr: A `float` for learning rate.\n",
    "        momentum: A `float` for momentum.\n",
    "        use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
    "        weight_decay: A `float` for weight decay.\n",
    "        exclude_from_weight_decay: A list of `string` for variable screening, if\n",
    "            any of the string appears in a variable's name, the variable will be\n",
    "            excluded for computing weight decay. For example, one could specify\n",
    "            the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
    "            from weight decay.\n",
    "        exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
    "            for layer adaptation. If it is None, it will be defaulted the same as\n",
    "            exclude_from_weight_decay.\n",
    "        classic_momentum: A `boolean` for whether to use classic (or popular)\n",
    "            momentum. The learning rate is applied during momeuntum update in\n",
    "            classic momentum, but after momentum for popular momentum.\n",
    "        eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
    "        name: The name for the scope.\n",
    "        \"\"\"\n",
    "\n",
    "        self.epoch = 0\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            use_nesterov=use_nesterov,\n",
    "            weight_decay=weight_decay,\n",
    "            exclude_from_weight_decay=exclude_from_weight_decay,\n",
    "            exclude_from_layer_adaptation=exclude_from_layer_adaptation,\n",
    "            classic_momentum=classic_momentum,\n",
    "            eeta=eeta,\n",
    "        )\n",
    "\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.use_nesterov = use_nesterov\n",
    "        self.classic_momentum = classic_momentum\n",
    "        self.eeta = eeta\n",
    "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
    "        # arg is None.\n",
    "        if exclude_from_layer_adaptation:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
    "        else:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
    "\n",
    "    def step(self, epoch=None, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "            eeta = group[\"eeta\"]\n",
    "            lr = group[\"lr\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param = p.data\n",
    "                grad = p.grad.data\n",
    "\n",
    "                param_state = self.state[p]\n",
    "\n",
    "                # TODO: get param names\n",
    "                # if self._use_weight_decay(param_name):\n",
    "                grad += self.weight_decay * param\n",
    "\n",
    "                if self.classic_momentum:\n",
    "                    trust_ratio = 1.0\n",
    "\n",
    "                    # TODO: get param names\n",
    "                    # if self._do_layer_adaptation(param_name):\n",
    "                    w_norm = torch.norm(param)\n",
    "                    g_norm = torch.norm(grad)\n",
    "\n",
    "                    device = g_norm.get_device()\n",
    "                    trust_ratio = torch.where(\n",
    "                        w_norm.gt(0),\n",
    "                        torch.where(\n",
    "                            g_norm.gt(0),\n",
    "                            (self.eeta * w_norm / g_norm),\n",
    "                            torch.Tensor([1.0]).to(device),\n",
    "                        ),\n",
    "                        torch.Tensor([1.0]).to(device),\n",
    "                    ).item()\n",
    "\n",
    "                    scaled_lr = lr * trust_ratio\n",
    "                    if \"momentum_buffer\" not in param_state:\n",
    "                        next_v = param_state[\"momentum_buffer\"] = torch.zeros_like(\n",
    "                            p.data\n",
    "                        )\n",
    "                    else:\n",
    "                        next_v = param_state[\"momentum_buffer\"]\n",
    "\n",
    "                    next_v.mul_(momentum).add_(scaled_lr, grad)\n",
    "                    if self.use_nesterov:\n",
    "                        update = (self.momentum * next_v) + (scaled_lr * grad)\n",
    "                    else:\n",
    "                        update = next_v\n",
    "\n",
    "                    p.data.add_(-update)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _use_weight_decay(self, param_name):\n",
    "        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "        if not self.weight_decay:\n",
    "            return False\n",
    "        if self.exclude_from_weight_decay:\n",
    "            for r in self.exclude_from_weight_decay:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def _do_layer_adaptation(self, param_name):\n",
    "        \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
    "        if self.exclude_from_layer_adaptation:\n",
    "            for r in self.exclude_from_layer_adaptation:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimCLR contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(SimCLR_Loss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "            \n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        We do not sample negative examples explicitly.\n",
    "        Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N âˆ’ 1) augmented examples within a minibatch as negative examples.\n",
    "        \"\"\"\n",
    "        batch_size = z_i.shape[0]\n",
    "        mask = self.mask_correlated_samples(batch_size)\n",
    "\n",
    "        N = 2 * batch_size #* self.world_size\n",
    "        \n",
    "        #z_i_ = z_i / torch.sqrt(torch.sum(torch.square(z_i),dim = 1, keepdim = True))\n",
    "        #z_j_ = z_j / torch.sqrt(torch.sum(torch.square(z_j),dim = 1, keepdim = True))\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "        \n",
    "        #print(sim.shape)\n",
    "\n",
    "        sim_i_j = torch.diag(sim, batch_size)\n",
    "        sim_j_i = torch.diag(sim, batch_size)\n",
    "        \n",
    "        \n",
    "        # We have 2N samples, but with Distributed training every GPU gets N examples too, resulting in: 2xNxN\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[mask].reshape(N, -1)\n",
    "        \n",
    "        \n",
    "        #SIMCLR\n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #.float()\n",
    "        #labels was torch.zeros(N)\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = LARS(\n",
    "    [params for params in model.parameters() if params.requires_grad],\n",
    "    lr=0.2,\n",
    "    weight_decay=1e-6,\n",
    "    exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    ")\n",
    "\n",
    "# \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (epoch+1)/10.0, verbose = False)\n",
    "mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, eta_min=0.05, last_epoch=-1, verbose = False)\n",
    "\n",
    "criterion = SimCLR_Loss(temperature=0.5)\n",
    "\n",
    "dataloader = DataLoader(constrastive_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "dataloader_no_contrastive = DataLoader(poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import copy\n",
    "\n",
    "def plot_features(model: nn.Module, dataloader: DataLoader, poison_indices: np.array, batches: int = None):\n",
    "    model.eval()\n",
    "\n",
    "    features = None\n",
    "    labels_poison = None\n",
    "    labels_true = None\n",
    "\n",
    "    for i, (img, labels_batch_poison, labels_batch_true) in enumerate(dataloader):\n",
    "\n",
    "        if batches is not None and i>=batches:\n",
    "            break\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            features_batch = model(img.to(device)).cpu().data.numpy()\n",
    "            \n",
    "        if features is None:\n",
    "            features = features_batch\n",
    "            labels_poison = labels_batch_poison\n",
    "            labels_true = labels_batch_true\n",
    "        else:\n",
    "            features = np.append(features, features_batch, axis=0)\n",
    "            labels_poison = np.append(labels_poison, labels_batch_poison, axis=0)\n",
    "            labels_true = np.append(labels_true, labels_batch_true, axis=0)\n",
    "    \n",
    "    n_neighbors = len(features)/100\n",
    "    alg = umap.UMAP(n_neighbors=n_neighbors)\n",
    "    #alg = TSNE(n_components = 2, perplexity = n_neighbors)\n",
    "    features_2d = alg.fit_transform(features)\n",
    "    num_classes = max(labels_batch_poison)\n",
    "\n",
    "    labels_10 = copy.deepcopy(labels_true)\n",
    "    labels_10[poison_indices[:len(features)]] = 10\n",
    "\n",
    "    for i in range(num_classes+1):\n",
    "        plt.scatter(features_2d[labels_10==i,1], features_2d[labels_10==i,0], s=7)\n",
    "    plt.scatter(features_2d[labels_10==10,1], features_2d[labels_10==10,0], c = \"black\", marker= \"x\", s=1)\n",
    "\n",
    "    plt.legend([str(i) for i in range(num_classes+1)] + [\"poison\"])\n",
    "    plt.show()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "#plot_features(model, dataloader_no_contrastive, poison_indices, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer):\n",
    "    loss_epoch = 0\n",
    "    \n",
    "    for step, (x_i, x_j) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x_i = x_i.to(device).float()\n",
    "        x_j = x_j.to(device).float()\n",
    "\n",
    "        # positive pair, with encoding\n",
    "        z_i = model(x_i)\n",
    "        z_j = model(x_j)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"\\tStep [{step}/{len(dataloader)}]\\t Loss: {round(loss.item(), 5)}\")\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "    return loss_epoch / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/new/', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "load_checkpoint = False\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/new', CHECKPOINT_NAME)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    mainscheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/250]\t\n",
      "\tStep [0/196]\t Loss: 5.88501\n",
      "\tStep [50/196]\t Loss: 5.02818\n",
      "\tStep [100/196]\t Loss: 4.6995\n",
      "\tStep [150/196]\t Loss: 4.61232\n",
      "\n",
      "\tTraining Loss: 4.883694184069731\n",
      "\tTime Taken: 3.4512234489123026 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [1/250]\t\n",
      "\tStep [0/196]\t Loss: 4.53542\n",
      "\tStep [50/196]\t Loss: 4.50572\n",
      "\tStep [100/196]\t Loss: 4.50045\n",
      "\tStep [150/196]\t Loss: 4.5061\n",
      "\n",
      "\tTraining Loss: 4.502425793482333\n",
      "\tTime Taken: 3.4367979089419047 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [2/250]\t\n",
      "\tStep [0/196]\t Loss: 4.49434\n",
      "\tStep [50/196]\t Loss: 4.49985\n",
      "\tStep [100/196]\t Loss: 4.47902\n",
      "\tStep [150/196]\t Loss: 4.46766\n",
      "\n",
      "\tTraining Loss: 4.4748291129968605\n",
      "\tTime Taken: 3.4334299246470135 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [3/250]\t\n",
      "\tStep [0/196]\t Loss: 4.4661\n",
      "\tStep [50/196]\t Loss: 4.44774\n",
      "\tStep [100/196]\t Loss: 4.45817\n",
      "\tStep [150/196]\t Loss: 4.45313\n",
      "\n",
      "\tTraining Loss: 4.450465013786238\n",
      "\tTime Taken: 3.431379473209381 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [4/250]\t\n",
      "\tStep [0/196]\t Loss: 4.44092\n",
      "\tStep [50/196]\t Loss: 4.44919\n",
      "\tStep [100/196]\t Loss: 4.42222\n",
      "\tStep [150/196]\t Loss: 4.43183\n",
      "\n",
      "\tTraining Loss: 4.429994462704172\n",
      "\tTime Taken: 3.4143254200617474 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [5/250]\t\n",
      "\tStep [0/196]\t Loss: 4.43406\n",
      "\tStep [50/196]\t Loss: 4.42742\n",
      "\tStep [100/196]\t Loss: 4.41195\n",
      "\tStep [150/196]\t Loss: 4.41955\n",
      "\n",
      "\tTraining Loss: 4.415106042307251\n",
      "\tTime Taken: 3.44233154853185 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [6/250]\t\n",
      "\tStep [0/196]\t Loss: 4.41717\n",
      "\tStep [50/196]\t Loss: 4.42231\n",
      "\tStep [100/196]\t Loss: 4.40894\n",
      "\tStep [150/196]\t Loss: 4.4124\n",
      "\n",
      "\tTraining Loss: 4.402995380820061\n",
      "\tTime Taken: 3.4427085598309835 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [7/250]\t\n",
      "\tStep [0/196]\t Loss: 4.41324\n",
      "\tStep [50/196]\t Loss: 4.40474\n",
      "\tStep [100/196]\t Loss: 4.38981\n",
      "\tStep [150/196]\t Loss: 4.40334\n",
      "\n",
      "\tTraining Loss: 4.392386851262073\n",
      "\tTime Taken: 3.42901789744695 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [8/250]\t\n",
      "\tStep [0/196]\t Loss: 4.38033\n",
      "\tStep [50/196]\t Loss: 4.4008\n",
      "\tStep [100/196]\t Loss: 4.38092\n",
      "\tStep [150/196]\t Loss: 4.3899\n",
      "\n",
      "\tTraining Loss: 4.383867865922499\n",
      "\tTime Taken: 3.42423437833786 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [9/250]\t\n",
      "\tStep [0/196]\t Loss: 4.38169\n",
      "\tStep [50/196]\t Loss: 4.38291\n",
      "\tStep [100/196]\t Loss: 4.37888\n",
      "\tStep [150/196]\t Loss: 4.38514\n",
      "\n",
      "\tTraining Loss: 4.376408420046982\n",
      "\tTime Taken: 3.420475387573242 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [10/250]\t\n",
      "\tStep [0/196]\t Loss: 4.38191\n",
      "\tStep [50/196]\t Loss: 4.36868\n",
      "\tStep [100/196]\t Loss: 4.38156\n",
      "\tStep [150/196]\t Loss: 4.37085\n",
      "\n",
      "\tTraining Loss: 4.369629294288401\n",
      "\tTime Taken: 3.4316086133321124 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [11/250]\t\n",
      "\tStep [0/196]\t Loss: 4.37913\n",
      "\tStep [50/196]\t Loss: 4.37062\n",
      "\tStep [100/196]\t Loss: 4.37813\n",
      "\tStep [150/196]\t Loss: 4.37492\n",
      "\n",
      "\tTraining Loss: 4.363745213771353\n",
      "\tTime Taken: 3.432350842158 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [12/250]\t\n",
      "\tStep [0/196]\t Loss: 4.36211\n",
      "\tStep [50/196]\t Loss: 4.36387\n",
      "\tStep [100/196]\t Loss: 4.35914\n",
      "\tStep [150/196]\t Loss: 4.38142\n",
      "\n",
      "\tTraining Loss: 4.359322933518157\n",
      "\tTime Taken: 3.4340919613838197 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [13/250]\t\n",
      "\tStep [0/196]\t Loss: 4.36415\n",
      "\tStep [50/196]\t Loss: 4.35952\n",
      "\tStep [100/196]\t Loss: 4.35878\n",
      "\tStep [150/196]\t Loss: 4.35705\n",
      "\n",
      "\tTraining Loss: 4.354705782569185\n",
      "\tTime Taken: 3.4321252942085265 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [14/250]\t\n",
      "\tStep [0/196]\t Loss: 4.36474\n",
      "\tStep [50/196]\t Loss: 4.36057\n",
      "\tStep [100/196]\t Loss: 4.35705\n",
      "\tStep [150/196]\t Loss: 4.35444\n",
      "\n",
      "\tTraining Loss: 4.351736565025485\n",
      "\tTime Taken: 3.5010248025258384 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [15/250]\t\n",
      "\tStep [0/196]\t Loss: 4.36172\n",
      "\tStep [50/196]\t Loss: 4.3579\n",
      "\tStep [100/196]\t Loss: 4.35789\n",
      "\tStep [150/196]\t Loss: 4.34788\n",
      "\n",
      "\tTraining Loss: 4.3489921032166\n",
      "\tTime Taken: 3.443258555730184 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [16/250]\t\n",
      "\tStep [0/196]\t Loss: 4.35636\n",
      "\tStep [50/196]\t Loss: 4.34975\n",
      "\tStep [100/196]\t Loss: 4.35757\n",
      "\tStep [150/196]\t Loss: 4.34794\n",
      "\n",
      "\tTraining Loss: 4.345935230352441\n",
      "\tTime Taken: 3.4421306292215985 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [17/250]\t\n",
      "\tStep [0/196]\t Loss: 4.35193\n",
      "\tStep [50/196]\t Loss: 4.35\n",
      "\tStep [100/196]\t Loss: 4.33974\n",
      "\tStep [150/196]\t Loss: 4.34489\n",
      "\n",
      "\tTraining Loss: 4.343821422177918\n",
      "\tTime Taken: 3.440386096636454 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [18/250]\t\n",
      "\tStep [0/196]\t Loss: 4.34552\n",
      "\tStep [50/196]\t Loss: 4.34966\n",
      "\tStep [100/196]\t Loss: 4.34775\n",
      "\tStep [150/196]\t Loss: 4.35087\n",
      "\n",
      "\tTraining Loss: 4.341851556787685\n",
      "\tTime Taken: 3.437350837389628 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [19/250]\t\n",
      "\tStep [0/196]\t Loss: 4.35009\n",
      "\tStep [50/196]\t Loss: 4.34646\n",
      "\tStep [100/196]\t Loss: 4.34445\n",
      "\tStep [150/196]\t Loss: 4.35526\n",
      "\n",
      "\tTraining Loss: 4.339137796236544\n",
      "\tTime Taken: 3.3737090468406676 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [20/250]\t\n",
      "\tStep [0/196]\t Loss: 4.34452\n",
      "\tStep [50/196]\t Loss: 4.33582\n",
      "\tStep [100/196]\t Loss: 4.33799\n",
      "\tStep [150/196]\t Loss: 4.34321\n",
      "\n",
      "\tTraining Loss: 4.338487608092172\n",
      "\tTime Taken: 3.4326252937316895 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [21/250]\t\n",
      "\tStep [0/196]\t Loss: 4.33904\n",
      "\tStep [50/196]\t Loss: 4.34348\n",
      "\tStep [100/196]\t Loss: 4.34248\n",
      "\tStep [150/196]\t Loss: 4.3428\n",
      "\n",
      "\tTraining Loss: 4.336102039230113\n",
      "\tTime Taken: 3.426708662509918 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [22/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3433\n",
      "\tStep [50/196]\t Loss: 4.34685\n",
      "\tStep [100/196]\t Loss: 4.33724\n",
      "\tStep [150/196]\t Loss: 4.3352\n",
      "\n",
      "\tTraining Loss: 4.334745352365533\n",
      "\tTime Taken: 3.420508694648743 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [23/250]\t\n",
      "\tStep [0/196]\t Loss: 4.34299\n",
      "\tStep [50/196]\t Loss: 4.33966\n",
      "\tStep [100/196]\t Loss: 4.33022\n",
      "\tStep [150/196]\t Loss: 4.33913\n",
      "\n",
      "\tTraining Loss: 4.3331299947232615\n",
      "\tTime Taken: 3.430421725908915 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [24/250]\t\n",
      "\tStep [0/196]\t Loss: 4.33379\n",
      "\tStep [50/196]\t Loss: 4.33422\n",
      "\tStep [100/196]\t Loss: 4.33614\n",
      "\tStep [150/196]\t Loss: 4.33504\n",
      "\n",
      "\tTraining Loss: 4.331498596133018\n",
      "\tTime Taken: 3.4452418526013693 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [25/250]\t\n",
      "\tStep [0/196]\t Loss: 4.33723\n",
      "\tStep [50/196]\t Loss: 4.33988\n",
      "\tStep [100/196]\t Loss: 4.33268\n",
      "\tStep [150/196]\t Loss: 4.33534\n",
      "\n",
      "\tTraining Loss: 4.331542719383629\n",
      "\tTime Taken: 3.4335252682367963 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [26/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3355\n",
      "\tStep [50/196]\t Loss: 4.33679\n",
      "\tStep [100/196]\t Loss: 4.32965\n",
      "\tStep [150/196]\t Loss: 4.33005\n",
      "\n",
      "\tTraining Loss: 4.329211997742555\n",
      "\tTime Taken: 3.4169753591219583 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [27/250]\t\n",
      "\tStep [0/196]\t Loss: 4.33543\n",
      "\tStep [50/196]\t Loss: 4.33759\n",
      "\tStep [100/196]\t Loss: 4.32797\n",
      "\tStep [150/196]\t Loss: 4.33527\n",
      "\n",
      "\tTraining Loss: 4.328367798912282\n",
      "\tTime Taken: 3.4450969139734906 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [28/250]\t\n",
      "\tStep [0/196]\t Loss: 4.33672\n",
      "\tStep [50/196]\t Loss: 4.33162\n",
      "\tStep [100/196]\t Loss: 4.33442\n",
      "\tStep [150/196]\t Loss: 4.33296\n",
      "\n",
      "\tTraining Loss: 4.32734779314119\n",
      "\tTime Taken: 3.4399674336115518 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [29/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32358\n",
      "\tStep [50/196]\t Loss: 4.32488\n",
      "\tStep [100/196]\t Loss: 4.3341\n",
      "\tStep [150/196]\t Loss: 4.33041\n",
      "\n",
      "\tTraining Loss: 4.326177740583614\n",
      "\tTime Taken: 3.4420418938001 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [30/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3269\n",
      "\tStep [50/196]\t Loss: 4.32613\n",
      "\tStep [100/196]\t Loss: 4.32846\n",
      "\tStep [150/196]\t Loss: 4.32835\n",
      "\n",
      "\tTraining Loss: 4.324953661889446\n",
      "\tTime Taken: 3.4388419429461163 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [31/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32527\n",
      "\tStep [50/196]\t Loss: 4.32981\n",
      "\tStep [100/196]\t Loss: 4.32938\n",
      "\tStep [150/196]\t Loss: 4.33168\n",
      "\n",
      "\tTraining Loss: 4.3246669343539645\n",
      "\tTime Taken: 3.4329586187998453 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [32/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32761\n",
      "\tStep [50/196]\t Loss: 4.32831\n",
      "\tStep [100/196]\t Loss: 4.32552\n",
      "\tStep [150/196]\t Loss: 4.32566\n",
      "\n",
      "\tTraining Loss: 4.324038593136534\n",
      "\tTime Taken: 3.428658664226532 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [33/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3291\n",
      "\tStep [50/196]\t Loss: 4.32624\n",
      "\tStep [100/196]\t Loss: 4.32969\n",
      "\tStep [150/196]\t Loss: 4.32722\n",
      "\n",
      "\tTraining Loss: 4.322684981385056\n",
      "\tTime Taken: 3.4300919731458026 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [34/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32519\n",
      "\tStep [50/196]\t Loss: 4.3325\n",
      "\tStep [100/196]\t Loss: 4.32719\n",
      "\tStep [150/196]\t Loss: 4.31818\n",
      "\n",
      "\tTraining Loss: 4.321795240956909\n",
      "\tTime Taken: 3.437258577346802 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [35/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32164\n",
      "\tStep [50/196]\t Loss: 4.32484\n",
      "\tStep [100/196]\t Loss: 4.32258\n",
      "\tStep [150/196]\t Loss: 4.3359\n",
      "\n",
      "\tTraining Loss: 4.321328991529893\n",
      "\tTime Taken: 3.4240753451983132 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [36/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32358\n",
      "\tStep [50/196]\t Loss: 4.3231\n",
      "\tStep [100/196]\t Loss: 4.32744\n",
      "\tStep [150/196]\t Loss: 4.31737\n",
      "\n",
      "\tTraining Loss: 4.320343744998076\n",
      "\tTime Taken: 3.412675615151723 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [37/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3236\n",
      "\tStep [50/196]\t Loss: 4.32935\n",
      "\tStep [100/196]\t Loss: 4.32405\n",
      "\tStep [150/196]\t Loss: 4.32437\n",
      "\n",
      "\tTraining Loss: 4.31975860133463\n",
      "\tTime Taken: 3.4234086672465005 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [38/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32279\n",
      "\tStep [50/196]\t Loss: 4.32226\n",
      "\tStep [100/196]\t Loss: 4.32382\n",
      "\tStep [150/196]\t Loss: 4.32054\n",
      "\n",
      "\tTraining Loss: 4.318518738357389\n",
      "\tTime Taken: 3.444608545303345 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [39/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32345\n",
      "\tStep [50/196]\t Loss: 4.32609\n",
      "\tStep [100/196]\t Loss: 4.32847\n",
      "\tStep [150/196]\t Loss: 4.32328\n",
      "\n",
      "\tTraining Loss: 4.3183092158667895\n",
      "\tTime Taken: 3.435992153485616 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [40/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32884\n",
      "\tStep [50/196]\t Loss: 4.32112\n",
      "\tStep [100/196]\t Loss: 4.326\n",
      "\tStep [150/196]\t Loss: 4.32395\n",
      "\n",
      "\tTraining Loss: 4.31729557076279\n",
      "\tTime Taken: 3.424475971857707 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [41/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32882\n",
      "\tStep [50/196]\t Loss: 4.32078\n",
      "\tStep [100/196]\t Loss: 4.32561\n",
      "\tStep [150/196]\t Loss: 4.32268\n",
      "\n",
      "\tTraining Loss: 4.316827719308892\n",
      "\tTime Taken: 3.397426195939382 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [42/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31868\n",
      "\tStep [50/196]\t Loss: 4.32048\n",
      "\tStep [100/196]\t Loss: 4.32091\n",
      "\tStep [150/196]\t Loss: 4.32309\n",
      "\n",
      "\tTraining Loss: 4.316312027220824\n",
      "\tTime Taken: 3.404842821756999 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [43/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31634\n",
      "\tStep [50/196]\t Loss: 4.32009\n",
      "\tStep [100/196]\t Loss: 4.31943\n",
      "\tStep [150/196]\t Loss: 4.32041\n",
      "\n",
      "\tTraining Loss: 4.315774585519518\n",
      "\tTime Taken: 3.420609410603841 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [44/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31665\n",
      "\tStep [50/196]\t Loss: 4.32579\n",
      "\tStep [100/196]\t Loss: 4.32514\n",
      "\tStep [150/196]\t Loss: 4.31999\n",
      "\n",
      "\tTraining Loss: 4.314899327803631\n",
      "\tTime Taken: 3.416509413719177 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [45/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32128\n",
      "\tStep [50/196]\t Loss: 4.32821\n",
      "\tStep [100/196]\t Loss: 4.31728\n",
      "\tStep [150/196]\t Loss: 4.32555\n",
      "\n",
      "\tTraining Loss: 4.314510315048452\n",
      "\tTime Taken: 3.406684947013855 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [46/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32432\n",
      "\tStep [50/196]\t Loss: 4.31813\n",
      "\tStep [100/196]\t Loss: 4.32675\n",
      "\tStep [150/196]\t Loss: 4.32238\n",
      "\n",
      "\tTraining Loss: 4.3143536205194435\n",
      "\tTime Taken: 3.4224093675613405 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [47/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32092\n",
      "\tStep [50/196]\t Loss: 4.31546\n",
      "\tStep [100/196]\t Loss: 4.31728\n",
      "\tStep [150/196]\t Loss: 4.31564\n",
      "\n",
      "\tTraining Loss: 4.3133833140743025\n",
      "\tTime Taken: 3.4150594234466554 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [48/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3233\n",
      "\tStep [50/196]\t Loss: 4.31749\n",
      "\tStep [100/196]\t Loss: 4.31933\n",
      "\tStep [150/196]\t Loss: 4.31932\n",
      "\n",
      "\tTraining Loss: 4.312987309329364\n",
      "\tTime Taken: 3.3929595947265625 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [49/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32804\n",
      "\tStep [50/196]\t Loss: 4.31594\n",
      "\tStep [100/196]\t Loss: 4.31986\n",
      "\tStep [150/196]\t Loss: 4.32356\n",
      "\n",
      "\tTraining Loss: 4.3126804293418415\n",
      "\tTime Taken: 3.4074170192082724 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [50/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31938\n",
      "\tStep [50/196]\t Loss: 4.31848\n",
      "\tStep [100/196]\t Loss: 4.31618\n",
      "\tStep [150/196]\t Loss: 4.31345\n",
      "\n",
      "\tTraining Loss: 4.311896534598604\n",
      "\tTime Taken: 3.4127920786539714 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [51/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31617\n",
      "\tStep [50/196]\t Loss: 4.31735\n",
      "\tStep [100/196]\t Loss: 4.31034\n",
      "\tStep [150/196]\t Loss: 4.32257\n",
      "\n",
      "\tTraining Loss: 4.311656085812316\n",
      "\tTime Taken: 3.413691826661428 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [52/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31613\n",
      "\tStep [50/196]\t Loss: 4.31715\n",
      "\tStep [100/196]\t Loss: 4.31423\n",
      "\tStep [150/196]\t Loss: 4.31121\n",
      "\n",
      "\tTraining Loss: 4.311167075925944\n",
      "\tTime Taken: 3.409954722722371 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [53/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31491\n",
      "\tStep [50/196]\t Loss: 4.31274\n",
      "\tStep [100/196]\t Loss: 4.31393\n",
      "\tStep [150/196]\t Loss: 4.31837\n",
      "\n",
      "\tTraining Loss: 4.310716068258091\n",
      "\tTime Taken: 3.407259420553843 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [54/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3175\n",
      "\tStep [50/196]\t Loss: 4.31432\n",
      "\tStep [100/196]\t Loss: 4.31768\n",
      "\tStep [150/196]\t Loss: 4.31956\n",
      "\n",
      "\tTraining Loss: 4.31021172051527\n",
      "\tTime Taken: 3.41485059261322 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [55/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31221\n",
      "\tStep [50/196]\t Loss: 4.31305\n",
      "\tStep [100/196]\t Loss: 4.32039\n",
      "\tStep [150/196]\t Loss: 4.31769\n",
      "\n",
      "\tTraining Loss: 4.3098088855646095\n",
      "\tTime Taken: 3.3932589133580526 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [56/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31474\n",
      "\tStep [50/196]\t Loss: 4.31588\n",
      "\tStep [100/196]\t Loss: 4.31562\n",
      "\tStep [150/196]\t Loss: 4.32079\n",
      "\n",
      "\tTraining Loss: 4.3094240414853\n",
      "\tTime Taken: 3.42159202893575 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [57/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3123\n",
      "\tStep [50/196]\t Loss: 4.31173\n",
      "\tStep [100/196]\t Loss: 4.31808\n",
      "\tStep [150/196]\t Loss: 4.31478\n",
      "\n",
      "\tTraining Loss: 4.30934696781392\n",
      "\tTime Taken: 3.415742075443268 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [58/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31191\n",
      "\tStep [50/196]\t Loss: 4.31605\n",
      "\tStep [100/196]\t Loss: 4.32127\n",
      "\tStep [150/196]\t Loss: 4.30913\n",
      "\n",
      "\tTraining Loss: 4.308815999906891\n",
      "\tTime Taken: 3.419485700130463 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [59/250]\t\n",
      "\tStep [0/196]\t Loss: 4.32252\n",
      "\tStep [50/196]\t Loss: 4.31788\n",
      "\tStep [100/196]\t Loss: 4.31875\n",
      "\tStep [150/196]\t Loss: 4.31994\n",
      "\n",
      "\tTraining Loss: 4.308540419656403\n",
      "\tTime Taken: 3.423998785018921 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [60/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31262\n",
      "\tStep [50/196]\t Loss: 4.31567\n",
      "\tStep [100/196]\t Loss: 4.31194\n",
      "\tStep [150/196]\t Loss: 4.31124\n",
      "\n",
      "\tTraining Loss: 4.307739460954861\n",
      "\tTime Taken: 3.4245593746503196 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [61/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31593\n",
      "\tStep [50/196]\t Loss: 4.31179\n",
      "\tStep [100/196]\t Loss: 4.3197\n",
      "\tStep [150/196]\t Loss: 4.30981\n",
      "\n",
      "\tTraining Loss: 4.307160111106172\n",
      "\tTime Taken: 3.423888985315959 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [62/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31446\n",
      "\tStep [50/196]\t Loss: 4.3155\n",
      "\tStep [100/196]\t Loss: 4.31259\n",
      "\tStep [150/196]\t Loss: 4.31838\n",
      "\n",
      "\tTraining Loss: 4.306938527798166\n",
      "\tTime Taken: 3.4090099334716797 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [63/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31492\n",
      "\tStep [50/196]\t Loss: 4.31335\n",
      "\tStep [100/196]\t Loss: 4.31614\n",
      "\tStep [150/196]\t Loss: 4.31215\n",
      "\n",
      "\tTraining Loss: 4.307005682770087\n",
      "\tTime Taken: 3.432422602176666 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [64/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30922\n",
      "\tStep [50/196]\t Loss: 4.31834\n",
      "\tStep [100/196]\t Loss: 4.3107\n",
      "\tStep [150/196]\t Loss: 4.31127\n",
      "\n",
      "\tTraining Loss: 4.306795505844817\n",
      "\tTime Taken: 3.4317030787467955 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [65/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31278\n",
      "\tStep [50/196]\t Loss: 4.31182\n",
      "\tStep [100/196]\t Loss: 4.30999\n",
      "\tStep [150/196]\t Loss: 4.30974\n",
      "\n",
      "\tTraining Loss: 4.305973403307856\n",
      "\tTime Taken: 3.424211549758911 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [66/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31734\n",
      "\tStep [50/196]\t Loss: 4.3149\n",
      "\tStep [100/196]\t Loss: 4.31742\n",
      "\tStep [150/196]\t Loss: 4.31961\n",
      "\n",
      "\tTraining Loss: 4.306102423035369\n",
      "\tTime Taken: 3.4150346517562866 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [67/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31267\n",
      "\tStep [50/196]\t Loss: 4.31338\n",
      "\tStep [100/196]\t Loss: 4.30952\n",
      "\tStep [150/196]\t Loss: 4.30684\n",
      "\n",
      "\tTraining Loss: 4.305405982903072\n",
      "\tTime Taken: 3.4274152278900147 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [68/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31103\n",
      "\tStep [50/196]\t Loss: 4.30388\n",
      "\tStep [100/196]\t Loss: 4.31228\n",
      "\tStep [150/196]\t Loss: 4.31561\n",
      "\n",
      "\tTraining Loss: 4.305374216060249\n",
      "\tTime Taken: 3.4402519186337788 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [69/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31138\n",
      "\tStep [50/196]\t Loss: 4.31435\n",
      "\tStep [100/196]\t Loss: 4.31461\n",
      "\tStep [150/196]\t Loss: 4.30709\n",
      "\n",
      "\tTraining Loss: 4.305049243021984\n",
      "\tTime Taken: 3.442271176973979 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [70/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31599\n",
      "\tStep [50/196]\t Loss: 4.30811\n",
      "\tStep [100/196]\t Loss: 4.3068\n",
      "\tStep [150/196]\t Loss: 4.31595\n",
      "\n",
      "\tTraining Loss: 4.3045598986197495\n",
      "\tTime Taken: 3.4212978720664977 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [71/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30739\n",
      "\tStep [50/196]\t Loss: 4.30824\n",
      "\tStep [100/196]\t Loss: 4.30789\n",
      "\tStep [150/196]\t Loss: 4.31451\n",
      "\n",
      "\tTraining Loss: 4.304420994252575\n",
      "\tTime Taken: 3.443807832400004 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [72/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30671\n",
      "\tStep [50/196]\t Loss: 4.30176\n",
      "\tStep [100/196]\t Loss: 4.30746\n",
      "\tStep [150/196]\t Loss: 4.30567\n",
      "\n",
      "\tTraining Loss: 4.304054845352562\n",
      "\tTime Taken: 3.448593803246816 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [73/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31014\n",
      "\tStep [50/196]\t Loss: 4.31334\n",
      "\tStep [100/196]\t Loss: 4.31034\n",
      "\tStep [150/196]\t Loss: 4.30932\n",
      "\n",
      "\tTraining Loss: 4.303617715835571\n",
      "\tTime Taken: 3.440900647640228 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [74/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30565\n",
      "\tStep [50/196]\t Loss: 4.30871\n",
      "\tStep [100/196]\t Loss: 4.30852\n",
      "\tStep [150/196]\t Loss: 4.30859\n",
      "\n",
      "\tTraining Loss: 4.303283418927874\n",
      "\tTime Taken: 3.41465957959493 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [75/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3063\n",
      "\tStep [50/196]\t Loss: 4.30605\n",
      "\tStep [100/196]\t Loss: 4.30585\n",
      "\tStep [150/196]\t Loss: 4.30948\n",
      "\n",
      "\tTraining Loss: 4.302755973776993\n",
      "\tTime Taken: 3.4337941368420917 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [76/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30939\n",
      "\tStep [50/196]\t Loss: 4.30963\n",
      "\tStep [100/196]\t Loss: 4.31191\n",
      "\tStep [150/196]\t Loss: 4.30589\n",
      "\n",
      "\tTraining Loss: 4.30291531767164\n",
      "\tTime Taken: 3.429525311787923 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [77/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30511\n",
      "\tStep [50/196]\t Loss: 4.30736\n",
      "\tStep [100/196]\t Loss: 4.30363\n",
      "\tStep [150/196]\t Loss: 4.31192\n",
      "\n",
      "\tTraining Loss: 4.301934930743004\n",
      "\tTime Taken: 3.427058633168538 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [78/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31191\n",
      "\tStep [50/196]\t Loss: 4.30797\n",
      "\tStep [100/196]\t Loss: 4.30589\n",
      "\tStep [150/196]\t Loss: 4.30888\n",
      "\n",
      "\tTraining Loss: 4.302731459238092\n",
      "\tTime Taken: 3.4223587115605674 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [79/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30714\n",
      "\tStep [50/196]\t Loss: 4.31072\n",
      "\tStep [100/196]\t Loss: 4.30573\n",
      "\tStep [150/196]\t Loss: 4.30279\n",
      "\n",
      "\tTraining Loss: 4.302165521650898\n",
      "\tTime Taken: 3.4252753376960756 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [80/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30587\n",
      "\tStep [50/196]\t Loss: 4.30318\n",
      "\tStep [100/196]\t Loss: 4.30704\n",
      "\tStep [150/196]\t Loss: 4.30796\n",
      "\n",
      "\tTraining Loss: 4.301712508104285\n",
      "\tTime Taken: 3.43404194911321 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [81/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3073\n",
      "\tStep [50/196]\t Loss: 4.30984\n",
      "\tStep [100/196]\t Loss: 4.30774\n",
      "\tStep [150/196]\t Loss: 4.31045\n",
      "\n",
      "\tTraining Loss: 4.301377266037221\n",
      "\tTime Taken: 3.425141990184784 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [82/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30531\n",
      "\tStep [50/196]\t Loss: 4.308\n",
      "\tStep [100/196]\t Loss: 4.30551\n",
      "\tStep [150/196]\t Loss: 4.3085\n",
      "\n",
      "\tTraining Loss: 4.301129062565005\n",
      "\tTime Taken: 3.425992023944855 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [83/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30106\n",
      "\tStep [50/196]\t Loss: 4.31315\n",
      "\tStep [100/196]\t Loss: 4.30913\n",
      "\tStep [150/196]\t Loss: 4.30348\n",
      "\n",
      "\tTraining Loss: 4.300692549773625\n",
      "\tTime Taken: 3.4205419977506 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [84/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31109\n",
      "\tStep [50/196]\t Loss: 4.30565\n",
      "\tStep [100/196]\t Loss: 4.30429\n",
      "\tStep [150/196]\t Loss: 4.31286\n",
      "\n",
      "\tTraining Loss: 4.300544078252753\n",
      "\tTime Taken: 3.435208594799042 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [85/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30804\n",
      "\tStep [50/196]\t Loss: 4.30428\n",
      "\tStep [100/196]\t Loss: 4.30141\n",
      "\tStep [150/196]\t Loss: 4.305\n",
      "\n",
      "\tTraining Loss: 4.30024636156705\n",
      "\tTime Taken: 3.4188087145487467 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [86/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3034\n",
      "\tStep [50/196]\t Loss: 4.30786\n",
      "\tStep [100/196]\t Loss: 4.30866\n",
      "\tStep [150/196]\t Loss: 4.30497\n",
      "\n",
      "\tTraining Loss: 4.300079889443456\n",
      "\tTime Taken: 3.4188753922780353 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [87/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3013\n",
      "\tStep [50/196]\t Loss: 4.30172\n",
      "\tStep [100/196]\t Loss: 4.30423\n",
      "\tStep [150/196]\t Loss: 4.30389\n",
      "\n",
      "\tTraining Loss: 4.299832297831165\n",
      "\tTime Taken: 3.430441987514496 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [88/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30896\n",
      "\tStep [50/196]\t Loss: 4.30607\n",
      "\tStep [100/196]\t Loss: 4.3054\n",
      "\tStep [150/196]\t Loss: 4.30304\n",
      "\n",
      "\tTraining Loss: 4.299743590306263\n",
      "\tTime Taken: 3.4304376522699993 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [89/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30191\n",
      "\tStep [50/196]\t Loss: 4.3047\n",
      "\tStep [100/196]\t Loss: 4.30668\n",
      "\tStep [150/196]\t Loss: 4.30492\n",
      "\n",
      "\tTraining Loss: 4.299620189228836\n",
      "\tTime Taken: 3.414659063021342 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [90/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30324\n",
      "\tStep [50/196]\t Loss: 4.30157\n",
      "\tStep [100/196]\t Loss: 4.30745\n",
      "\tStep [150/196]\t Loss: 4.30153\n",
      "\n",
      "\tTraining Loss: 4.2993674436394045\n",
      "\tTime Taken: 3.4158760984738668 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [91/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30476\n",
      "\tStep [50/196]\t Loss: 4.30708\n",
      "\tStep [100/196]\t Loss: 4.30636\n",
      "\tStep [150/196]\t Loss: 4.30425\n",
      "\n",
      "\tTraining Loss: 4.299249087061201\n",
      "\tTime Taken: 3.4269593556722007 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [92/250]\t\n",
      "\tStep [0/196]\t Loss: 4.31333\n",
      "\tStep [50/196]\t Loss: 4.30276\n",
      "\tStep [100/196]\t Loss: 4.30527\n",
      "\tStep [150/196]\t Loss: 4.30082\n",
      "\n",
      "\tTraining Loss: 4.299115785530636\n",
      "\tTime Taken: 3.419376027584076 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [93/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30543\n",
      "\tStep [50/196]\t Loss: 4.30216\n",
      "\tStep [100/196]\t Loss: 4.30648\n",
      "\tStep [150/196]\t Loss: 4.30216\n",
      "\n",
      "\tTraining Loss: 4.298523293465984\n",
      "\tTime Taken: 3.400526197751363 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [94/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30333\n",
      "\tStep [50/196]\t Loss: 4.30953\n",
      "\tStep [100/196]\t Loss: 4.30925\n",
      "\tStep [150/196]\t Loss: 4.30295\n",
      "\n",
      "\tTraining Loss: 4.298517044709653\n",
      "\tTime Taken: 3.4220688263575236 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [95/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30559\n",
      "\tStep [50/196]\t Loss: 4.30691\n",
      "\tStep [100/196]\t Loss: 4.30279\n",
      "\tStep [150/196]\t Loss: 4.30631\n",
      "\n",
      "\tTraining Loss: 4.298353715818756\n",
      "\tTime Taken: 3.474909027417501 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [96/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30301\n",
      "\tStep [50/196]\t Loss: 4.30769\n",
      "\tStep [100/196]\t Loss: 4.30485\n",
      "\tStep [150/196]\t Loss: 4.30347\n",
      "\n",
      "\tTraining Loss: 4.2980333067932905\n",
      "\tTime Taken: 3.513926148414612 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [97/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3033\n",
      "\tStep [50/196]\t Loss: 4.30462\n",
      "\tStep [100/196]\t Loss: 4.30991\n",
      "\tStep [150/196]\t Loss: 4.30292\n",
      "\n",
      "\tTraining Loss: 4.297947423798697\n",
      "\tTime Taken: 3.4844089070955913 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [98/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3003\n",
      "\tStep [50/196]\t Loss: 4.30332\n",
      "\tStep [100/196]\t Loss: 4.30244\n",
      "\tStep [150/196]\t Loss: 4.30529\n",
      "\n",
      "\tTraining Loss: 4.297362680337867\n",
      "\tTime Taken: 3.5369585951169333 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [99/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30362\n",
      "\tStep [50/196]\t Loss: 4.30587\n",
      "\tStep [100/196]\t Loss: 4.29991\n",
      "\tStep [150/196]\t Loss: 4.30198\n",
      "\n",
      "\tTraining Loss: 4.297549636996522\n",
      "\tTime Taken: 3.527360991636912 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [100/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30065\n",
      "\tStep [50/196]\t Loss: 4.30537\n",
      "\tStep [100/196]\t Loss: 4.30147\n",
      "\tStep [150/196]\t Loss: 4.30147\n",
      "\n",
      "\tTraining Loss: 4.297389510942965\n",
      "\tTime Taken: 3.5171573758125305 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [101/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30283\n",
      "\tStep [50/196]\t Loss: 4.3047\n",
      "\tStep [100/196]\t Loss: 4.2999\n",
      "\tStep [150/196]\t Loss: 4.30009\n",
      "\n",
      "\tTraining Loss: 4.297443004287019\n",
      "\tTime Taken: 3.5327236930529278 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [102/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30254\n",
      "\tStep [50/196]\t Loss: 4.30896\n",
      "\tStep [100/196]\t Loss: 4.3061\n",
      "\tStep [150/196]\t Loss: 4.2989\n",
      "\n",
      "\tTraining Loss: 4.2970381075022175\n",
      "\tTime Taken: 3.527823793888092 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [103/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30203\n",
      "\tStep [50/196]\t Loss: 4.3024\n",
      "\tStep [100/196]\t Loss: 4.30051\n",
      "\tStep [150/196]\t Loss: 4.30127\n",
      "\n",
      "\tTraining Loss: 4.296677288960438\n",
      "\tTime Taken: 3.530540410677592 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [104/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30119\n",
      "\tStep [50/196]\t Loss: 4.30271\n",
      "\tStep [100/196]\t Loss: 4.30514\n",
      "\tStep [150/196]\t Loss: 4.30534\n",
      "\n",
      "\tTraining Loss: 4.296655732758191\n",
      "\tTime Taken: 3.5338403304417927 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [105/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30137\n",
      "\tStep [50/196]\t Loss: 4.30509\n",
      "\tStep [100/196]\t Loss: 4.29945\n",
      "\tStep [150/196]\t Loss: 4.30108\n",
      "\n",
      "\tTraining Loss: 4.296234054224832\n",
      "\tTime Taken: 3.533173684279124 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [106/250]\t\n",
      "\tStep [0/196]\t Loss: 4.2987\n",
      "\tStep [50/196]\t Loss: 4.30373\n",
      "\tStep [100/196]\t Loss: 4.30424\n",
      "\tStep [150/196]\t Loss: 4.30038\n",
      "\n",
      "\tTraining Loss: 4.296260123350183\n",
      "\tTime Taken: 3.524923833211263 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [107/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30311\n",
      "\tStep [50/196]\t Loss: 4.30571\n",
      "\tStep [100/196]\t Loss: 4.30653\n",
      "\tStep [150/196]\t Loss: 4.30651\n",
      "\n",
      "\tTraining Loss: 4.296252733590651\n",
      "\tTime Taken: 3.529873736699422 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [108/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29774\n",
      "\tStep [50/196]\t Loss: 4.30351\n",
      "\tStep [100/196]\t Loss: 4.30243\n",
      "\tStep [150/196]\t Loss: 4.30091\n",
      "\n",
      "\tTraining Loss: 4.295613937231959\n",
      "\tTime Taken: 3.525107189019521 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [109/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30594\n",
      "\tStep [50/196]\t Loss: 4.30137\n",
      "\tStep [100/196]\t Loss: 4.30372\n",
      "\tStep [150/196]\t Loss: 4.30398\n",
      "\n",
      "\tTraining Loss: 4.295671606550411\n",
      "\tTime Taken: 3.5363673051198323 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [110/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30454\n",
      "\tStep [50/196]\t Loss: 4.29789\n",
      "\tStep [100/196]\t Loss: 4.30166\n",
      "\tStep [150/196]\t Loss: 4.29684\n",
      "\n",
      "\tTraining Loss: 4.295335789116061\n",
      "\tTime Taken: 3.522469480832418 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [111/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30307\n",
      "\tStep [50/196]\t Loss: 4.29952\n",
      "\tStep [100/196]\t Loss: 4.30306\n",
      "\tStep [150/196]\t Loss: 4.29987\n",
      "\n",
      "\tTraining Loss: 4.295048553116468\n",
      "\tTime Taken: 3.528086050351461 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [112/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29663\n",
      "\tStep [50/196]\t Loss: 4.3045\n",
      "\tStep [100/196]\t Loss: 4.29911\n",
      "\tStep [150/196]\t Loss: 4.29832\n",
      "\n",
      "\tTraining Loss: 4.2951193135611865\n",
      "\tTime Taken: 3.522886081536611 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [113/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30108\n",
      "\tStep [50/196]\t Loss: 4.29984\n",
      "\tStep [100/196]\t Loss: 4.30748\n",
      "\tStep [150/196]\t Loss: 4.30281\n",
      "\n",
      "\tTraining Loss: 4.29513636778812\n",
      "\tTime Taken: 3.5108029723167418 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [114/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30341\n",
      "\tStep [50/196]\t Loss: 4.3008\n",
      "\tStep [100/196]\t Loss: 4.30229\n",
      "\tStep [150/196]\t Loss: 4.3019\n",
      "\n",
      "\tTraining Loss: 4.29465530113298\n",
      "\tTime Taken: 3.5301360209782917 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [115/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30139\n",
      "\tStep [50/196]\t Loss: 4.30378\n",
      "\tStep [100/196]\t Loss: 4.30105\n",
      "\tStep [150/196]\t Loss: 4.30112\n",
      "\n",
      "\tTraining Loss: 4.294569893759125\n",
      "\tTime Taken: 3.531702661514282 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [116/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30192\n",
      "\tStep [50/196]\t Loss: 4.30254\n",
      "\tStep [100/196]\t Loss: 4.30501\n",
      "\tStep [150/196]\t Loss: 4.29648\n",
      "\n",
      "\tTraining Loss: 4.2945780535133515\n",
      "\tTime Taken: 3.5133362889289854 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [117/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30094\n",
      "\tStep [50/196]\t Loss: 4.29862\n",
      "\tStep [100/196]\t Loss: 4.30569\n",
      "\tStep [150/196]\t Loss: 4.2984\n",
      "\n",
      "\tTraining Loss: 4.294198784292961\n",
      "\tTime Taken: 3.5192861676216127 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [118/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29969\n",
      "\tStep [50/196]\t Loss: 4.30316\n",
      "\tStep [100/196]\t Loss: 4.29866\n",
      "\tStep [150/196]\t Loss: 4.29733\n",
      "\n",
      "\tTraining Loss: 4.294039401472832\n",
      "\tTime Taken: 3.530639123916626 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [119/250]\t\n",
      "\tStep [0/196]\t Loss: 4.3014\n",
      "\tStep [50/196]\t Loss: 4.30059\n",
      "\tStep [100/196]\t Loss: 4.30426\n",
      "\tStep [150/196]\t Loss: 4.2961\n",
      "\n",
      "\tTraining Loss: 4.293856978416443\n",
      "\tTime Taken: 3.517231837908427 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [120/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29901\n",
      "\tStep [50/196]\t Loss: 4.30176\n",
      "\tStep [100/196]\t Loss: 4.29794\n",
      "\tStep [150/196]\t Loss: 4.29788\n",
      "\n",
      "\tTraining Loss: 4.293999686533091\n",
      "\tTime Taken: 3.511048460006714 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [121/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29989\n",
      "\tStep [50/196]\t Loss: 4.3032\n",
      "\tStep [100/196]\t Loss: 4.30156\n",
      "\tStep [150/196]\t Loss: 4.30195\n",
      "\n",
      "\tTraining Loss: 4.293817625970257\n",
      "\tTime Taken: 3.529798368612925 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [122/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29995\n",
      "\tStep [50/196]\t Loss: 4.29883\n",
      "\tStep [100/196]\t Loss: 4.30278\n",
      "\tStep [150/196]\t Loss: 4.29341\n",
      "\n",
      "\tTraining Loss: 4.29313062648384\n",
      "\tTime Taken: 3.530448397000631 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [123/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29741\n",
      "\tStep [50/196]\t Loss: 4.29795\n",
      "\tStep [100/196]\t Loss: 4.29997\n",
      "\tStep [150/196]\t Loss: 4.29956\n",
      "\n",
      "\tTraining Loss: 4.293303206258891\n",
      "\tTime Taken: 3.5170406699180603 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [124/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30105\n",
      "\tStep [50/196]\t Loss: 4.30168\n",
      "\tStep [100/196]\t Loss: 4.29882\n",
      "\tStep [150/196]\t Loss: 4.29653\n",
      "\n",
      "\tTraining Loss: 4.293102551479729\n",
      "\tTime Taken: 3.4646990458170572 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [125/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29913\n",
      "\tStep [50/196]\t Loss: 4.29735\n",
      "\tStep [100/196]\t Loss: 4.29849\n",
      "\tStep [150/196]\t Loss: 4.29367\n",
      "\n",
      "\tTraining Loss: 4.293145223539703\n",
      "\tTime Taken: 3.538415022691091 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [126/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29737\n",
      "\tStep [50/196]\t Loss: 4.29517\n",
      "\tStep [100/196]\t Loss: 4.29902\n",
      "\tStep [150/196]\t Loss: 4.29816\n",
      "\n",
      "\tTraining Loss: 4.292524510500383\n",
      "\tTime Taken: 3.532798365751902 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [127/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29564\n",
      "\tStep [50/196]\t Loss: 4.29825\n",
      "\tStep [100/196]\t Loss: 4.29837\n",
      "\tStep [150/196]\t Loss: 4.30068\n",
      "\n",
      "\tTraining Loss: 4.2925856490524446\n",
      "\tTime Taken: 3.5247984449068706 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [128/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29882\n",
      "\tStep [50/196]\t Loss: 4.29916\n",
      "\tStep [100/196]\t Loss: 4.29637\n",
      "\tStep [150/196]\t Loss: 4.30359\n",
      "\n",
      "\tTraining Loss: 4.292395652556906\n",
      "\tTime Taken: 3.520316843191783 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [129/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29912\n",
      "\tStep [50/196]\t Loss: 4.29578\n",
      "\tStep [100/196]\t Loss: 4.29631\n",
      "\tStep [150/196]\t Loss: 4.2999\n",
      "\n",
      "\tTraining Loss: 4.292438239467387\n",
      "\tTime Taken: 3.5437299966812135 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [130/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29766\n",
      "\tStep [50/196]\t Loss: 4.29946\n",
      "\tStep [100/196]\t Loss: 4.2955\n",
      "\tStep [150/196]\t Loss: 4.29447\n",
      "\n",
      "\tTraining Loss: 4.292255505007141\n",
      "\tTime Taken: 3.5311869978904724 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [131/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29546\n",
      "\tStep [50/196]\t Loss: 4.29759\n",
      "\tStep [100/196]\t Loss: 4.29991\n",
      "\tStep [150/196]\t Loss: 4.29806\n",
      "\n",
      "\tTraining Loss: 4.292132057705704\n",
      "\tTime Taken: 3.534453554948171 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [132/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30046\n",
      "\tStep [50/196]\t Loss: 4.29771\n",
      "\tStep [100/196]\t Loss: 4.29545\n",
      "\tStep [150/196]\t Loss: 4.29536\n",
      "\n",
      "\tTraining Loss: 4.291990446801088\n",
      "\tTime Taken: 3.5186204234759013 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [133/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29875\n",
      "\tStep [50/196]\t Loss: 4.29746\n",
      "\tStep [100/196]\t Loss: 4.29931\n",
      "\tStep [150/196]\t Loss: 4.29531\n",
      "\n",
      "\tTraining Loss: 4.2917675193475215\n",
      "\tTime Taken: 3.5250709215799967 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [134/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29608\n",
      "\tStep [50/196]\t Loss: 4.29973\n",
      "\tStep [100/196]\t Loss: 4.29843\n",
      "\tStep [150/196]\t Loss: 4.29729\n",
      "\n",
      "\tTraining Loss: 4.2917198526616\n",
      "\tTime Taken: 3.5324703296025595 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [135/250]\t\n",
      "\tStep [0/196]\t Loss: 4.30127\n",
      "\tStep [50/196]\t Loss: 4.30144\n",
      "\tStep [100/196]\t Loss: 4.30049\n",
      "\tStep [150/196]\t Loss: 4.29331\n",
      "\n",
      "\tTraining Loss: 4.291753277486684\n",
      "\tTime Taken: 3.5284036954243976 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [136/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29845\n",
      "\tStep [50/196]\t Loss: 4.29636\n",
      "\tStep [100/196]\t Loss: 4.29343\n",
      "\tStep [150/196]\t Loss: 4.29465\n",
      "\n",
      "\tTraining Loss: 4.291397375719888\n",
      "\tTime Taken: 3.5223704020182294 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [137/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29937\n",
      "\tStep [50/196]\t Loss: 4.29815\n",
      "\tStep [100/196]\t Loss: 4.29787\n",
      "\tStep [150/196]\t Loss: 4.30131\n",
      "\n",
      "\tTraining Loss: 4.291273373730329\n",
      "\tTime Taken: 3.5370036085446674 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [138/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29845\n",
      "\tStep [50/196]\t Loss: 4.2947\n",
      "\tStep [100/196]\t Loss: 4.29619\n",
      "\tStep [150/196]\t Loss: 4.29682\n",
      "\n",
      "\tTraining Loss: 4.290959500536626\n",
      "\tTime Taken: 3.5289392431577045 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [139/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29583\n",
      "\tStep [50/196]\t Loss: 4.29676\n",
      "\tStep [100/196]\t Loss: 4.2943\n",
      "\tStep [150/196]\t Loss: 4.29732\n",
      "\n",
      "\tTraining Loss: 4.290952756696818\n",
      "\tTime Taken: 3.5201062321662904 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [140/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29387\n",
      "\tStep [50/196]\t Loss: 4.29714\n",
      "\tStep [100/196]\t Loss: 4.29275\n",
      "\tStep [150/196]\t Loss: 4.29297\n",
      "\n",
      "\tTraining Loss: 4.290558788241173\n",
      "\tTime Taken: 3.5230561892191568 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [141/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29343\n",
      "\tStep [50/196]\t Loss: 4.29814\n",
      "\tStep [100/196]\t Loss: 4.29495\n",
      "\tStep [150/196]\t Loss: 4.29557\n",
      "\n",
      "\tTraining Loss: 4.2908090717938485\n",
      "\tTime Taken: 3.5270228306452434 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [142/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29452\n",
      "\tStep [50/196]\t Loss: 4.29943\n",
      "\tStep [100/196]\t Loss: 4.2973\n",
      "\tStep [150/196]\t Loss: 4.29914\n",
      "\n",
      "\tTraining Loss: 4.290993397333184\n",
      "\tTime Taken: 3.5373560945192972 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [143/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29397\n",
      "\tStep [50/196]\t Loss: 4.29672\n",
      "\tStep [100/196]\t Loss: 4.2996\n",
      "\tStep [150/196]\t Loss: 4.29787\n",
      "\n",
      "\tTraining Loss: 4.2904781103134155\n",
      "\tTime Taken: 3.521106227238973 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [144/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29862\n",
      "\tStep [50/196]\t Loss: 4.29714\n",
      "\tStep [100/196]\t Loss: 4.29863\n",
      "\tStep [150/196]\t Loss: 4.29617\n",
      "\n",
      "\tTraining Loss: 4.290323820649361\n",
      "\tTime Taken: 3.5151062607765198 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [145/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29537\n",
      "\tStep [50/196]\t Loss: 4.29445\n",
      "\tStep [100/196]\t Loss: 4.29753\n",
      "\tStep [150/196]\t Loss: 4.29562\n",
      "\n",
      "\tTraining Loss: 4.290287696585363\n",
      "\tTime Taken: 3.5345561305681863 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [146/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29886\n",
      "\tStep [50/196]\t Loss: 4.29699\n",
      "\tStep [100/196]\t Loss: 4.29809\n",
      "\tStep [150/196]\t Loss: 4.29651\n",
      "\n",
      "\tTraining Loss: 4.290047887636691\n",
      "\tTime Taken: 3.5346227924029034 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [147/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29804\n",
      "\tStep [50/196]\t Loss: 4.30294\n",
      "\tStep [100/196]\t Loss: 4.29686\n",
      "\tStep [150/196]\t Loss: 4.29874\n",
      "\n",
      "\tTraining Loss: 4.2896048086030145\n",
      "\tTime Taken: 3.5301897287368775 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [148/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29598\n",
      "\tStep [50/196]\t Loss: 4.29301\n",
      "\tStep [100/196]\t Loss: 4.29717\n",
      "\tStep [150/196]\t Loss: 4.29933\n",
      "\n",
      "\tTraining Loss: 4.289565229902462\n",
      "\tTime Taken: 3.5169739802678426 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [149/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29948\n",
      "\tStep [50/196]\t Loss: 4.2941\n",
      "\tStep [100/196]\t Loss: 4.29498\n",
      "\tStep [150/196]\t Loss: 4.29975\n",
      "\n",
      "\tTraining Loss: 4.289668819125818\n",
      "\tTime Taken: 3.530154013633728 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [150/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29385\n",
      "\tStep [50/196]\t Loss: 4.29556\n",
      "\tStep [100/196]\t Loss: 4.2964\n",
      "\tStep [150/196]\t Loss: 4.29415\n",
      "\n",
      "\tTraining Loss: 4.289711227222365\n",
      "\tTime Taken: 3.533521513144175 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [151/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29384\n",
      "\tStep [50/196]\t Loss: 4.29556\n",
      "\tStep [100/196]\t Loss: 4.29423\n",
      "\tStep [150/196]\t Loss: 4.29642\n",
      "\n",
      "\tTraining Loss: 4.289573664567908\n",
      "\tTime Taken: 3.521273930867513 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [152/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29236\n",
      "\tStep [50/196]\t Loss: 4.29436\n",
      "\tStep [100/196]\t Loss: 4.29442\n",
      "\tStep [150/196]\t Loss: 4.29514\n",
      "\n",
      "\tTraining Loss: 4.289628870633184\n",
      "\tTime Taken: 3.5243239402770996 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [153/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29313\n",
      "\tStep [50/196]\t Loss: 4.29426\n",
      "\tStep [100/196]\t Loss: 4.29838\n",
      "\tStep [150/196]\t Loss: 4.29724\n",
      "\n",
      "\tTraining Loss: 4.2895642251384505\n",
      "\tTime Taken: 3.5261072476704913 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [154/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29182\n",
      "\tStep [50/196]\t Loss: 4.29524\n",
      "\tStep [100/196]\t Loss: 4.29446\n",
      "\tStep [150/196]\t Loss: 4.29696\n",
      "\n",
      "\tTraining Loss: 4.28930760646353\n",
      "\tTime Taken: 3.5322657386461893 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [155/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29381\n",
      "\tStep [50/196]\t Loss: 4.29606\n",
      "\tStep [100/196]\t Loss: 4.29201\n",
      "\tStep [150/196]\t Loss: 4.29007\n",
      "\n",
      "\tTraining Loss: 4.288844038029106\n",
      "\tTime Taken: 3.515990646680196 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [156/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29747\n",
      "\tStep [50/196]\t Loss: 4.29489\n",
      "\tStep [100/196]\t Loss: 4.2909\n",
      "\tStep [150/196]\t Loss: 4.2961\n",
      "\n",
      "\tTraining Loss: 4.288792877781148\n",
      "\tTime Taken: 3.5267072161038717 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [157/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29089\n",
      "\tStep [50/196]\t Loss: 4.29397\n",
      "\tStep [100/196]\t Loss: 4.29472\n",
      "\tStep [150/196]\t Loss: 4.29485\n",
      "\n",
      "\tTraining Loss: 4.288645929219771\n",
      "\tTime Taken: 3.521689462661743 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [158/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29232\n",
      "\tStep [50/196]\t Loss: 4.29377\n",
      "\tStep [100/196]\t Loss: 4.29857\n",
      "\tStep [150/196]\t Loss: 4.29736\n",
      "\n",
      "\tTraining Loss: 4.28877302213591\n",
      "\tTime Taken: 3.5228909452756247 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [159/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29475\n",
      "\tStep [50/196]\t Loss: 4.29219\n",
      "\tStep [100/196]\t Loss: 4.2924\n",
      "\tStep [150/196]\t Loss: 4.29616\n",
      "\n",
      "\tTraining Loss: 4.288580792290824\n",
      "\tTime Taken: 3.5258909106254577 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [160/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29462\n",
      "\tStep [50/196]\t Loss: 4.29797\n",
      "\tStep [100/196]\t Loss: 4.2952\n",
      "\tStep [150/196]\t Loss: 4.29805\n",
      "\n",
      "\tTraining Loss: 4.28855795884619\n",
      "\tTime Taken: 3.5276909430821735 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [161/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29472\n",
      "\tStep [50/196]\t Loss: 4.29939\n",
      "\tStep [100/196]\t Loss: 4.2958\n",
      "\tStep [150/196]\t Loss: 4.29879\n",
      "\n",
      "\tTraining Loss: 4.288521093981607\n",
      "\tTime Taken: 3.5220242937405906 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [162/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29076\n",
      "\tStep [50/196]\t Loss: 4.28941\n",
      "\tStep [100/196]\t Loss: 4.29323\n",
      "\tStep [150/196]\t Loss: 4.29132\n",
      "\n",
      "\tTraining Loss: 4.288370262603371\n",
      "\tTime Taken: 3.5144910256067914 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [163/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29134\n",
      "\tStep [50/196]\t Loss: 4.29297\n",
      "\tStep [100/196]\t Loss: 4.2914\n",
      "\tStep [150/196]\t Loss: 4.29144\n",
      "\n",
      "\tTraining Loss: 4.288039323018522\n",
      "\tTime Taken: 3.528057559331258 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [164/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29523\n",
      "\tStep [50/196]\t Loss: 4.29207\n",
      "\tStep [100/196]\t Loss: 4.29003\n",
      "\tStep [150/196]\t Loss: 4.29507\n",
      "\n",
      "\tTraining Loss: 4.288120263693284\n",
      "\tTime Taken: 3.526340933640798 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [165/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29215\n",
      "\tStep [50/196]\t Loss: 4.29048\n",
      "\tStep [100/196]\t Loss: 4.29387\n",
      "\tStep [150/196]\t Loss: 4.29355\n",
      "\n",
      "\tTraining Loss: 4.287777290052297\n",
      "\tTime Taken: 3.516574291388194 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [166/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29459\n",
      "\tStep [50/196]\t Loss: 4.29306\n",
      "\tStep [100/196]\t Loss: 4.29506\n",
      "\tStep [150/196]\t Loss: 4.29738\n",
      "\n",
      "\tTraining Loss: 4.287700150694166\n",
      "\tTime Taken: 3.520440963904063 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [167/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29235\n",
      "\tStep [50/196]\t Loss: 4.29256\n",
      "\tStep [100/196]\t Loss: 4.29477\n",
      "\tStep [150/196]\t Loss: 4.29242\n",
      "\n",
      "\tTraining Loss: 4.28757813876989\n",
      "\tTime Taken: 3.5294575532277426 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [168/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29076\n",
      "\tStep [50/196]\t Loss: 4.29174\n",
      "\tStep [100/196]\t Loss: 4.29089\n",
      "\tStep [150/196]\t Loss: 4.29194\n",
      "\n",
      "\tTraining Loss: 4.287412013326373\n",
      "\tTime Taken: 3.527656853199005 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [169/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29345\n",
      "\tStep [50/196]\t Loss: 4.29718\n",
      "\tStep [100/196]\t Loss: 4.29208\n",
      "\tStep [150/196]\t Loss: 4.29199\n",
      "\n",
      "\tTraining Loss: 4.2871168085506985\n",
      "\tTime Taken: 3.518574333190918 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [170/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29056\n",
      "\tStep [50/196]\t Loss: 4.2941\n",
      "\tStep [100/196]\t Loss: 4.29231\n",
      "\tStep [150/196]\t Loss: 4.28976\n",
      "\n",
      "\tTraining Loss: 4.287220759051187\n",
      "\tTime Taken: 3.5205076257387797 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [171/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29201\n",
      "\tStep [50/196]\t Loss: 4.29173\n",
      "\tStep [100/196]\t Loss: 4.29294\n",
      "\tStep [150/196]\t Loss: 4.29525\n",
      "\n",
      "\tTraining Loss: 4.287323687757764\n",
      "\tTime Taken: 3.5310742179552714 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [172/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28911\n",
      "\tStep [50/196]\t Loss: 4.29288\n",
      "\tStep [100/196]\t Loss: 4.28952\n",
      "\tStep [150/196]\t Loss: 4.29191\n",
      "\n",
      "\tTraining Loss: 4.287074995284178\n",
      "\tTime Taken: 3.5264336546262105 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [173/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29823\n",
      "\tStep [50/196]\t Loss: 4.29171\n",
      "\tStep [100/196]\t Loss: 4.2962\n",
      "\tStep [150/196]\t Loss: 4.29356\n",
      "\n",
      "\tTraining Loss: 4.287216603755951\n",
      "\tTime Taken: 3.644806722799937 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [174/250]\t\n",
      "\tStep [0/196]\t Loss: 4.293\n",
      "\tStep [50/196]\t Loss: 4.29596\n",
      "\tStep [100/196]\t Loss: 4.29469\n",
      "\tStep [150/196]\t Loss: 4.28831\n",
      "\n",
      "\tTraining Loss: 4.287156351975033\n",
      "\tTime Taken: 3.574840545654297 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [175/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29272\n",
      "\tStep [50/196]\t Loss: 4.28779\n",
      "\tStep [100/196]\t Loss: 4.29727\n",
      "\tStep [150/196]\t Loss: 4.29219\n",
      "\n",
      "\tTraining Loss: 4.2869481225402986\n",
      "\tTime Taken: 3.553324051698049 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [176/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29099\n",
      "\tStep [50/196]\t Loss: 4.28982\n",
      "\tStep [100/196]\t Loss: 4.28958\n",
      "\tStep [150/196]\t Loss: 4.29584\n",
      "\n",
      "\tTraining Loss: 4.286723184342287\n",
      "\tTime Taken: 3.5325075586636863 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [177/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29068\n",
      "\tStep [50/196]\t Loss: 4.29181\n",
      "\tStep [100/196]\t Loss: 4.29268\n",
      "\tStep [150/196]\t Loss: 4.28959\n",
      "\n",
      "\tTraining Loss: 4.286528477863389\n",
      "\tTime Taken: 3.5218576232592267 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [178/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29248\n",
      "\tStep [50/196]\t Loss: 4.29244\n",
      "\tStep [100/196]\t Loss: 4.29302\n",
      "\tStep [150/196]\t Loss: 4.29105\n",
      "\n",
      "\tTraining Loss: 4.286368022159654\n",
      "\tTime Taken: 3.527624221642812 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [179/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29099\n",
      "\tStep [50/196]\t Loss: 4.29345\n",
      "\tStep [100/196]\t Loss: 4.28914\n",
      "\tStep [150/196]\t Loss: 4.2948\n",
      "\n",
      "\tTraining Loss: 4.286356250850522\n",
      "\tTime Taken: 3.527007571856181 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [180/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29201\n",
      "\tStep [50/196]\t Loss: 4.29107\n",
      "\tStep [100/196]\t Loss: 4.29312\n",
      "\tStep [150/196]\t Loss: 4.29518\n",
      "\n",
      "\tTraining Loss: 4.286278146870282\n",
      "\tTime Taken: 3.5212743163108824 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [181/250]\t\n",
      "\tStep [0/196]\t Loss: 4.2956\n",
      "\tStep [50/196]\t Loss: 4.29264\n",
      "\tStep [100/196]\t Loss: 4.29302\n",
      "\tStep [150/196]\t Loss: 4.29003\n",
      "\n",
      "\tTraining Loss: 4.285864657285262\n",
      "\tTime Taken: 3.525424273808797 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [182/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29217\n",
      "\tStep [50/196]\t Loss: 4.28955\n",
      "\tStep [100/196]\t Loss: 4.28724\n",
      "\tStep [150/196]\t Loss: 4.29153\n",
      "\n",
      "\tTraining Loss: 4.285963395420386\n",
      "\tTime Taken: 3.535474197069804 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [183/250]\t\n",
      "\tStep [0/196]\t Loss: 4.2911\n",
      "\tStep [50/196]\t Loss: 4.2938\n",
      "\tStep [100/196]\t Loss: 4.29468\n",
      "\tStep [150/196]\t Loss: 4.29238\n",
      "\n",
      "\tTraining Loss: 4.286008148777242\n",
      "\tTime Taken: 3.5153743386268617 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [184/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29516\n",
      "\tStep [50/196]\t Loss: 4.29215\n",
      "\tStep [100/196]\t Loss: 4.28958\n",
      "\tStep [150/196]\t Loss: 4.29487\n",
      "\n",
      "\tTraining Loss: 4.286015100625097\n",
      "\tTime Taken: 3.5247575958569843 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [185/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29312\n",
      "\tStep [50/196]\t Loss: 4.28993\n",
      "\tStep [100/196]\t Loss: 4.29245\n",
      "\tStep [150/196]\t Loss: 4.29034\n",
      "\n",
      "\tTraining Loss: 4.285750565480213\n",
      "\tTime Taken: 3.5301575581232707 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [186/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28988\n",
      "\tStep [50/196]\t Loss: 4.29284\n",
      "\tStep [100/196]\t Loss: 4.29281\n",
      "\tStep [150/196]\t Loss: 4.29452\n",
      "\n",
      "\tTraining Loss: 4.285827139202429\n",
      "\tTime Taken: 3.520957358678182 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [187/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28962\n",
      "\tStep [50/196]\t Loss: 4.289\n",
      "\tStep [100/196]\t Loss: 4.29294\n",
      "\tStep [150/196]\t Loss: 4.28867\n",
      "\n",
      "\tTraining Loss: 4.285714057027077\n",
      "\tTime Taken: 3.5215075969696046 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [188/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29255\n",
      "\tStep [50/196]\t Loss: 4.29024\n",
      "\tStep [100/196]\t Loss: 4.29249\n",
      "\tStep [150/196]\t Loss: 4.29082\n",
      "\n",
      "\tTraining Loss: 4.285589272878608\n",
      "\tTime Taken: 3.608875783284505 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [189/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29546\n",
      "\tStep [50/196]\t Loss: 4.28794\n",
      "\tStep [100/196]\t Loss: 4.28667\n",
      "\tStep [150/196]\t Loss: 4.29073\n",
      "\n",
      "\tTraining Loss: 4.285454560299309\n",
      "\tTime Taken: 3.534540871779124 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [190/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29204\n",
      "\tStep [50/196]\t Loss: 4.28894\n",
      "\tStep [100/196]\t Loss: 4.29211\n",
      "\tStep [150/196]\t Loss: 4.29148\n",
      "\n",
      "\tTraining Loss: 4.285222416021386\n",
      "\tTime Taken: 3.5171576579411825 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [191/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29216\n",
      "\tStep [50/196]\t Loss: 4.29474\n",
      "\tStep [100/196]\t Loss: 4.2909\n",
      "\tStep [150/196]\t Loss: 4.28976\n",
      "\n",
      "\tTraining Loss: 4.28527170419693\n",
      "\tTime Taken: 3.523090934753418 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [192/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29294\n",
      "\tStep [50/196]\t Loss: 4.28995\n",
      "\tStep [100/196]\t Loss: 4.2897\n",
      "\tStep [150/196]\t Loss: 4.28979\n",
      "\n",
      "\tTraining Loss: 4.2851281993243155\n",
      "\tTime Taken: 3.5333574930826823 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [193/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28883\n",
      "\tStep [50/196]\t Loss: 4.28917\n",
      "\tStep [100/196]\t Loss: 4.29107\n",
      "\tStep [150/196]\t Loss: 4.29438\n",
      "\n",
      "\tTraining Loss: 4.285111084276316\n",
      "\tTime Taken: 3.547932517528534 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [194/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29\n",
      "\tStep [50/196]\t Loss: 4.29341\n",
      "\tStep [100/196]\t Loss: 4.29161\n",
      "\tStep [150/196]\t Loss: 4.29335\n",
      "\n",
      "\tTraining Loss: 4.284960749197979\n",
      "\tTime Taken: 3.517307662963867 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [195/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28855\n",
      "\tStep [50/196]\t Loss: 4.2888\n",
      "\tStep [100/196]\t Loss: 4.29069\n",
      "\tStep [150/196]\t Loss: 4.29142\n",
      "\n",
      "\tTraining Loss: 4.284900288192594\n",
      "\tTime Taken: 3.5231576124827066 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [196/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29088\n",
      "\tStep [50/196]\t Loss: 4.28878\n",
      "\tStep [100/196]\t Loss: 4.2916\n",
      "\tStep [150/196]\t Loss: 4.2945\n",
      "\n",
      "\tTraining Loss: 4.284942333795587\n",
      "\tTime Taken: 3.5291242162386576 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [197/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29674\n",
      "\tStep [50/196]\t Loss: 4.29073\n",
      "\tStep [100/196]\t Loss: 4.28656\n",
      "\tStep [150/196]\t Loss: 4.28848\n",
      "\n",
      "\tTraining Loss: 4.28457785625847\n",
      "\tTime Taken: 3.5259576042493186 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [198/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28939\n",
      "\tStep [50/196]\t Loss: 4.28804\n",
      "\tStep [100/196]\t Loss: 4.29074\n",
      "\tStep [150/196]\t Loss: 4.28842\n",
      "\n",
      "\tTraining Loss: 4.284637772307104\n",
      "\tTime Taken: 3.5211576342582704 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [199/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29199\n",
      "\tStep [50/196]\t Loss: 4.29133\n",
      "\tStep [100/196]\t Loss: 4.29304\n",
      "\tStep [150/196]\t Loss: 4.29014\n",
      "\n",
      "\tTraining Loss: 4.284351205339237\n",
      "\tTime Taken: 3.512591028213501 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [200/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28481\n",
      "\tStep [50/196]\t Loss: 4.28706\n",
      "\tStep [100/196]\t Loss: 4.29066\n",
      "\tStep [150/196]\t Loss: 4.28999\n",
      "\n",
      "\tTraining Loss: 4.284418665632909\n",
      "\tTime Taken: 3.5272909283638 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [201/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28818\n",
      "\tStep [50/196]\t Loss: 4.28947\n",
      "\tStep [100/196]\t Loss: 4.28863\n",
      "\tStep [150/196]\t Loss: 4.29103\n",
      "\n",
      "\tTraining Loss: 4.284413078609778\n",
      "\tTime Taken: 3.5284576137860615 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [202/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29416\n",
      "\tStep [50/196]\t Loss: 4.28945\n",
      "\tStep [100/196]\t Loss: 4.28964\n",
      "\tStep [150/196]\t Loss: 4.28996\n",
      "\n",
      "\tTraining Loss: 4.284039016889066\n",
      "\tTime Taken: 3.520107638835907 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [203/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29159\n",
      "\tStep [50/196]\t Loss: 4.28721\n",
      "\tStep [100/196]\t Loss: 4.28892\n",
      "\tStep [150/196]\t Loss: 4.28886\n",
      "\n",
      "\tTraining Loss: 4.283945700343774\n",
      "\tTime Taken: 3.5228576064109802 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [204/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28748\n",
      "\tStep [50/196]\t Loss: 4.29242\n",
      "\tStep [100/196]\t Loss: 4.29044\n",
      "\tStep [150/196]\t Loss: 4.28901\n",
      "\n",
      "\tTraining Loss: 4.284347970874942\n",
      "\tTime Taken: 3.52282429933548 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [205/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29119\n",
      "\tStep [50/196]\t Loss: 4.29092\n",
      "\tStep [100/196]\t Loss: 4.28941\n",
      "\tStep [150/196]\t Loss: 4.28965\n",
      "\n",
      "\tTraining Loss: 4.284072153422297\n",
      "\tTime Taken: 3.5259095350901286 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [206/250]\t\n",
      "\tStep [0/196]\t Loss: 4.2872\n",
      "\tStep [50/196]\t Loss: 4.28619\n",
      "\tStep [100/196]\t Loss: 4.29018\n",
      "\tStep [150/196]\t Loss: 4.29727\n",
      "\n",
      "\tTraining Loss: 4.284043567521231\n",
      "\tTime Taken: 3.422524607181549 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [207/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29081\n",
      "\tStep [50/196]\t Loss: 4.28859\n",
      "\tStep [100/196]\t Loss: 4.29087\n",
      "\tStep [150/196]\t Loss: 4.29267\n",
      "\n",
      "\tTraining Loss: 4.283730967920654\n",
      "\tTime Taken: 3.415092432498932 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [208/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28883\n",
      "\tStep [50/196]\t Loss: 4.29012\n",
      "\tStep [100/196]\t Loss: 4.29221\n",
      "\tStep [150/196]\t Loss: 4.28906\n",
      "\n",
      "\tTraining Loss: 4.283596906126762\n",
      "\tTime Taken: 3.4106924533843994 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [209/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28953\n",
      "\tStep [50/196]\t Loss: 4.2906\n",
      "\tStep [100/196]\t Loss: 4.28796\n",
      "\tStep [150/196]\t Loss: 4.29025\n",
      "\n",
      "\tTraining Loss: 4.28350801492224\n",
      "\tTime Taken: 3.4222590486208597 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [210/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28909\n",
      "\tStep [50/196]\t Loss: 4.2894\n",
      "\tStep [100/196]\t Loss: 4.2902\n",
      "\tStep [150/196]\t Loss: 4.28595\n",
      "\n",
      "\tTraining Loss: 4.283728610496132\n",
      "\tTime Taken: 3.409542465209961 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [211/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28983\n",
      "\tStep [50/196]\t Loss: 4.28853\n",
      "\tStep [100/196]\t Loss: 4.28897\n",
      "\tStep [150/196]\t Loss: 4.29259\n",
      "\n",
      "\tTraining Loss: 4.283444380273624\n",
      "\tTime Taken: 3.4017925024032594 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [212/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29006\n",
      "\tStep [50/196]\t Loss: 4.28835\n",
      "\tStep [100/196]\t Loss: 4.28881\n",
      "\tStep [150/196]\t Loss: 4.28874\n",
      "\n",
      "\tTraining Loss: 4.283556336042833\n",
      "\tTime Taken: 3.4188757220904034 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [213/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28601\n",
      "\tStep [50/196]\t Loss: 4.28779\n",
      "\tStep [100/196]\t Loss: 4.28895\n",
      "\tStep [150/196]\t Loss: 4.28686\n",
      "\n",
      "\tTraining Loss: 4.283289198972741\n",
      "\tTime Taken: 3.432325621445974 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [214/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29169\n",
      "\tStep [50/196]\t Loss: 4.29039\n",
      "\tStep [100/196]\t Loss: 4.29487\n",
      "\tStep [150/196]\t Loss: 4.28751\n",
      "\n",
      "\tTraining Loss: 4.283226218758797\n",
      "\tTime Taken: 3.4193841735521953 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [215/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28782\n",
      "\tStep [50/196]\t Loss: 4.28798\n",
      "\tStep [100/196]\t Loss: 4.28766\n",
      "\tStep [150/196]\t Loss: 4.28668\n",
      "\n",
      "\tTraining Loss: 4.283098385042074\n",
      "\tTime Taken: 3.4086424589157103 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [216/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28878\n",
      "\tStep [50/196]\t Loss: 4.28884\n",
      "\tStep [100/196]\t Loss: 4.28991\n",
      "\tStep [150/196]\t Loss: 4.29039\n",
      "\n",
      "\tTraining Loss: 4.283452514483004\n",
      "\tTime Taken: 3.402619973818461 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [217/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28917\n",
      "\tStep [50/196]\t Loss: 4.28685\n",
      "\tStep [100/196]\t Loss: 4.28588\n",
      "\tStep [150/196]\t Loss: 4.28853\n",
      "\n",
      "\tTraining Loss: 4.282868010657174\n",
      "\tTime Taken: 3.4250916878382367 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [218/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28976\n",
      "\tStep [50/196]\t Loss: 4.28721\n",
      "\tStep [100/196]\t Loss: 4.28932\n",
      "\tStep [150/196]\t Loss: 4.28751\n",
      "\n",
      "\tTraining Loss: 4.2828337087923165\n",
      "\tTime Taken: 3.4203250368436175 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [219/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29042\n",
      "\tStep [50/196]\t Loss: 4.28822\n",
      "\tStep [100/196]\t Loss: 4.29186\n",
      "\tStep [150/196]\t Loss: 4.28865\n",
      "\n",
      "\tTraining Loss: 4.282739645364333\n",
      "\tTime Taken: 3.412841761112213 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [220/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28745\n",
      "\tStep [50/196]\t Loss: 4.28421\n",
      "\tStep [100/196]\t Loss: 4.28886\n",
      "\tStep [150/196]\t Loss: 4.28762\n",
      "\n",
      "\tTraining Loss: 4.282623439419027\n",
      "\tTime Taken: 3.4063751379648846 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [221/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28798\n",
      "\tStep [50/196]\t Loss: 4.28793\n",
      "\tStep [100/196]\t Loss: 4.28753\n",
      "\tStep [150/196]\t Loss: 4.28792\n",
      "\n",
      "\tTraining Loss: 4.282928492341723\n",
      "\tTime Taken: 3.430091615517934 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [222/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28621\n",
      "\tStep [50/196]\t Loss: 4.28828\n",
      "\tStep [100/196]\t Loss: 4.28728\n",
      "\tStep [150/196]\t Loss: 4.29135\n",
      "\n",
      "\tTraining Loss: 4.282648129122598\n",
      "\tTime Taken: 3.41328356663386 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [223/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28562\n",
      "\tStep [50/196]\t Loss: 4.29008\n",
      "\tStep [100/196]\t Loss: 4.28813\n",
      "\tStep [150/196]\t Loss: 4.28685\n",
      "\n",
      "\tTraining Loss: 4.282491702206281\n",
      "\tTime Taken: 3.407508452733358 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [224/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28729\n",
      "\tStep [50/196]\t Loss: 4.28638\n",
      "\tStep [100/196]\t Loss: 4.2918\n",
      "\tStep [150/196]\t Loss: 4.28816\n",
      "\n",
      "\tTraining Loss: 4.282466047880601\n",
      "\tTime Taken: 3.426391625404358 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [225/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28518\n",
      "\tStep [50/196]\t Loss: 4.2862\n",
      "\tStep [100/196]\t Loss: 4.2868\n",
      "\tStep [150/196]\t Loss: 4.28739\n",
      "\n",
      "\tTraining Loss: 4.282674109449192\n",
      "\tTime Taken: 3.4147340575853984 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [226/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28722\n",
      "\tStep [50/196]\t Loss: 4.28933\n",
      "\tStep [100/196]\t Loss: 4.29188\n",
      "\tStep [150/196]\t Loss: 4.28805\n",
      "\n",
      "\tTraining Loss: 4.282398708012639\n",
      "\tTime Taken: 3.4086591521898906 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [227/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28702\n",
      "\tStep [50/196]\t Loss: 4.28625\n",
      "\tStep [100/196]\t Loss: 4.28893\n",
      "\tStep [150/196]\t Loss: 4.28778\n",
      "\n",
      "\tTraining Loss: 4.2824280164679704\n",
      "\tTime Taken: 3.4108091235160827 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [228/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28906\n",
      "\tStep [50/196]\t Loss: 4.28908\n",
      "\tStep [100/196]\t Loss: 4.28682\n",
      "\tStep [150/196]\t Loss: 4.28715\n",
      "\n",
      "\tTraining Loss: 4.282004931751563\n",
      "\tTime Taken: 3.424309019247691 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [229/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28718\n",
      "\tStep [50/196]\t Loss: 4.28427\n",
      "\tStep [100/196]\t Loss: 4.28773\n",
      "\tStep [150/196]\t Loss: 4.28882\n",
      "\n",
      "\tTraining Loss: 4.281805025071514\n",
      "\tTime Taken: 3.427925678094228 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [230/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28889\n",
      "\tStep [50/196]\t Loss: 4.28439\n",
      "\tStep [100/196]\t Loss: 4.291\n",
      "\tStep [150/196]\t Loss: 4.28645\n",
      "\n",
      "\tTraining Loss: 4.281929010031175\n",
      "\tTime Taken: 3.411770526568095 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [231/250]\t\n",
      "\tStep [0/196]\t Loss: 4.2876\n",
      "\tStep [50/196]\t Loss: 4.28659\n",
      "\tStep [100/196]\t Loss: 4.28602\n",
      "\tStep [150/196]\t Loss: 4.28854\n",
      "\n",
      "\tTraining Loss: 4.281701165802625\n",
      "\tTime Taken: 3.394086603323619 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [232/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28899\n",
      "\tStep [50/196]\t Loss: 4.28773\n",
      "\tStep [100/196]\t Loss: 4.29037\n",
      "\tStep [150/196]\t Loss: 4.28542\n",
      "\n",
      "\tTraining Loss: 4.281820687712456\n",
      "\tTime Taken: 3.4175590872764587 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [233/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28598\n",
      "\tStep [50/196]\t Loss: 4.28524\n",
      "\tStep [100/196]\t Loss: 4.28725\n",
      "\tStep [150/196]\t Loss: 4.28517\n",
      "\n",
      "\tTraining Loss: 4.2816197470742825\n",
      "\tTime Taken: 3.432525646686554 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [234/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28533\n",
      "\tStep [50/196]\t Loss: 4.28613\n",
      "\tStep [100/196]\t Loss: 4.28627\n",
      "\tStep [150/196]\t Loss: 4.28657\n",
      "\n",
      "\tTraining Loss: 4.281598177491402\n",
      "\tTime Taken: 3.4189923882484434 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [235/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28469\n",
      "\tStep [50/196]\t Loss: 4.28675\n",
      "\tStep [100/196]\t Loss: 4.28655\n",
      "\tStep [150/196]\t Loss: 4.28505\n",
      "\n",
      "\tTraining Loss: 4.2816232588826395\n",
      "\tTime Taken: 3.3933423797289533 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [236/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28925\n",
      "\tStep [50/196]\t Loss: 4.28719\n",
      "\tStep [100/196]\t Loss: 4.28904\n",
      "\tStep [150/196]\t Loss: 4.28917\n",
      "\n",
      "\tTraining Loss: 4.281675752328367\n",
      "\tTime Taken: 3.4034223715464273 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [237/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28637\n",
      "\tStep [50/196]\t Loss: 4.29258\n",
      "\tStep [100/196]\t Loss: 4.29309\n",
      "\tStep [150/196]\t Loss: 4.2897\n",
      "\n",
      "\tTraining Loss: 4.281537782172768\n",
      "\tTime Taken: 3.397641869386037 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [238/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28512\n",
      "\tStep [50/196]\t Loss: 4.28866\n",
      "\tStep [100/196]\t Loss: 4.28725\n",
      "\tStep [150/196]\t Loss: 4.28177\n",
      "\n",
      "\tTraining Loss: 4.281213250695442\n",
      "\tTime Taken: 3.3982585112253827 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [239/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28938\n",
      "\tStep [50/196]\t Loss: 4.29028\n",
      "\tStep [100/196]\t Loss: 4.28716\n",
      "\tStep [150/196]\t Loss: 4.28669\n",
      "\n",
      "\tTraining Loss: 4.281694674978451\n",
      "\tTime Taken: 3.3937585512797037 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [240/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28892\n",
      "\tStep [50/196]\t Loss: 4.28644\n",
      "\tStep [100/196]\t Loss: 4.28486\n",
      "\tStep [150/196]\t Loss: 4.28942\n",
      "\n",
      "\tTraining Loss: 4.281424590519497\n",
      "\tTime Taken: 3.3976752122243243 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [241/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28936\n",
      "\tStep [50/196]\t Loss: 4.28766\n",
      "\tStep [100/196]\t Loss: 4.2835\n",
      "\tStep [150/196]\t Loss: 4.28877\n",
      "\n",
      "\tTraining Loss: 4.281222604975408\n",
      "\tTime Taken: 3.393773376941681 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [242/250]\t\n",
      "\tStep [0/196]\t Loss: 4.2879\n",
      "\tStep [50/196]\t Loss: 4.28685\n",
      "\tStep [100/196]\t Loss: 4.28835\n",
      "\tStep [150/196]\t Loss: 4.29077\n",
      "\n",
      "\tTraining Loss: 4.280831546199565\n",
      "\tTime Taken: 3.4077669938405353 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [243/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28432\n",
      "\tStep [50/196]\t Loss: 4.28554\n",
      "\tStep [100/196]\t Loss: 4.28937\n",
      "\tStep [150/196]\t Loss: 4.28843\n",
      "\n",
      "\tTraining Loss: 4.2808848047743036\n",
      "\tTime Taken: 3.391741931438446 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [244/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28996\n",
      "\tStep [50/196]\t Loss: 4.28833\n",
      "\tStep [100/196]\t Loss: 4.28662\n",
      "\tStep [150/196]\t Loss: 4.28496\n",
      "\n",
      "\tTraining Loss: 4.281139510018485\n",
      "\tTime Taken: 3.3925085226694742 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [245/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28874\n",
      "\tStep [50/196]\t Loss: 4.28549\n",
      "\tStep [100/196]\t Loss: 4.28238\n",
      "\tStep [150/196]\t Loss: 4.28724\n",
      "\n",
      "\tTraining Loss: 4.280885336350422\n",
      "\tTime Taken: 3.3949918786684674 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [246/250]\t\n",
      "\tStep [0/196]\t Loss: 4.2865\n",
      "\tStep [50/196]\t Loss: 4.29012\n",
      "\tStep [100/196]\t Loss: 4.29145\n",
      "\tStep [150/196]\t Loss: 4.28802\n",
      "\n",
      "\tTraining Loss: 4.280794338304169\n",
      "\tTime Taken: 3.399525161584218 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [247/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28944\n",
      "\tStep [50/196]\t Loss: 4.28436\n",
      "\tStep [100/196]\t Loss: 4.28455\n",
      "\tStep [150/196]\t Loss: 4.28314\n",
      "\n",
      "\tTraining Loss: 4.280845851314311\n",
      "\tTime Taken: 3.397858989238739 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [248/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28517\n",
      "\tStep [50/196]\t Loss: 4.28792\n",
      "\tStep [100/196]\t Loss: 4.28515\n",
      "\tStep [150/196]\t Loss: 4.28612\n",
      "\n",
      "\tTraining Loss: 4.28066164984995\n",
      "\tTime Taken: 3.399691851933797 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [249/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28532\n",
      "\tStep [50/196]\t Loss: 4.28539\n",
      "\tStep [100/196]\t Loss: 4.28665\n",
      "\tStep [150/196]\t Loss: 4.28492\n",
      "\n",
      "\tTraining Loss: 4.280545025455709\n",
      "\tTime Taken: 3.3951918721199035 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "Epoch [250/250]\t\n",
      "\tStep [0/196]\t Loss: 4.29002\n",
      "\tStep [50/196]\t Loss: 4.28872\n",
      "\tStep [100/196]\t Loss: 4.28554\n",
      "\tStep [150/196]\t Loss: 4.28312\n",
      "\n",
      "\tTraining Loss: 4.2806640656626955\n",
      "\tTime Taken: 3.3987585147221884 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig0-SimCLR-NEW.pt\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(start_epoch, epochs+1):\n",
    "    print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "    stime = time.time()\n",
    "\n",
    "    model.train()\n",
    "    loss = train(dataloader, model, criterion, optimizer)\n",
    "    losses.append(loss)\n",
    "\n",
    "    if epoch <= 10:\n",
    "        warmupscheduler.step()\n",
    "    if epoch > 10:\n",
    "        mainscheduler.step()\n",
    "    \n",
    "    print()\n",
    "    print(f\"\\tTraining Loss: {loss}\")\n",
    "    time_taken = (time.time()-stime)/60\n",
    "    print(f\"\\tTime Taken: {time_taken} minutes\")\n",
    "\n",
    "    save_model(model, optimizer, mainscheduler, epoch, f\"{DATASET}-SimCLR-NEW.pt\")\n",
    "\n",
    "## end training\n",
    "save_model(model, optimizer, mainscheduler, epochs, f\"{DATASET}-SimCLR-NEW.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
