{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from datasets import prepare_poison_dataset\n",
    "from simclr import SimClrBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "epochs = 250\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"sig1\"\n",
    "\n",
    "LOAD_CHECKPOINT = True\n",
    "CHECKPOINT_NAME = f\"{DATASET}-SimCLR-NEW.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poison dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poison_dataset, poison_indices, target_class, dataset = prepare_poison_dataset(DATASET, train=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimCLR data augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrastiveDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, s: int = 0.5):\n",
    "        self.original_dataset = original_dataset\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomResizedCrop(32,(0.8,1.0),antialias=False),\n",
    "            transforms.Compose([transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)], p = 0.8),\n",
    "                transforms.RandomGrayscale(p=0.2)]),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = self.original_dataset[index][0]\n",
    "        augmented_img_1 = self.transforms(img)\n",
    "        augmented_img_2 = self.transforms(img)\n",
    "        return augmented_img_1, augmented_img_2\n",
    "    \n",
    "constrastive_dataset = ConstrastiveDataset(poison_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimCLR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Luka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = SimClrBackbone()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LARS optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "import re\n",
    "\n",
    "EETA_DEFAULT = 0.001\n",
    "\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    \"\"\"\n",
    "    Layer-wise Adaptive Rate Scaling for large batch training.\n",
    "    Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
    "    I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=required,\n",
    "        momentum=0.9,\n",
    "        use_nesterov=False,\n",
    "        weight_decay=0.0,\n",
    "        exclude_from_weight_decay=None,\n",
    "        exclude_from_layer_adaptation=None,\n",
    "        classic_momentum=True,\n",
    "        eeta=EETA_DEFAULT,\n",
    "    ):\n",
    "        \"\"\"Constructs a LARSOptimizer.\n",
    "        Args:\n",
    "        lr: A `float` for learning rate.\n",
    "        momentum: A `float` for momentum.\n",
    "        use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
    "        weight_decay: A `float` for weight decay.\n",
    "        exclude_from_weight_decay: A list of `string` for variable screening, if\n",
    "            any of the string appears in a variable's name, the variable will be\n",
    "            excluded for computing weight decay. For example, one could specify\n",
    "            the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
    "            from weight decay.\n",
    "        exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
    "            for layer adaptation. If it is None, it will be defaulted the same as\n",
    "            exclude_from_weight_decay.\n",
    "        classic_momentum: A `boolean` for whether to use classic (or popular)\n",
    "            momentum. The learning rate is applied during momeuntum update in\n",
    "            classic momentum, but after momentum for popular momentum.\n",
    "        eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
    "        name: The name for the scope.\n",
    "        \"\"\"\n",
    "\n",
    "        self.epoch = 0\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            use_nesterov=use_nesterov,\n",
    "            weight_decay=weight_decay,\n",
    "            exclude_from_weight_decay=exclude_from_weight_decay,\n",
    "            exclude_from_layer_adaptation=exclude_from_layer_adaptation,\n",
    "            classic_momentum=classic_momentum,\n",
    "            eeta=eeta,\n",
    "        )\n",
    "\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.use_nesterov = use_nesterov\n",
    "        self.classic_momentum = classic_momentum\n",
    "        self.eeta = eeta\n",
    "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
    "        # arg is None.\n",
    "        if exclude_from_layer_adaptation:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
    "        else:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
    "\n",
    "    def step(self, epoch=None, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "            eeta = group[\"eeta\"]\n",
    "            lr = group[\"lr\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param = p.data\n",
    "                grad = p.grad.data\n",
    "\n",
    "                param_state = self.state[p]\n",
    "\n",
    "                # TODO: get param names\n",
    "                # if self._use_weight_decay(param_name):\n",
    "                grad += self.weight_decay * param\n",
    "\n",
    "                if self.classic_momentum:\n",
    "                    trust_ratio = 1.0\n",
    "\n",
    "                    # TODO: get param names\n",
    "                    # if self._do_layer_adaptation(param_name):\n",
    "                    w_norm = torch.norm(param)\n",
    "                    g_norm = torch.norm(grad)\n",
    "\n",
    "                    device = g_norm.get_device()\n",
    "                    trust_ratio = torch.where(\n",
    "                        w_norm.gt(0),\n",
    "                        torch.where(\n",
    "                            g_norm.gt(0),\n",
    "                            (self.eeta * w_norm / g_norm),\n",
    "                            torch.Tensor([1.0]).to(device),\n",
    "                        ),\n",
    "                        torch.Tensor([1.0]).to(device),\n",
    "                    ).item()\n",
    "\n",
    "                    scaled_lr = lr * trust_ratio\n",
    "                    if \"momentum_buffer\" not in param_state:\n",
    "                        next_v = param_state[\"momentum_buffer\"] = torch.zeros_like(\n",
    "                            p.data\n",
    "                        )\n",
    "                    else:\n",
    "                        next_v = param_state[\"momentum_buffer\"]\n",
    "\n",
    "                    next_v.mul_(momentum).add_(scaled_lr, grad)\n",
    "                    if self.use_nesterov:\n",
    "                        update = (self.momentum * next_v) + (scaled_lr * grad)\n",
    "                    else:\n",
    "                        update = next_v\n",
    "\n",
    "                    p.data.add_(-update)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _use_weight_decay(self, param_name):\n",
    "        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "        if not self.weight_decay:\n",
    "            return False\n",
    "        if self.exclude_from_weight_decay:\n",
    "            for r in self.exclude_from_weight_decay:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def _do_layer_adaptation(self, param_name):\n",
    "        \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
    "        if self.exclude_from_layer_adaptation:\n",
    "            for r in self.exclude_from_layer_adaptation:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimCLR contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(SimCLR_Loss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "            \n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        We do not sample negative examples explicitly.\n",
    "        Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N − 1) augmented examples within a minibatch as negative examples.\n",
    "        \"\"\"\n",
    "        batch_size = z_i.shape[0]\n",
    "        mask = self.mask_correlated_samples(batch_size)\n",
    "\n",
    "        N = 2 * batch_size #* self.world_size\n",
    "        \n",
    "        #z_i_ = z_i / torch.sqrt(torch.sum(torch.square(z_i),dim = 1, keepdim = True))\n",
    "        #z_j_ = z_j / torch.sqrt(torch.sum(torch.square(z_j),dim = 1, keepdim = True))\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "        \n",
    "        #print(sim.shape)\n",
    "\n",
    "        sim_i_j = torch.diag(sim, batch_size)\n",
    "        sim_j_i = torch.diag(sim, batch_size)\n",
    "        \n",
    "        \n",
    "        # We have 2N samples, but with Distributed training every GPU gets N examples too, resulting in: 2xNxN\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[mask].reshape(N, -1)\n",
    "        \n",
    "        \n",
    "        #SIMCLR\n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #.float()\n",
    "        #labels was torch.zeros(N)\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = LARS(\n",
    "    [params for params in model.parameters() if params.requires_grad],\n",
    "    lr=0.2,\n",
    "    weight_decay=1e-6,\n",
    "    exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    ")\n",
    "\n",
    "# \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (epoch+1)/10.0, verbose = False)\n",
    "mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, eta_min=0.05, last_epoch=-1, verbose = False)\n",
    "\n",
    "criterion = SimCLR_Loss(temperature=0.5)\n",
    "\n",
    "dataloader = DataLoader(constrastive_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "dataloader_no_contrastive = DataLoader(poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import copy\n",
    "\n",
    "def plot_features(model: nn.Module, dataloader: DataLoader, poison_indices: np.array, batches: int = None):\n",
    "    model.eval()\n",
    "\n",
    "    features = None\n",
    "    labels_poison = None\n",
    "    labels_true = None\n",
    "\n",
    "    for i, (img, labels_batch_poison, labels_batch_true) in enumerate(dataloader):\n",
    "\n",
    "        if batches is not None and i>=batches:\n",
    "            break\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            features_batch = model(img.to(device)).cpu().data.numpy()\n",
    "            \n",
    "        if features is None:\n",
    "            features = features_batch\n",
    "            labels_poison = labels_batch_poison\n",
    "            labels_true = labels_batch_true\n",
    "        else:\n",
    "            features = np.append(features, features_batch, axis=0)\n",
    "            labels_poison = np.append(labels_poison, labels_batch_poison, axis=0)\n",
    "            labels_true = np.append(labels_true, labels_batch_true, axis=0)\n",
    "    \n",
    "    n_neighbors = len(features)/100\n",
    "    alg = umap.UMAP(n_neighbors=n_neighbors)\n",
    "    #alg = TSNE(n_components = 2, perplexity = n_neighbors)\n",
    "    features_2d = alg.fit_transform(features)\n",
    "    num_classes = max(labels_batch_poison)\n",
    "\n",
    "    labels_10 = copy.deepcopy(labels_true)\n",
    "    labels_10[poison_indices[:len(features)]] = 10\n",
    "\n",
    "    for i in range(num_classes+1):\n",
    "        plt.scatter(features_2d[labels_10==i,1], features_2d[labels_10==i,0], s=7)\n",
    "    plt.scatter(features_2d[labels_10==10,1], features_2d[labels_10==10,0], c = \"black\", marker= \"x\", s=1)\n",
    "\n",
    "    plt.legend([str(i) for i in range(num_classes+1)] + [\"poison\"])\n",
    "    plt.show()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "#plot_features(model, dataloader_no_contrastive, poison_indices, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer):\n",
    "    loss_epoch = 0\n",
    "    \n",
    "    for step, (x_i, x_j) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x_i = x_i.to(device).float()\n",
    "        x_j = x_j.to(device).float()\n",
    "\n",
    "        # positive pair, with encoding\n",
    "        z_i = model(x_i)\n",
    "        z_j = model(x_j)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"\\tStep [{step}/{len(dataloader)}]\\t Loss: {round(loss.item(), 5)}\")\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "    return loss_epoch / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/new/', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "load_checkpoint = False\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/new', CHECKPOINT_NAME)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    mainscheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [226/250]\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_7644\\1065139194.py:129: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1519.)\n",
      "  next_v.mul_(momentum).add_(scaled_lr, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tStep [0/196]\t Loss: 4.27933\n",
      "\tStep [50/196]\t Loss: 4.28151\n",
      "\tStep [100/196]\t Loss: 4.28037\n",
      "\tStep [150/196]\t Loss: 4.28196\n",
      "\n",
      "\tTraining Loss: 4.275846385225957\n",
      "\tTime Taken: 3.4317378838857016 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [227/250]\t\n",
      "\tStep [0/196]\t Loss: 4.2819\n",
      "\tStep [50/196]\t Loss: 4.27958\n",
      "\tStep [100/196]\t Loss: 4.27981\n",
      "\tStep [150/196]\t Loss: 4.28191\n",
      "\n",
      "\tTraining Loss: 4.275451086005386\n",
      "\tTime Taken: 3.3820772687594096 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [228/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28406\n",
      "\tStep [50/196]\t Loss: 4.28207\n",
      "\tStep [100/196]\t Loss: 4.27878\n",
      "\tStep [150/196]\t Loss: 4.27935\n",
      "\n",
      "\tTraining Loss: 4.275589120631316\n",
      "\tTime Taken: 3.375327416261037 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [229/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28076\n",
      "\tStep [50/196]\t Loss: 4.28101\n",
      "\tStep [100/196]\t Loss: 4.28267\n",
      "\tStep [150/196]\t Loss: 4.28166\n",
      "\n",
      "\tTraining Loss: 4.2753126998336946\n",
      "\tTime Taken: 3.373110806941986 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [230/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28337\n",
      "\tStep [50/196]\t Loss: 4.28215\n",
      "\tStep [100/196]\t Loss: 4.27945\n",
      "\tStep [150/196]\t Loss: 4.28042\n",
      "\n",
      "\tTraining Loss: 4.275535618772312\n",
      "\tTime Taken: 3.3817368745803833 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [231/250]\t\n",
      "\tStep [0/196]\t Loss: 4.27961\n",
      "\tStep [50/196]\t Loss: 4.27907\n",
      "\tStep [100/196]\t Loss: 4.28327\n",
      "\tStep [150/196]\t Loss: 4.28149\n",
      "\n",
      "\tTraining Loss: 4.275476104142714\n",
      "\tTime Taken: 3.37829346259435 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [232/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28044\n",
      "\tStep [50/196]\t Loss: 4.28108\n",
      "\tStep [100/196]\t Loss: 4.28397\n",
      "\tStep [150/196]\t Loss: 4.28122\n",
      "\n",
      "\tTraining Loss: 4.275404934980432\n",
      "\tTime Taken: 3.3717100183169046 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [233/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28106\n",
      "\tStep [50/196]\t Loss: 4.28007\n",
      "\tStep [100/196]\t Loss: 4.28253\n",
      "\tStep [150/196]\t Loss: 4.27692\n",
      "\n",
      "\tTraining Loss: 4.275199565352226\n",
      "\tTime Taken: 3.382126533985138 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [234/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28167\n",
      "\tStep [50/196]\t Loss: 4.28274\n",
      "\tStep [100/196]\t Loss: 4.2814\n",
      "\tStep [150/196]\t Loss: 4.28038\n",
      "\n",
      "\tTraining Loss: 4.27495632974469\n",
      "\tTime Taken: 3.3721267302831013 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [235/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28103\n",
      "\tStep [50/196]\t Loss: 4.2799\n",
      "\tStep [100/196]\t Loss: 4.28309\n",
      "\tStep [150/196]\t Loss: 4.28202\n",
      "\n",
      "\tTraining Loss: 4.275036601387725\n",
      "\tTime Taken: 3.567288831869761 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [236/250]\t\n",
      "\tStep [0/196]\t Loss: 4.27767\n",
      "\tStep [50/196]\t Loss: 4.28363\n",
      "\tStep [100/196]\t Loss: 4.27993\n",
      "\tStep [150/196]\t Loss: 4.28475\n",
      "\n",
      "\tTraining Loss: 4.274822317824071\n",
      "\tTime Taken: 3.5302860299746195 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [237/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28057\n",
      "\tStep [50/196]\t Loss: 4.27856\n",
      "\tStep [100/196]\t Loss: 4.28144\n",
      "\tStep [150/196]\t Loss: 4.2811\n",
      "\n",
      "\tTraining Loss: 4.275103204104365\n",
      "\tTime Taken: 3.8029394586881002 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [238/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28042\n",
      "\tStep [50/196]\t Loss: 4.28583\n",
      "\tStep [100/196]\t Loss: 4.28102\n",
      "\tStep [150/196]\t Loss: 4.28166\n",
      "\n",
      "\tTraining Loss: 4.2750203049912745\n",
      "\tTime Taken: 3.5942545692125956 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [239/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28298\n",
      "\tStep [50/196]\t Loss: 4.2809\n",
      "\tStep [100/196]\t Loss: 4.27956\n",
      "\tStep [150/196]\t Loss: 4.28296\n",
      "\n",
      "\tTraining Loss: 4.2748814054897855\n",
      "\tTime Taken: 3.5824687679608664 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [240/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28229\n",
      "\tStep [50/196]\t Loss: 4.2827\n",
      "\tStep [100/196]\t Loss: 4.28219\n",
      "\tStep [150/196]\t Loss: 4.28012\n",
      "\n",
      "\tTraining Loss: 4.274850442701457\n",
      "\tTime Taken: 3.600425112247467 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [241/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28061\n",
      "\tStep [50/196]\t Loss: 4.28042\n",
      "\tStep [100/196]\t Loss: 4.27839\n",
      "\tStep [150/196]\t Loss: 4.28253\n",
      "\n",
      "\tTraining Loss: 4.274447849818638\n",
      "\tTime Taken: 4.1043230851491295 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [242/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28157\n",
      "\tStep [50/196]\t Loss: 4.28048\n",
      "\tStep [100/196]\t Loss: 4.28255\n",
      "\tStep [150/196]\t Loss: 4.28011\n",
      "\n",
      "\tTraining Loss: 4.274590960570744\n",
      "\tTime Taken: 4.749233249823252 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [243/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28209\n",
      "\tStep [50/196]\t Loss: 4.27973\n",
      "\tStep [100/196]\t Loss: 4.27976\n",
      "\tStep [150/196]\t Loss: 4.28065\n",
      "\n",
      "\tTraining Loss: 4.274632442970665\n",
      "\tTime Taken: 3.7939936995506285 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [244/250]\t\n",
      "\tStep [0/196]\t Loss: 4.27868\n",
      "\tStep [50/196]\t Loss: 4.28168\n",
      "\tStep [100/196]\t Loss: 4.27905\n",
      "\tStep [150/196]\t Loss: 4.28119\n",
      "\n",
      "\tTraining Loss: 4.274688116141728\n",
      "\tTime Taken: 3.76604913075765 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [245/250]\t\n",
      "\tStep [0/196]\t Loss: 4.27657\n",
      "\tStep [50/196]\t Loss: 4.27925\n",
      "\tStep [100/196]\t Loss: 4.27901\n",
      "\tStep [150/196]\t Loss: 4.27904\n",
      "\n",
      "\tTraining Loss: 4.27443078221107\n",
      "\tTime Taken: 4.0644261558850605 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [246/250]\t\n",
      "\tStep [0/196]\t Loss: 4.27717\n",
      "\tStep [50/196]\t Loss: 4.28165\n",
      "\tStep [100/196]\t Loss: 4.28263\n",
      "\tStep [150/196]\t Loss: 4.28033\n",
      "\n",
      "\tTraining Loss: 4.27429894890104\n",
      "\tTime Taken: 3.4441182971000672 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [247/250]\t\n",
      "\tStep [0/196]\t Loss: 4.28242\n",
      "\tStep [50/196]\t Loss: 4.27786\n",
      "\tStep [100/196]\t Loss: 4.28146\n",
      "\tStep [150/196]\t Loss: 4.28168\n",
      "\n",
      "\tTraining Loss: 4.274398897375379\n",
      "\tTime Taken: 3.425851809978485 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [248/250]\t\n",
      "\tStep [0/196]\t Loss: 4.27787\n",
      "\tStep [50/196]\t Loss: 4.28043\n",
      "\tStep [100/196]\t Loss: 4.27903\n",
      "\tStep [150/196]\t Loss: 4.27925\n",
      "\n",
      "\tTraining Loss: 4.27463810662834\n",
      "\tTime Taken: 3.417501942316691 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [249/250]\t\n",
      "\tStep [0/196]\t Loss: 4.2787\n",
      "\tStep [50/196]\t Loss: 4.27951\n",
      "\tStep [100/196]\t Loss: 4.27711\n",
      "\tStep [150/196]\t Loss: 4.28008\n",
      "\n",
      "\tTraining Loss: 4.274271667003632\n",
      "\tTime Taken: 3.4345438281695047 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "Epoch [250/250]\t\n",
      "\tStep [0/196]\t Loss: 4.27874\n",
      "\tStep [50/196]\t Loss: 4.28188\n",
      "\tStep [100/196]\t Loss: 4.27817\n",
      "\tStep [150/196]\t Loss: 4.28262\n",
      "\n",
      "\tTraining Loss: 4.274065912986289\n",
      "\tTime Taken: 3.440912103652954 minutes\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/new/sig1-SimCLR-NEW.pt\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(start_epoch, epochs+1):\n",
    "    print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "    stime = time.time()\n",
    "\n",
    "    model.train()\n",
    "    loss = train(dataloader, model, criterion, optimizer)\n",
    "    losses.append(loss)\n",
    "\n",
    "    if epoch <= 10:\n",
    "        warmupscheduler.step()\n",
    "    if epoch > 10:\n",
    "        mainscheduler.step()\n",
    "    \n",
    "    print()\n",
    "    print(f\"\\tTraining Loss: {loss}\")\n",
    "    time_taken = (time.time()-stime)/60\n",
    "    print(f\"\\tTime Taken: {time_taken} minutes\")\n",
    "\n",
    "    save_model(model, optimizer, mainscheduler, epoch, f\"{DATASET}-SimCLR-NEW.pt\")\n",
    "\n",
    "## end training\n",
    "save_model(model, optimizer, mainscheduler, epochs, f\"{DATASET}-SimCLR-NEW.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
