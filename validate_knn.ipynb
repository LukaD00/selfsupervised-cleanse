{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from math import ceil, floor\n",
    "\n",
    "import copy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import prepare_poison_dataset\n",
    "from simclr import SimClrBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SimCLR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_simclr(simclr_model_name: str) -> SimClrBackbone:\n",
    "    model = SimClrBackbone()\n",
    "    out = os.path.join('./saved_models/simclr/', simclr_model_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    epochs = checkpoint[\"epoch\"]\n",
    "\n",
    "    return model, epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract SimCLR features for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_simclr_features(model: SimClrBackbone, dataset: VisionDataset, layer: str = \"loss\"):\n",
    "\n",
    "    assert layer in [\"loss\", \"repr\"]\n",
    "\n",
    "    simclr_feature_size = 128 if layer == \"loss\" else 512\n",
    "    num_examples = len(dataset)\n",
    "\n",
    "    features = np.zeros((num_examples, simclr_feature_size))\n",
    "    labels_poison = np.zeros((num_examples))\n",
    "    labels_true = np.zeros((num_examples))\n",
    "\n",
    "    batch_size = 256\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    for i, (img, labels_batch_poison, labels_batch_true) in enumerate(dataloader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if layer == \"loss\":\n",
    "                features_batch = model(img.to(device)).cpu().data.numpy()\n",
    "            elif layer == \"repr\":\n",
    "                features_batch = model.forward_repr(img.to(device)).cpu().data.numpy()\n",
    "            \n",
    "        features[i*batch_size : i*batch_size+len(features_batch)] = features_batch\n",
    "        labels_poison[i*batch_size : i*batch_size+len(labels_batch_poison)] = labels_batch_poison.long()\n",
    "        labels_true[i*batch_size : i*batch_size+len(labels_batch_true)] = labels_batch_true.long()\n",
    "\n",
    "    labels_poison = labels_poison.astype(int)\n",
    "    labels_true = labels_true.astype(int)\n",
    "\n",
    "    return features, labels_poison, labels_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2-D features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "def calculate_features_2d(features: np.array, n_neighbors: int = 100, algorithm: str = \"umap\", min_dist: float = 0.1) -> np.array:\n",
    "    assert algorithm in [\"umap\", \"tsne\"]\n",
    "    \n",
    "    if algorithm == \"umap\":\n",
    "        alg = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "    elif algorithm == \"tsne\":\n",
    "        alg = TSNE(n_components = 2, perplexity = n_neighbors)\n",
    "    features_2d = alg.fit_transform(features)\n",
    "    return features_2d\n",
    "\n",
    "def plot_features_2d(features_2d: np.array, labels: np.array, poison_indices: np.array, legend: bool = True) -> None:\n",
    "    num_classes = int(max(labels).item())\n",
    "\n",
    "    # label poison examples as 10\n",
    "    labels_10 = copy.deepcopy(labels)\n",
    "    labels_10[poison_indices] = 10\n",
    "\n",
    "    for i in range(num_classes+1):\n",
    "        plt.scatter(features_2d[labels_10==i,1], features_2d[labels_10==i,0], s=7)\n",
    "    plt.scatter(features_2d[labels_10==10,1], features_2d[labels_10==10,0], c = \"black\", marker= \"x\", s=1)\n",
    "\n",
    "    if legend:\n",
    "        plt.legend([str(i) for i in range(num_classes+1)] + [\"poison\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_and_plot_features_2d(features: np.array, labels: np.array, poison_indices: np.array, subset_size: int = None, legend: bool = True) -> np.array:\n",
    "    # Plot only a subset\n",
    "    if subset_size is None:\n",
    "        subset_size = len(features)\n",
    "    features_subset = features[:subset_size]\n",
    "    labels_subset = labels[:subset_size]\n",
    "    poison_indices_subset = poison_indices[:subset_size]\n",
    "    \n",
    "    features_2d = calculate_features_2d(features_subset)\n",
    "    plot_features_2d(features_2d, labels_subset, poison_indices_subset, legend=legend)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions for all cleanses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluate_cleanse(poison_predicted: np.array, poison_indices: np.array, verbose: bool = True) -> float:\n",
    "\n",
    "    tp = (poison_indices & poison_predicted).sum()\n",
    "    fp = (np.invert(poison_indices) & poison_predicted).sum()\n",
    "    fn = (poison_indices & np.invert(poison_predicted)).sum()\n",
    "    tn = (np.invert(poison_indices) & np.invert(poison_predicted)).sum()\n",
    "\n",
    "    fnr = fn/(fn+tp) if fn+tp!=0 else 0\n",
    "    tnr = tn/(tn+fp) if tn+fp!=0 else 0\n",
    "    poison_rate = fn/(fn+tn) if fn+tn!=0 else 0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"{tp} \\t {fp}\")\n",
    "        print(f\"{fn} \\t {tn}\")\n",
    "        print(f\"Percentage of poisoned images (out of all poisoned) kept: {100*fnr: .2f}%\")\n",
    "        print(f\"Percentage of clean images (out of all clean) kept: {100*tnr: .2f}%\")\n",
    "        print(f\"Percentage of remaining poisoned images (out of all remaining): {100*poison_rate: .2f}%\")\n",
    "\n",
    "def evaluate_cleanse(poison_predicted: np.array, poison_indices: np.array, verbose: bool = True) -> float:\n",
    "\n",
    "    tp = (poison_indices & poison_predicted).sum()\n",
    "    fp = (np.invert(poison_indices) & poison_predicted).sum()\n",
    "    fn = (poison_indices & np.invert(poison_predicted)).sum()\n",
    "    tn = (np.invert(poison_indices) & np.invert(poison_predicted)).sum()\n",
    "\n",
    "    fnr = fn/(fn+tp) if fn+tp!=0 else 0\n",
    "    tnr = tn/(tn+fp) if tn+fp!=0 else 0\n",
    "    poison_rate = fn/(fn+tn) if fn+tn!=0 else 0\n",
    "\n",
    "    return poison_rate, fnr, tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predicted_indices(predicted_indices: np.array, save_name: str):\n",
    "    with open(f\"./cleansed_labels/{save_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(predicted_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_poisoned(values: np.array, poison_indices: np.array = None, is_integer: bool = False, bins_num: int = 100, separation_line: float = None) -> None:\n",
    "    if poison_indices is not None:\n",
    "        values_clean = values[np.invert(poison_indices)]\n",
    "        values_poisoned = values[poison_indices]\n",
    "    else:\n",
    "        values_clean = values[:]\n",
    "        values_poisoned = []\n",
    "\n",
    "    bins = np.linspace(floor(np.min(values)), ceil(np.max(values)), int(np.max(values)) if is_integer else bins_num)\n",
    "    plt.hist(values_clean, bins, alpha=0.5, label='clean')\n",
    "    plt.hist(values_poisoned, bins, alpha=0.5, label='poisoned')\n",
    "\n",
    "    if separation_line:\n",
    "        plt.axvline(separation_line, color='red', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Non-disruptive cleanse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn_cleanse(features: np.array, labels_poison: np.array, num_classes: int, n_neighbors: int = None) -> np.array:\n",
    "    \n",
    "    if n_neighbors is None:\n",
    "        examples_per_class = len(features) / num_classes\n",
    "        n_neighbors=int(examples_per_class/2)\n",
    "        \n",
    "    knn = KNeighborsClassifier(n_neighbors)\n",
    "    knn.fit(features, labels_poison)\n",
    "    labels_predicted = knn.predict(features)\n",
    "\n",
    "    return labels_predicted != labels_poison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyClassifier():\n",
    "\n",
    "    def __init__(self, t=1):\n",
    "        self.t = t\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.C = int(np.max(y))\n",
    "        self.Ic = {c:[i for i in range(len(y)) if y[i]==c] for c in range(self.C)}\n",
    "        \n",
    "    def predict_index(self, i):\n",
    "        # consider improving with numpy and batch\n",
    "\n",
    "        xi = self.X[i]\n",
    "\n",
    "        exp_all = np.exp([xi*self.X[k]/self.t for k in range(len(self.X))])\n",
    "        sum_exp_all_except_xi = np.sum([exp_all[k] for k in range(len(self.X)) if k!=i])\n",
    "        mean_exp_c = [np.mean([exp_all[k] for k in self.Ic[c] if k!=i]) for c in range(self.C)]\n",
    "    \n",
    "        Scs = mean_exp_c / sum_exp_all_except_xi\n",
    "        return np.argmax(Scs)\n",
    "\n",
    "    def predict(self):\n",
    "        predicted = np.zeros((len(self.X)))\n",
    "        for i in tqdm(range(len(self.X))):\n",
    "            predicted[i] = self.predict_index(i)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_cleanse(features: np.array, labels_poison: np.array, t: float = 10) -> np.array:\n",
    "    \n",
    "    # if DATASET == \"badnets\":\n",
    "    #     T = 100\n",
    "    # elif DATASET == \"wanet\":\n",
    "    #     T = 10\n",
    "    # elif DATASET == \"sig\":\n",
    "    #     T = 1\n",
    "    # else:\n",
    "    #     raise Exception(\"Invalid dataset\")\n",
    "\n",
    "    energy = EnergyClassifier(t=t)\n",
    "    energy.fit(features, labels_poison)\n",
    "    labels_predicted = energy.predict()\n",
    "\n",
    "    return labels_predicted != labels_poison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disruptive detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_cleanse(features: np.array, poison_indices: np.array) -> np.array:\n",
    "    centroid = np.sum(features, axis=0) / features.shape[0]\n",
    "    distances = np.linalg.norm(features - centroid, axis=1)\n",
    "    \n",
    "    plot_histogram_poisoned(distances, poison_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def gauss_cleanse(features: np.array, discard_percentage: float, poison_indices: np.array = None) -> np.array:\n",
    "    mean = np.mean(features, axis=0)\n",
    "    cov = np.cov(features, rowvar=0)\n",
    "\n",
    "    probabilities = multivariate_normal.pdf(features, mean=mean, cov=cov, allow_singular=True)\n",
    "    probabilities[probabilities <= 0] = 1e-100\n",
    "    probabilities = -np.log(probabilities)\n",
    "\n",
    "    discard_line = np.percentile(probabilities, (1-discard_percentage)*100)\n",
    "    plot_histogram_poisoned(probabilities, poison_indices, separation_line=discard_line)\n",
    "\n",
    "    predicted_poison_indices = probabilities > discard_line\n",
    "    return predicted_poison_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def kmeans_cleanse(features: np.array, means: int = 50) -> np.array:\n",
    "\tkmeans = KMeans(n_clusters=means, init=\"k-means++\")\n",
    "\tkmeans.fit(features)\n",
    "\t\n",
    "\tcentroid = np.mean(features, axis=0)\n",
    "\tcluster_center_distances = [euclidean(center, centroid) for center in kmeans.cluster_centers_]\n",
    "\tpoison_cluster_index = cluster_center_distances.index(max(cluster_center_distances))\n",
    "\n",
    "\tpredicted_poison_indices = kmeans.predict(features) == poison_cluster_index\n",
    "\treturn predicted_poison_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def kmeans_cleanse(features: np.array, means: int = 50, mode: str = \"both\") -> np.array:\n",
    "\n",
    "\tassert mode in [\"distance\", \"size\", \"both\"]\n",
    "\n",
    "\tkmeans = KMeans(n_clusters=means, init=\"k-means++\")\n",
    "\tkmeans.fit(features)\n",
    "\t\n",
    "\tif mode == \"distance\":\n",
    "\t\tcentroid = np.mean(features, axis=0)\n",
    "\t\tcluster_center_distances = [euclidean(center, centroid) for center in kmeans.cluster_centers_]\n",
    "\t\tpoison_cluster_index = cluster_center_distances.index(max(cluster_center_distances))\n",
    "\t\n",
    "\telif mode == \"size\":\n",
    "\t\tpredicted_cluster = kmeans.predict(features)\n",
    "\t\t_, counts = np.unique(predicted_cluster, return_counts=True)\n",
    "\t\tpoison_cluster_index = np.argmin(counts)\n",
    "\t\n",
    "\telif mode == \"both\":\n",
    "\t\tcentroid = np.mean(features, axis=0)\n",
    "\t\tcluster_center_distances = [euclidean(center, centroid) for center in kmeans.cluster_centers_]\n",
    "\t\tpoison_cluster_index_1 = cluster_center_distances.index(max(cluster_center_distances))\n",
    "\n",
    "\t\tpredicted_cluster = kmeans.predict(features)\n",
    "\t\t_, counts = np.unique(predicted_cluster, return_counts=True)\n",
    "\t\tpoison_cluster_index_2 = np.argmin(counts)\n",
    "\n",
    "\t\tif poison_cluster_index_1 != poison_cluster_index_2:\n",
    "\t\t\t# No poison detected\n",
    "\t\t\treturn np.zeros(features.shape[0]).astype(bool)\n",
    "\t\telse:\n",
    "\t\t\tpoison_cluster_index = poison_cluster_index_1\n",
    "\n",
    "\n",
    "\tpredicted_poison_indices = kmeans.predict(features) == poison_cluster_index\n",
    "\treturn predicted_poison_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poison reclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak binary classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoisonClassificationDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, poison_indices: np.array) -> None:\n",
    "        self.original_dataset = original_dataset\n",
    "        self.poison_indices = poison_indices\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.original_dataset)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        out = F.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNetBinary():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "class ConvolutionalBinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1568, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_binary_classifier(dataset: VisionDataset, predicted_poison_indices: np.array) -> nn.Module:\n",
    "    poison_classification_dataset = PoisonClassificationDataset(dataset, predicted_poison_indices)\n",
    "    # sampler for class imbalance\n",
    "    positives = sum([1 for _, target in poison_classification_dataset if target==1])\n",
    "    total = len(poison_classification_dataset)\n",
    "    positive_weight = 0.5 / positives\n",
    "    negative_weight = 0.5 / (total - positives)\n",
    "    weights = [positive_weight if target==1 else negative_weight for _, target in poison_classification_dataset]\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "    dataloader = DataLoader(poison_classification_dataset, batch_size=128, sampler=sampler)\n",
    "    model = ResNetBinary().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1-1e-4)\n",
    "    num_epochs = 10\n",
    "\n",
    "    for _ in tqdm(range(num_epochs)):\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.forward(inputs).squeeze(-1)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def binary_reclassification(dataset: VisionDataset, model: nn.Module):\n",
    "    predicted_poison_indices = np.zeros((len(dataset)))\n",
    "    batch_size = 128\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    for i, (inputs, _, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(inputs).squeeze(-1)\n",
    "            predictions = (logits>0.5).cpu().numpy()\n",
    "        predicted_poison_indices[i*batch_size : i*batch_size+len(predictions)] = predictions\n",
    "    return predicted_poison_indices==1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong multiclass classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, predicted_poison: np.array, transforms: torch.nn.Module = None):\n",
    "        self.data = [poison_dataset[i][0] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.labels = [poison_dataset[i][1] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transforms:\n",
    "            item = self.transforms(item)\n",
    "\n",
    "        return item, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiclass_classifier(dataset: VisionDataset, predicted_poison_indices: np.array) -> nn.Module:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    cleansed_dataset = CleansedDataset(dataset, predicted_poison_indices, transform_train)\n",
    "    dataloader = DataLoader(cleansed_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = ResNet18()\n",
    "    model.to(device)\n",
    "\n",
    "    epochs = 35\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100.*correct/total\n",
    "        scheduler.step()\n",
    "        print(epoch, train_loss, acc)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_reclassification(dataset: VisionDataset, model: nn.Module, true_labels: np.array):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_dataset = CleansedDataset(dataset, np.zeros(len(dataset)), transform_train)\n",
    "    \n",
    "    predicted_labels = np.zeros((len(dataset)))\n",
    "    batch_size = 128\n",
    "    dataloader = DataLoader(transform_dataset, batch_size=batch_size, shuffle=False)\n",
    "    for i, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(inputs)\n",
    "            predictions = torch.argmax(logits, 1).cpu().numpy()\n",
    "        predicted_labels[i*batch_size : i*batch_size+len(predictions)] = predictions\n",
    "    return predicted_labels!=true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing badnets TRAIN\n",
      "\tk = d/5000 = 10:\n",
      "\t\tpoison rate:  1.20\t( 1.31,  0.91,  1.39, )\n",
      "\t\tclean kept:   84.83\t( 84.37,  85.39,  84.74, )\n",
      "\tk = d/2500 = 20:\n",
      "\t\tpoison rate:  0.60\t( 0.63,  0.34,  0.82, )\n",
      "\t\tclean kept:   84.00\t( 83.40,  84.78,  83.81, )\n",
      "\tk = d/1000 = 50:\n",
      "\t\tpoison rate:  0.42\t( 0.45,  0.14,  0.67, )\n",
      "\t\tclean kept:   82.89\t( 82.27,  83.76,  82.63, )\n",
      "\tk = d/500 = 100:\n",
      "\t\tpoison rate:  0.39\t( 0.40,  0.12,  0.65, )\n",
      "\t\tclean kept:   82.24\t( 81.69,  83.04,  81.99, )\n",
      "\tk = d/100 = 500:\n",
      "\t\tpoison rate:  0.46\t( 0.47,  0.11,  0.80, )\n",
      "\t\tclean kept:   80.58\t( 79.94,  81.60,  80.21, )\n",
      "\tk = d/50 = 1000:\n",
      "\t\tpoison rate:  0.53\t( 0.57,  0.11,  0.92, )\n",
      "\t\tclean kept:   79.61\t( 78.84,  80.77,  79.22, )\n",
      "\tk = d/20 = 2500:\n",
      "\t\tpoison rate:  0.80\t( 0.83,  0.14,  1.43, )\n",
      "\t\tclean kept:   77.47\t( 76.47,  79.13,  76.82, )\n",
      "\tk = d/10 = 5000:\n",
      "\t\tpoison rate:  1.25\t( 1.41,  0.17,  2.18, )\n",
      "\t\tclean kept:   74.46\t( 73.26,  76.67,  73.46, )\n",
      "Testing badnets TEST\n",
      "\tk = d/5000 = 2:\n",
      "\t\tpoison rate:  11.07\t( 12.41,  10.97,  9.83, )\n",
      "\t\tclean kept:   80.30\t( 78.43,  80.90,  81.57, )\n",
      "\tk = d/2500 = 4:\n",
      "\t\tpoison rate:  4.50\t( 4.79,  4.51,  4.19, )\n",
      "\t\tclean kept:   82.91\t( 81.96,  83.19,  83.60, )\n",
      "\tk = d/1000 = 10:\n",
      "\t\tpoison rate:  1.41\t( 1.45,  1.17,  1.62, )\n",
      "\t\tclean kept:   82.98\t( 82.50,  83.41,  83.03, )\n",
      "\tk = d/500 = 20:\n",
      "\t\tpoison rate:  0.85\t( 0.93,  0.43,  1.19, )\n",
      "\t\tclean kept:   82.07\t( 81.60,  82.74,  81.87, )\n",
      "\tk = d/100 = 100:\n",
      "\t\tpoison rate:  0.63\t( 0.81,  0.14,  0.94, )\n",
      "\t\tclean kept:   80.03\t( 79.31,  80.87,  79.91, )\n",
      "\tk = d/50 = 200:\n",
      "\t\tpoison rate:  0.68\t( 0.92,  0.15,  0.98, )\n",
      "\t\tclean kept:   79.01\t( 78.10,  79.94,  78.99, )\n",
      "\tk = d/20 = 500:\n",
      "\t\tpoison rate:  0.93\t( 1.36,  0.16,  1.29, )\n",
      "\t\tclean kept:   76.26\t( 75.08,  77.81,  75.88, )\n",
      "\tk = d/10 = 1000:\n",
      "\t\tpoison rate:  1.45\t( 1.97,  0.21,  2.19, )\n",
      "\t\tclean kept:   72.66\t( 71.50,  74.40,  72.07, )\n",
      "Testing wanet TRAIN\n",
      "\tk = d/5000 = 10:\n",
      "\t\tpoison rate:  1.69\t( 1.70,  1.48,  1.89, )\n",
      "\t\tclean kept:   83.40\t( 83.45,  83.58,  83.16, )\n",
      "\tk = d/2500 = 20:\n",
      "\t\tpoison rate:  0.89\t( 0.87,  0.59,  1.21, )\n",
      "\t\tclean kept:   82.59\t( 82.67,  82.89,  82.21, )\n",
      "\tk = d/1000 = 50:\n",
      "\t\tpoison rate:  0.62\t( 0.65,  0.30,  0.93, )\n",
      "\t\tclean kept:   81.46\t( 81.56,  81.81,  81.00, )\n",
      "\tk = d/500 = 100:\n",
      "\t\tpoison rate:  0.57\t( 0.56,  0.22,  0.92, )\n",
      "\t\tclean kept:   80.72\t( 80.71,  81.11,  80.34, )\n",
      "\tk = d/100 = 500:\n",
      "\t\tpoison rate:  0.64\t( 0.64,  0.18,  1.09, )\n",
      "\t\tclean kept:   78.91\t( 78.99,  79.44,  78.30, )\n",
      "\tk = d/50 = 1000:\n",
      "\t\tpoison rate:  0.74\t( 0.70,  0.19,  1.35, )\n",
      "\t\tclean kept:   77.77\t( 77.95,  78.43,  76.92, )\n",
      "\tk = d/20 = 2500:\n",
      "\t\tpoison rate:  1.08\t( 0.96,  0.28,  2.00, )\n",
      "\t\tclean kept:   75.33\t( 75.85,  76.53,  73.63, )\n",
      "\tk = d/10 = 5000:\n",
      "\t\tpoison rate:  1.76\t( 1.53,  0.58,  3.17, )\n",
      "\t\tclean kept:   72.28\t( 73.07,  74.29,  69.47, )\n",
      "Testing wanet TEST\n",
      "\tk = d/5000 = 2:\n",
      "\t\tpoison rate:  11.07\t( 12.30,  10.89,  10.03, )\n",
      "\t\tclean kept:   80.32\t( 79.19,  80.64,  81.13, )\n",
      "\tk = d/2500 = 4:\n",
      "\t\tpoison rate:  5.36\t( 6.11,  4.95,  5.02, )\n",
      "\t\tclean kept:   81.74\t( 81.10,  81.72,  82.41, )\n",
      "\tk = d/1000 = 10:\n",
      "\t\tpoison rate:  2.02\t( 2.14,  1.79,  2.14, )\n",
      "\t\tclean kept:   81.24\t( 80.79,  81.68,  81.26, )\n",
      "\tk = d/500 = 20:\n",
      "\t\tpoison rate:  1.20\t( 1.35,  0.75,  1.51, )\n",
      "\t\tclean kept:   80.70\t( 80.38,  81.14,  80.57, )\n",
      "\tk = d/100 = 100:\n",
      "\t\tpoison rate:  0.96\t( 1.44,  0.31,  1.12, )\n",
      "\t\tclean kept:   78.05\t( 77.44,  78.97,  77.73, )\n",
      "\tk = d/50 = 200:\n",
      "\t\tpoison rate:  1.08\t( 1.59,  0.34,  1.29, )\n",
      "\t\tclean kept:   76.66\t( 76.12,  77.57,  76.30, )\n",
      "\tk = d/20 = 500:\n",
      "\t\tpoison rate:  1.45\t( 2.00,  0.43,  1.93, )\n",
      "\t\tclean kept:   73.46\t( 72.78,  75.37,  72.22, )\n",
      "\tk = d/10 = 1000:\n",
      "\t\tpoison rate:  2.45\t( 3.15,  0.66,  3.55, )\n",
      "\t\tclean kept:   69.56\t( 69.01,  72.34,  67.31, )\n",
      "Testing sig TRAIN\n",
      "\tk = d/5000 = 10:\n",
      "\t\tpoison rate:  1.17\t( 1.18,  1.16,  1.16, )\n",
      "\t\tclean kept:   85.64\t( 84.93,  85.76,  86.22, )\n",
      "\tk = d/2500 = 20:\n",
      "\t\tpoison rate:  1.18\t( 1.19,  1.18,  1.17, )\n",
      "\t\tclean kept:   84.49\t( 83.83,  84.62,  85.01, )\n",
      "\tk = d/1000 = 50:\n",
      "\t\tpoison rate:  1.20\t( 1.21,  1.20,  1.19, )\n",
      "\t\tclean kept:   83.19\t( 82.42,  83.39,  83.75, )\n",
      "\tk = d/500 = 100:\n",
      "\t\tpoison rate:  1.21\t( 1.22,  1.21,  1.20, )\n",
      "\t\tclean kept:   82.51\t( 81.70,  82.64,  83.19, )\n",
      "\tk = d/100 = 500:\n",
      "\t\tpoison rate:  1.23\t( 1.24,  1.23,  1.22, )\n",
      "\t\tclean kept:   80.91\t( 79.80,  81.19,  81.74, )\n",
      "\tk = d/50 = 1000:\n",
      "\t\tpoison rate:  1.24\t( 1.24,  1.24,  1.23, )\n",
      "\t\tclean kept:   79.96\t( 78.73,  80.29,  80.86, )\n",
      "\tk = d/20 = 2500:\n",
      "\t\tpoison rate:  1.17\t( 0.98,  1.27,  1.26, )\n",
      "\t\tclean kept:   78.01\t( 76.38,  78.39,  79.27, )\n",
      "\tk = d/10 = 5000:\n",
      "\t\tpoison rate:  0.21\t( 0.27,  0.00,  0.37, )\n",
      "\t\tclean kept:   75.63\t( 73.68,  76.10,  77.11, )\n",
      "Testing sig TEST\n",
      "\tk = d/5000 = 2:\n",
      "\t\tpoison rate:  1.13\t( 1.14,  1.14,  1.13, )\n",
      "\t\tclean kept:   88.06\t( 87.61,  87.84,  88.74, )\n",
      "\tk = d/2500 = 4:\n",
      "\t\tpoison rate:  1.16\t( 1.16,  1.16,  1.15, )\n",
      "\t\tclean kept:   86.03\t( 85.41,  86.06,  86.63, )\n",
      "\tk = d/1000 = 10:\n",
      "\t\tpoison rate:  1.18\t( 1.18,  1.18,  1.18, )\n",
      "\t\tclean kept:   84.09\t( 83.42,  84.28,  84.57, )\n",
      "\tk = d/500 = 20:\n",
      "\t\tpoison rate:  1.19\t( 1.19,  1.20,  1.19, )\n",
      "\t\tclean kept:   83.02\t( 82.12,  83.17,  83.76, )\n",
      "\tk = d/100 = 100:\n",
      "\t\tpoison rate:  1.23\t( 1.23,  1.23,  1.23, )\n",
      "\t\tclean kept:   80.57\t( 79.53,  80.94,  81.23, )\n",
      "\tk = d/50 = 200:\n",
      "\t\tpoison rate:  1.24\t( 1.23,  1.25,  1.25, )\n",
      "\t\tclean kept:   79.22\t( 78.17,  79.69,  79.79, )\n",
      "\tk = d/20 = 500:\n",
      "\t\tpoison rate:  1.15\t( 0.98,  1.18,  1.28, )\n",
      "\t\tclean kept:   76.79\t( 75.41,  77.05,  77.92, )\n",
      "\tk = d/10 = 1000:\n",
      "\t\tpoison rate:  0.12\t( 0.29,  0.00,  0.05, )\n",
      "\t\tclean kept:   73.87\t( 72.58,  73.82,  75.20, )\n"
     ]
    }
   ],
   "source": [
    "k_range = lambda d: [\n",
    "    d/5000,   # 10\n",
    "    d/2500,\n",
    "    d/1000,\n",
    "    d/500,\n",
    "    d/100,\n",
    "    d/50,   # d/5num_classes\n",
    "    d/20,   # d/2num_classes \n",
    "    d/10    # d/num_classes\n",
    "]\n",
    "\n",
    "k_range_str = lambda d: [\n",
    "    \"d/5000\",   # 10\n",
    "    \"d/2500\",\n",
    "    \"d/1000\",\n",
    "    \"d/500\",\n",
    "    \"d/100\",\n",
    "    \"d/50\",   # d/5num_classes\n",
    "    \"d/20\",   # d/2num_classes \n",
    "    \"d/10\"    # d/num_classes\n",
    "]\n",
    "\n",
    "for dataset_name in [\"badnets1\", \"wanet\", \"sig\"]:\n",
    "    for train in [True, False]:\n",
    "        \n",
    "        train_str = \"TRAIN\" if train else \"TEST\"\n",
    "        print(f\"Testing {dataset_name} {train_str}\")\n",
    "        \n",
    "        dataset_size = 50000 if train else 10000\n",
    "        for k, k_str in zip(k_range(dataset_size), k_range_str(dataset_size)):\n",
    "\n",
    "            k = int(k)\n",
    "            poison_rates = []\n",
    "            clean_kepts = []\n",
    "            for dataset_index in [0,1,2]:\n",
    "                dataset_name_complete = dataset_name + \"-\" + str(dataset_index)\n",
    "\n",
    "                simclr_model_name = f\"{dataset_name_complete}-SimCLR.pt\"\n",
    "                dataset, true_poison_indices, _, _ = prepare_poison_dataset(dataset_name_complete, train)\n",
    "                simclr, epochs = load_simclr(simclr_model_name)\n",
    "                features, labels_poison, labels_true = extract_simclr_features(simclr, dataset, layer=\"repr\")\n",
    "\n",
    "                predicted_poison_indices_nondisruptive = knn_cleanse(features, labels_poison, None, n_neighbors=k)\n",
    "                poison_rate, _, clean_kept = evaluate_cleanse(predicted_poison_indices_nondisruptive, true_poison_indices)\n",
    "\n",
    "                poison_rates.append(poison_rate)\n",
    "                clean_kepts.append(clean_kept)\n",
    "        \n",
    "            poison_rate = sum(poison_rates)/3\n",
    "            clean_kept = sum(clean_kepts)/3\n",
    "            print(f\"\\tk = {k_str} = {k}:\")\n",
    "            print(f\"\\t\\tpoison rate: {100*poison_rate: .2f}\\t(\", end=\"\")\n",
    "            for pr in poison_rates:\n",
    "                print(f\"{100*pr: .2f}, \", end=\"\")\n",
    "            print(\")\")\n",
    "            print(f\"\\t\\tclean kept:  {100*clean_kept: .2f}\\t(\", end=\"\")\n",
    "            for ck in clean_kepts:\n",
    "                print(f\"{100*ck: .2f}, \", end=\"\")\n",
    "            print(\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
