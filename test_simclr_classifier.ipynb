{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from math import ceil, floor\n",
    "\n",
    "import copy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from datasets import BadNetsDataset, WaNetDataset, SIGDataset\n",
    "from simclr import SimClrBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poison dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_poison_dataset(dataset_name: str, train: bool) -> VisionDataset:\n",
    "    clean_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=train)\n",
    "\n",
    "    if dataset_name == \"badnets\":\n",
    "        poison_dataset = BadNetsDataset(clean_dataset, 1, \"triggers/trigger_10.png\", seed=1)\n",
    "    elif dataset_name == \"wanet\":\n",
    "        poison_dataset = WaNetDataset(clean_dataset, 0, seed=1)\n",
    "    elif dataset_name == \"sig\":\n",
    "        poison_dataset = SIGDataset(clean_dataset, 1, 20, 6, seed=1)\n",
    "    else:\n",
    "        raise Exception(\"Invalid dataset\")\n",
    "\n",
    "    poison_indices = np.array([poison_dataset.is_poison(i) for i in range(len(poison_dataset))])\n",
    "\n",
    "    return poison_dataset, poison_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SimCLR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_simclr(simclr_model_name: str) -> SimClrBackbone:\n",
    "    model = SimClrBackbone()\n",
    "    out = os.path.join('./saved_models/', simclr_model_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract SimCLR features for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_simclr_features(model: SimClrBackbone, dataset: VisionDataset):\n",
    "\n",
    "    simclr_feature_size = 128\n",
    "    num_examples = len(dataset)\n",
    "\n",
    "    features = np.zeros((num_examples, simclr_feature_size))\n",
    "    labels_poison = np.zeros((num_examples))\n",
    "    labels_true = np.zeros((num_examples))\n",
    "\n",
    "    batch_size = 256\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    for i, (img, labels_batch_poison, labels_batch_true) in enumerate(dataloader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features_batch = model(img.to(device)).cpu().data.numpy()\n",
    "            \n",
    "        features[i*batch_size : i*batch_size+len(features_batch)] = features_batch\n",
    "        labels_poison[i*batch_size : i*batch_size+len(labels_batch_poison)] = labels_batch_poison.long()\n",
    "        labels_true[i*batch_size : i*batch_size+len(labels_batch_true)] = labels_batch_true.long()\n",
    "\n",
    "    labels_poison = labels_poison.astype(int)\n",
    "    labels_true = labels_true.astype(int)\n",
    "\n",
    "    return features, labels_poison, labels_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot t-SNE features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tsne_features(features: np.array, perplexity: int = 50) -> np.array:\n",
    "    tsne = TSNE(n_components = 2, perplexity = perplexity)\n",
    "    tsne_features = tsne.fit_transform(features)\n",
    "    return tsne_features\n",
    "\n",
    "def plot_tsne_features(tsne_features: np.array, labels: np.array, poison_indices: np.array, legend: bool = True) -> None:\n",
    "    num_classes = int(max(labels).item())\n",
    "\n",
    "    # label poison examples as 10\n",
    "    labels_10 = copy.deepcopy(labels)\n",
    "    labels_10[poison_indices] = 10\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        plt.scatter(tsne_features[labels_10==i,1], tsne_features[labels_10==i,0])\n",
    "    plt.scatter(tsne_features[labels_10==10,1], tsne_features[labels_10==10,0], c = \"black\", marker= \"x\")\n",
    "\n",
    "    if legend:\n",
    "        plt.legend([str(i) for i in range(num_classes)] + [\"poison\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_and_plot_tsne(features: np.array, labels: np.array, poison_indices: np.array, subset_size: int = None, legend: bool = True) -> np.array:\n",
    "    # Plot only a subset\n",
    "    if subset_size is None:\n",
    "        subset_size = len(features)\n",
    "    features_subset = features[:subset_size]\n",
    "    labels_subset = labels[:subset_size]\n",
    "    poison_indices_subset = poison_indices[:subset_size]\n",
    "    \n",
    "    tsne_features = calculate_tsne_features(features_subset)\n",
    "    plot_tsne_features(tsne_features, labels_subset, poison_indices_subset, legend=legend)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions for all cleanses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cleanse(poison_predicted: np.array, poison_indices: np.array):\n",
    "\n",
    "    tp = (poison_indices & poison_predicted).sum()\n",
    "    fp = (np.invert(poison_indices) & poison_predicted).sum()\n",
    "    fn = (poison_indices & np.invert(poison_predicted)).sum()\n",
    "    tn = (np.invert(poison_indices) & np.invert(poison_predicted)).sum()\n",
    "\n",
    "    fnr = fn/(fn+tp) if fn+tp!=0 else 0\n",
    "    tnr = tn/(tn+fp) if tn+fp!=0 else 0\n",
    "    poison_rate = fn/(fn+tn) if fn+tn!=0 else 0\n",
    "\n",
    "    print(f\"{tp} \\t {fp}\")\n",
    "    print(f\"{fn} \\t {tn}\")\n",
    "    print(f\"Percentage of poisoned images (out of all poisoned) kept: {100*fnr: .2f}%\")\n",
    "    print(f\"Percentage of clean images (out of all clean) kept: {100*tnr: .2f}%\")\n",
    "    print(f\"Percentage of remaining poisoned images (out of all remaining): {100*poison_rate: .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predicted_indices(predicted_indices: np.array, save_name: str):\n",
    "    with open(f\"./cleansed_labels/{save_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(predicted_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_poisoned(values: np.array, poison_indices: np.array = None, is_integer: bool = False, bins_num: int = 100, separation_line: float = None) -> None:\n",
    "    if poison_indices is not None:\n",
    "        values_clean = values[np.invert(poison_indices)]\n",
    "        values_poisoned = values[poison_indices]\n",
    "    else:\n",
    "        values_clean = values[:]\n",
    "        values_poisoned = []\n",
    "\n",
    "    bins = np.linspace(floor(np.min(values)), ceil(np.max(values)), int(np.max(values)) if is_integer else bins_num)\n",
    "    plt.hist(values_clean, bins, alpha=0.5, label='clean')\n",
    "    plt.hist(values_poisoned, bins, alpha=0.5, label='poisoned')\n",
    "\n",
    "    if separation_line:\n",
    "        plt.axvline(separation_line, color='red', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Non-disruptive cleanse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn_cleanse(features: np.array, labels_poison: np.array, num_classes: int) -> np.array:\n",
    "    \n",
    "    examples_per_class = len(features) / num_classes\n",
    "    knn = KNeighborsClassifier(n_neighbors=int(examples_per_class/2))\n",
    "    knn.fit(features, labels_poison)\n",
    "    labels_predicted = knn.predict(features)\n",
    "\n",
    "    return labels_predicted != labels_poison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "\n",
    "CIFAR-10 train \\\n",
    "BadNets \n",
    "\n",
    "    4905 \t 9481\n",
    "    95 \t 35519\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  1.90%\n",
    "    Percentage of clean images (out of all clean) kept:  78.93%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  0.27%\n",
    "\n",
    "CIFAR-10 test \\\n",
    "BadNets \n",
    "\n",
    "    960 \t 2056\n",
    "    40 \t 6944\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  4.00%\n",
    "    Percentage of clean images (out of all clean) kept:  77.16%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  0.57%\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "CIFAR-10 train \\\n",
    "WaNet \n",
    "\n",
    "    3975 \t 14331\n",
    "    1025 \t 30669\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  20.50%\n",
    "    Percentage of clean images (out of all clean) kept:  68.15%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  3.23%\n",
    "\n",
    "CIFAR-10 test \\\n",
    "WaNet \n",
    "\n",
    "    712 \t 3058\n",
    "    288 \t 5942\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  28.80%\n",
    "    Percentage of clean images (out of all clean) kept:  66.02%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  4.62%\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "CIFAR-10 train \\\n",
    "SIG\n",
    "\n",
    "    0 \t 9195\n",
    "    500 \t 40305\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  100.00%\n",
    "    Percentage of clean images (out of all clean) kept:  81.42%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  1.23%\n",
    "\n",
    "CIFAR-10 test \\\n",
    "SIG\n",
    "\n",
    "    0 \t 2179\n",
    "    100 \t 7721\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  100.00%\n",
    "    Percentage of clean images (out of all clean) kept:  77.99%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  1.28%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyClassifier():\n",
    "\n",
    "    def __init__(self, t=1):\n",
    "        self.t = t\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.C = int(np.max(y))\n",
    "        self.Ic = {c:[i for i in range(len(y)) if y[i]==c] for c in range(self.C)}\n",
    "        \n",
    "    def predict_index(self, i):\n",
    "        # consider improving with numpy and batch\n",
    "\n",
    "        xi = self.X[i]\n",
    "\n",
    "        exp_all = np.exp([xi*self.X[k]/self.t for k in range(len(self.X))])\n",
    "        sum_exp_all_except_xi = np.sum([exp_all[k] for k in range(len(self.X)) if k!=i])\n",
    "        mean_exp_c = [np.mean([exp_all[k] for k in self.Ic[c] if k!=i]) for c in range(self.C)]\n",
    "    \n",
    "        Scs = mean_exp_c / sum_exp_all_except_xi\n",
    "        return np.argmax(Scs)\n",
    "\n",
    "    def predict(self):\n",
    "        predicted = np.zeros((len(self.X)))\n",
    "        for i in tqdm(range(len(self.X))):\n",
    "            predicted[i] = self.predict_index(i)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_cleanse(features: np.array, labels_poison: np.array, t: float = 10) -> np.array:\n",
    "    \n",
    "    # if DATASET == \"badnets\":\n",
    "    #     T = 100\n",
    "    # elif DATASET == \"wanet\":\n",
    "    #     T = 10\n",
    "    # elif DATASET == \"sig\":\n",
    "    #     T = 1\n",
    "    # else:\n",
    "    #     raise Exception(\"Invalid dataset\")\n",
    "\n",
    "    energy = EnergyClassifier(t=t)\n",
    "    energy.fit(features, labels_poison)\n",
    "    labels_predicted = energy.predict()\n",
    "\n",
    "    return labels_predicted != labels_poison\n",
    "\n",
    "# if RUN_ENERGY:\n",
    "#     labels_predicted_energy = energy_cleanse(features, labels_poison)\n",
    "#     evaluate_cleanse(labels_predicted_energy != labels_poison, poison_indices)\n",
    "\n",
    "#     save_name = f\"__NEW__{DATASET_NAME}-Energy-{'train' if TRAIN else 'test'}\"\n",
    "#     save_predicted_labels(labels_predicted_knn, save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 train \\\n",
    "BadNets\n",
    "\n",
    "\t50000 / 50000\n",
    "\t\t4618 \t 12732\n",
    "\t\t382 \t 32268\n",
    "\tPercentage of poisoned images (out of all poisoned) kept:  7.64%\n",
    "\tPercentage of clean images (out of all clean) kept:  71.71%\n",
    "\tPercentage of remaining poisoned images (out of all remaining):  1.17%\n",
    "\n",
    "CIFAR-10 test \\\n",
    "BadNets\n",
    "\n",
    "\t10000 / 10000\n",
    "\t\t920 \t 2585\n",
    "\t\t80 \t 6415\n",
    "\tPercentage of poisoned images (out of all poisoned) kept:  8.00%\n",
    "\tPercentage of clean images (out of all clean) kept:  71.28%\n",
    "\tPercentage of remaining poisoned images (out of all remaining):  1.23%\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 train \\\n",
    "WaNet\n",
    "\n",
    "\t50000 / 50000\n",
    "\t\t4725 \t 14096\n",
    "\t\t275 \t 30904\n",
    "\tPercentage of poisoned images (out of all poisoned) kept:  5.50%\n",
    "\tPercentage of clean images (out of all clean) kept:  68.68%\n",
    "\tPercentage of remaining poisoned images (out of all remaining):  0.88%\n",
    "\n",
    "CIFAR-10 test \\\n",
    "WaNet\n",
    "\n",
    "\t10000 / 10000\n",
    "\t\t951 \t 2851\n",
    "\t\t49 \t 6149\n",
    "\tPercentage of poisoned images (out of all poisoned) kept:  4.90%\n",
    "\tPercentage of clean images (out of all clean) kept:  68.32%\n",
    "\tPercentage of remaining poisoned images (out of all remaining):  0.79%\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 train \\\n",
    "SIG\n",
    "\n",
    "\t50000 / 50000\n",
    "\t\t2 \t 15777\n",
    "\t\t498 \t 33723\n",
    "\tPercentage of poisoned images (out of all poisoned) kept:  99.60%\n",
    "\tPercentage of clean images (out of all clean) kept:  68.13%\n",
    "\tPercentage of remaining poisoned images (out of all remaining):  1.46%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO cleanup, delete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.linear(x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimClrFeaturesDataset(VisionDataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if RUN_LOGREG:\n",
    "    \n",
    "#     BATCH_SIZE = 1024\n",
    "#     NUM_MODELS = 10\n",
    "#     EPOCHS = 60\n",
    "\n",
    "#     classifications = np.zeros((NUM_MODELS, len(features)))\n",
    "\n",
    "#     for num_model in range(NUM_MODELS):\n",
    "#         print(f\"Learning logistic regression model {num_model+1}/{NUM_MODELS}\")\n",
    "#         simclr_features_dataset = SimClrFeaturesDataset(features, labels_poison)\n",
    "#         dataloader = DataLoader(dataset=simclr_features_dataset, batch_size=BATCH_SIZE, shuffle=True) \n",
    "#         model = LogReg(128, 10)\n",
    "#         optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-1)\n",
    "#         criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "#         # train\n",
    "#         for epoch in range(EPOCHS):\n",
    "#             correct = 0\n",
    "#             for i, (batch_features, batch_labels) in enumerate(dataloader):\n",
    "#                 batch_features, batch_labels = batch_features.float(), batch_labels.long()\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = model(batch_features)\n",
    "#                 loss = criterion(outputs, batch_labels)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 correct += (predicted == batch_labels).sum()\n",
    "#             accuracy = 100 * (correct.item()) / len(simclr_features_dataset)\n",
    "#         print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "#         # get model classifications\n",
    "#         dataloader = DataLoader(dataset=simclr_features_dataset, batch_size=BATCH_SIZE, shuffle=False) \n",
    "#         for i, (batch_features, batch_labels) in enumerate(dataloader):\n",
    "#             batch_features, batch_labels = batch_features.float(), batch_labels.long()\n",
    "\n",
    "#             outputs = model(batch_features)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             predicted = predicted.numpy()\n",
    "#             classifications[num_model, (i*BATCH_SIZE):(i*BATCH_SIZE+len(predicted))] = predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if RUN_LOGREG:\n",
    "\n",
    "#     labels_poison_repeated = np.tile(labels_poison, (NUM_MODELS,1))\n",
    "#     misclassifications = np.sum(classifications != labels_poison_repeated, axis=0)\n",
    "\n",
    "#     misclassifications_poisoned = misclassifications[poison_indices]\n",
    "#     misclassifications_clean = misclassifications[np.invert(poison_indices)]\n",
    "\n",
    "#     # print(misclassifications_poisoned.shape)\n",
    "#     # print(misclassifications_clean.shape)\n",
    "\n",
    "#     # print(np.average(misclassifications_poisoned))\n",
    "#     # print(np.average(misclassifications_clean))\n",
    "\n",
    "#     bins = np.linspace(0, np.max(misclassifications), np.max(misclassifications))\n",
    "#     plt.hist(misclassifications_clean, bins, alpha=0.5, label='clean')\n",
    "#     plt.hist(misclassifications_poisoned, bins, alpha=0.5, label='poisoned')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if RUN_LOGREG:\n",
    "\n",
    "#     class_num = 1\n",
    "\n",
    "#     misclassifications_class = misclassifications[labels_poison == class_num]\n",
    "#     misclassifications_class_poisoned = misclassifications[np.logical_and(labels_poison == class_num, poison_indices)]\n",
    "#     misclassifications_class_clean = misclassifications[np.logical_and(labels_poison == class_num, np.invert(poison_indices))]\n",
    "\n",
    "#     print((labels_poison == class_num & np.invert(poison_indices)).shape)\n",
    "#     print(misclassifications_class.shape)\n",
    "#     print(misclassifications_class_poisoned.shape)\n",
    "#     print(misclassifications_class_clean.shape)\n",
    "\n",
    "#     bins = np.linspace(0, np.max(misclassifications_class), np.max(misclassifications_class))\n",
    "#     plt.hist(misclassifications_class_clean, bins, alpha=0.5, label='clean')\n",
    "#     plt.hist(misclassifications_class_poisoned, bins, alpha=0.5, label='poisoned')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disruptive detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_cleanse(features: np.array, poison_indices: np.array) -> np.array:\n",
    "    centroid = np.sum(features, axis=0) / features.shape[0]\n",
    "    distances = np.linalg.norm(features - centroid, axis=1)\n",
    "    \n",
    "    plot_histogram_poisoned(distances, poison_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def gauss_cleanse(features: np.array, discard_percentage: float, poison_indices: np.array = None) -> np.array:\n",
    "    mean = np.mean(features, axis=0)\n",
    "    cov = np.cov(features, rowvar=0)\n",
    "\n",
    "    probabilities = multivariate_normal.pdf(features, mean=mean, cov=cov, allow_singular=True)\n",
    "    probabilities[probabilities <= 0] = 1e-100\n",
    "    probabilities = -np.log(probabilities)\n",
    "\n",
    "    discard_line = np.percentile(probabilities, (1-discard_percentage)*100)\n",
    "    plot_histogram_poisoned(probabilities, poison_indices, separation_line=discard_line)\n",
    "\n",
    "    predicted_poison_indices = probabilities > discard_line\n",
    "    return predicted_poison_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def kmeans_cleanse(features: np.array) -> np.array:\n",
    "\tkmeans = KMeans(n_clusters=50, init=\"k-means++\")\n",
    "\tkmeans.fit(features)\n",
    "\t\n",
    "\tcentroid = np.mean(features, axis=0)\n",
    "\tcluster_center_distances = [euclidean(center, centroid) for center in kmeans.cluster_centers_]\n",
    "\tpoison_cluster_index = cluster_center_distances.index(max(cluster_center_distances))\n",
    "\n",
    "\tpredicted_poison_indices = kmeans.predict(features) == poison_cluster_index\n",
    "\treturn predicted_poison_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poison reclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak binary classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoisonClassificationDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, poison_indices: np.array) -> None:\n",
    "        self.original_dataset = original_dataset\n",
    "        self.poison_indices = poison_indices\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.original_dataset)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "class ConvolutionalBinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1568, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_binary_classifier(dataset: VisionDataset, predicted_poison_indices: np.array) -> nn.Module:\n",
    "    poison_classification_dataset = PoisonClassificationDataset(dataset, predicted_poison_indices)\n",
    "    # sampler for class imbalance\n",
    "    positives = sum([1 for _, target in poison_classification_dataset if target==1])\n",
    "    total = len(poison_classification_dataset)\n",
    "    positive_weight = 0.5 / positives\n",
    "    negative_weight = 0.5 / (total - positives)\n",
    "    weights = [positive_weight if target==1 else negative_weight for _, target in poison_classification_dataset]\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "    dataloader = DataLoader(poison_classification_dataset, batch_size=128, sampler=sampler)\n",
    "    model = ConvolutionalBinaryClassifier().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1-1e-4)\n",
    "    num_epochs = 10\n",
    "\n",
    "    for _ in tqdm(range(num_epochs)):\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.forward(inputs).squeeze(-1)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def binary_reclassification(dataset: VisionDataset, model: nn.Module):\n",
    "    predicted_poison_indices = np.zeros((len(dataset)))\n",
    "    batch_size = 128\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    for i, (inputs, _, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(inputs).squeeze(-1)\n",
    "            predictions = (logits>0.5).cpu().numpy()\n",
    "        predicted_poison_indices[i*batch_size : i*batch_size+len(predictions)] = predictions\n",
    "    return predicted_poison_indices==1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong multiclass classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, predicted_poison: np.array, transforms: torch.nn.Module = None):\n",
    "        self.data = [poison_dataset[i][0] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.labels = [poison_dataset[i][1] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transforms:\n",
    "            item = self.transforms(item)\n",
    "\n",
    "        return item, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiclass_classifier(dataset: VisionDataset, predicted_poison_indices: np.array) -> nn.Module:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    cleansed_dataset = CleansedDataset(dataset, predicted_poison_indices, transform_train)\n",
    "    dataloader = DataLoader(cleansed_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = ResNet18()\n",
    "    model.to(device)\n",
    "\n",
    "    epochs = 35\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100.*correct/total\n",
    "        scheduler.step()\n",
    "        print(epoch, train_loss, acc)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_reclassification(dataset: VisionDataset, model: nn.Module, true_labels: np.array):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_dataset = CleansedDataset(dataset, np.zeros(len(dataset)), transform_train)\n",
    "    \n",
    "    predicted_labels = np.zeros((len(dataset)))\n",
    "    batch_size = 128\n",
    "    dataloader = DataLoader(transform_dataset, batch_size=batch_size, shuffle=False)\n",
    "    for i, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(inputs)\n",
    "            predictions = torch.argmax(logits, 1).cpu().numpy()\n",
    "        predicted_labels[i*batch_size : i*batch_size+len(predictions)] = predictions\n",
    "    return predicted_labels!=true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final cleanse pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "transform_train= transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "batch_size = 1024\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=True, download=False, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_test)\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Luka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class SimClrLR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        dataset_name = \"badnets\"\n",
    "        simclr_model_name = f\"{dataset_name}-SimCLR.pt\"\n",
    "        self.simclr = load_simclr(simclr_model_name)\n",
    "        \n",
    "        for p in self.simclr.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        for p in self.simclr.projector.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.lastlayer = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.simclr.pretrained(x)\n",
    "        out = self.lastlayer(out)\n",
    "        return out\n",
    "    \n",
    "model = SimClrLR().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([params for params in model.parameters() if params.requires_grad],lr = 0.6, momentum = 0.9, weight_decay=0., nesterov=True)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98, last_epoch=-1, verbose = True)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ep_loss = []\n",
    "tr_ep_acc = []\n",
    "\n",
    "val_ep_loss = []\n",
    "val_ep_acc = []\n",
    "\n",
    "min_val_loss = 100.0\n",
    "\n",
    "EPOCHS = 10\n",
    "num_cl = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== Epoch :   1 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.7347071486360887\n",
      "TRAINING BINARY ACCURACY:  0.77056\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5633732123034341\n",
      "VALIDATION BINARY ACCURACY:  0.8144\n",
      "Time Taken : 0.47 minutes\n",
      "=============== Epoch :   2 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.5546618962989134\n",
      "TRAINING BINARY ACCURACY:  0.81396\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5457522869110107\n",
      "VALIDATION BINARY ACCURACY:  0.8175\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :   3 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.530631818315562\n",
      "TRAINING BINARY ACCURACY:  0.81678\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5774041499410357\n",
      "VALIDATION BINARY ACCURACY:  0.7998\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :   4 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.5245118947590098\n",
      "TRAINING BINARY ACCURACY:  0.82\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.504025033542088\n",
      "VALIDATION BINARY ACCURACY:  0.827\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :   5 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.5163788663990357\n",
      "TRAINING BINARY ACCURACY:  0.8237\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5148256463663918\n",
      "VALIDATION BINARY ACCURACY:  0.8218\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :   6 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.5038819812676486\n",
      "TRAINING BINARY ACCURACY:  0.82568\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5029228925704956\n",
      "VALIDATION BINARY ACCURACY:  0.8248\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :   7 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.5029178849037956\n",
      "TRAINING BINARY ACCURACY:  0.82628\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5480748329843793\n",
      "VALIDATION BINARY ACCURACY:  0.8088\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :   8 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.5084785021403256\n",
      "TRAINING BINARY ACCURACY:  0.82482\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5078126192092896\n",
      "VALIDATION BINARY ACCURACY:  0.8235\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :   9 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.49419382039238424\n",
      "TRAINING BINARY ACCURACY:  0.82936\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5260156052453178\n",
      "VALIDATION BINARY ACCURACY:  0.8182\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :  10 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.49465614206650677\n",
      "TRAINING BINARY ACCURACY:  0.82908\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5241965012890952\n",
      "VALIDATION BINARY ACCURACY:  0.8218\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :  11 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.49041687215075774\n",
      "TRAINING BINARY ACCURACY:  0.83008\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5136628150939941\n",
      "VALIDATION BINARY ACCURACY:  0.8203\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :  12 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.49341900208417105\n",
      "TRAINING BINARY ACCURACY:  0.82896\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5057025126048497\n",
      "VALIDATION BINARY ACCURACY:  0.8209\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :  13 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.48589224149199095\n",
      "TRAINING BINARY ACCURACY:  0.83102\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.532657435962132\n",
      "VALIDATION BINARY ACCURACY:  0.8149\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :  14 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.48025863135562225\n",
      "TRAINING BINARY ACCURACY:  0.8342\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5368602318423135\n",
      "VALIDATION BINARY ACCURACY:  0.8161\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :  15 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.48339687375461354\n",
      "TRAINING BINARY ACCURACY:  0.83042\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5374097100325993\n",
      "VALIDATION BINARY ACCURACY:  0.8186\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :  16 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.479539651204558\n",
      "TRAINING BINARY ACCURACY:  0.8332\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5584202834538051\n",
      "VALIDATION BINARY ACCURACY:  0.8103\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :  17 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.4772042544449077\n",
      "TRAINING BINARY ACCURACY:  0.83308\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5190160359655108\n",
      "VALIDATION BINARY ACCURACY:  0.8192\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :  18 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.4906550610766691\n",
      "TRAINING BINARY ACCURACY:  0.83094\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5371618909495217\n",
      "VALIDATION BINARY ACCURACY:  0.8167\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :  19 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.46881162769654217\n",
      "TRAINING BINARY ACCURACY:  0.8369\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5196714997291565\n",
      "VALIDATION BINARY ACCURACY:  0.8208\n",
      "Time Taken : 0.46 minutes\n",
      "=============== Epoch :  20 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.48270977332311515\n",
      "TRAINING BINARY ACCURACY:  0.83214\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5601948499679565\n",
      "VALIDATION BINARY ACCURACY:  0.8131\n",
      "Time Taken : 0.46 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    stime = time.time()\n",
    "    print(\"=============== Epoch : %3d ===============\"%(epoch+1))\n",
    "    \n",
    "    loss_sublist = np.array([])\n",
    "    acc_sublist = np.array([])\n",
    "    \n",
    "    #iter_num = 0\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for x,y in trainloader:\n",
    "        x = x.squeeze().to(device, dtype = torch.float)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        z = model(x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tr_loss = loss_fn(z,y)\n",
    "        tr_loss.backward()\n",
    "\n",
    "        preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sublist = np.append(loss_sublist, tr_loss.cpu().data)\n",
    "        acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n",
    "        \n",
    "    print('ESTIMATING TRAINING METRICS.............')\n",
    "    \n",
    "    print('TRAINING BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist))\n",
    "    print('TRAINING BINARY ACCURACY: ',np.mean(acc_sublist))\n",
    "    \n",
    "    tr_ep_loss.append(np.mean(loss_sublist))\n",
    "    tr_ep_acc.append(np.mean(acc_sublist))\n",
    "    \n",
    "    print('ESTIMATING VALIDATION METRICS.............')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    loss_sublist = np.array([])\n",
    "    acc_sublist = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y in testloader:\n",
    "            x = x.squeeze().to(device = 'cuda:0', dtype = torch.float)\n",
    "            y = y.to(device = 'cuda:0')\n",
    "            z = model(x)\n",
    "\n",
    "            val_loss = loss_fn(z,y)\n",
    "\n",
    "            preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n",
    "\n",
    "            loss_sublist = np.append(loss_sublist, val_loss.cpu().data)\n",
    "            acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n",
    "    \n",
    "    print('VALIDATION BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist))\n",
    "    print('VALIDATION BINARY ACCURACY: ',np.mean(acc_sublist))\n",
    "    \n",
    "    val_ep_loss.append(np.mean(loss_sublist))\n",
    "    val_ep_acc.append(np.mean(acc_sublist))\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    #dg.on_epoch_end()\n",
    "    \n",
    "    # if np.mean(loss_sublist) <= min_val_loss:\n",
    "    #     min_val_loss = np.mean(loss_sublist) \n",
    "    #     print('Saving model...')\n",
    "    #     torch.save({'model_state_dict': dsmodel.state_dict(),\n",
    "    #             'optimizer_state_dict': dsoptimizer.state_dict()}, \n",
    "    #            '/content/saved_models/cifar10_rn50_p128_sgd0p01_decay0p98_all_lincls.pt')\n",
    "    \n",
    "    print(\"Time Taken : %.2f minutes\"%((time.time()-stime)/60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
