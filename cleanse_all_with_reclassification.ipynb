{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import prepare_poison_dataset\n",
    "from util import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, predicted_poison: np.array, transforms: torch.nn.Module = None):\n",
    "        self.data = [poison_dataset[i][0] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.labels = [poison_dataset[i][1] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transforms:\n",
    "            item = self.transforms(item)\n",
    "\n",
    "        return item, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiclass_classifier(dataset: VisionDataset, predicted_poison_indices: np.array, epochs: int = 25) -> nn.Module:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    cleansed_dataset = CleansedDataset(dataset, predicted_poison_indices, transform_train)\n",
    "    dataloader = DataLoader(cleansed_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = ResNet18()\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100.*correct/total\n",
    "        scheduler.step()\n",
    "        #print(epoch, train_loss, acc)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_reclassification(dataset: VisionDataset, model: nn.Module, original_labels: np.array):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_dataset = CleansedDataset(dataset, np.zeros(len(dataset)), transform_train)\n",
    "    \n",
    "    predicted_labels = np.zeros((len(dataset)))\n",
    "    batch_size = 128\n",
    "    dataloader = DataLoader(transform_dataset, batch_size=batch_size, shuffle=False)\n",
    "    for i, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(inputs)\n",
    "            predictions = torch.argmax(logits, 1).cpu().numpy()\n",
    "        predicted_labels[i*batch_size : i*batch_size+len(predictions)] = predictions\n",
    "\n",
    "    return predicted_labels != original_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "badnets1-train\n",
      "\tpoison rate:  0.02%\t( 0.03,  0.00,  0.03, )\n",
      "\tclean kept:   72.41%\t( 71.96,  74.82,  70.46, )\n",
      "badnets1-test\n",
      "\tpoison rate:  0.13%\t( 0.34,  0.00,  0.06, )\n",
      "\tclean kept:   60.48%\t( 58.36,  55.41,  67.66, )\n",
      "badnets10-train\n",
      "\tpoison rate:  0.31%\t( 0.35,  0.26,  0.34, )\n",
      "\tclean kept:   76.17%\t( 76.97,  80.63,  70.91, )\n",
      "badnets10-test\n",
      "\tpoison rate:  1.52%\t( 1.77,  0.03,  2.76, )\n",
      "\tclean kept:   63.37%\t( 64.30,  64.63,  61.18, )\n",
      "sig-train\n",
      "\tpoison rate:  0.16%\t( 0.00,  0.00,  0.49, )\n",
      "\tclean kept:   80.13%\t( 79.85,  79.31,  81.24, )\n",
      "sig-test\n",
      "\tpoison rate:  0.24%\t( 0.00,  0.00,  0.71, )\n",
      "\tclean kept:   73.29%\t( 75.31,  69.40,  75.16, )\n",
      "wanet-train\n",
      "\tpoison rate:  0.82%\t( 0.63,  0.38,  1.46, )\n",
      "\tclean kept:   76.91%\t( 75.38,  77.86,  77.48, )\n",
      "wanet-test\n",
      "\tpoison rate:  2.22%\t( 1.99,  1.96,  2.71, )\n",
      "\tclean kept:   55.48%\t( 65.78,  64.01,  36.64, )\n"
     ]
    }
   ],
   "source": [
    "def save_predicted_indices(predicted_indices: np.array, save_name: str):\n",
    "            with open(f\"./cleansed_labels/{save_name}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(predicted_indices, f)\n",
    "\n",
    "for dataset_str in [\"badnets1\", \"badnets10\", \"sig\", \"wanet\"]:\n",
    "    for train in [True, False]:\n",
    "\n",
    "        poison_rates = []\n",
    "        clean_kepts = []\n",
    "\n",
    "        train_str = \"train\" if train else \"test\"\n",
    "        print(f\"{dataset_str}-{train_str}\")\n",
    "\n",
    "        for dataset_index in [0,1,2]:\n",
    "        \n",
    "            dataset_name = f\"{dataset_str}-{dataset_index}\"\n",
    "            simclr_model_name = f\"{dataset_name}-SimCLR.pt\"\n",
    "\n",
    "            dataset, true_poison_indices, _, _ = prepare_poison_dataset(dataset_name, train)\n",
    "            simclr, _ = load_simclr(simclr_model_name)\n",
    "            features, labels_poison, labels_true = extract_simclr_features(simclr, dataset, layer=\"repr\")\n",
    "            num_classes = int(max(labels_poison).item())\n",
    "\n",
    "            n_neighbors = int(len(dataset) / 500)\n",
    "\n",
    "            # Nondisruptive cleanse\n",
    "            predicted_poison_indices_nondisruptive = knn_cleanse(features, labels_poison, n_neighbors=n_neighbors)\n",
    "\n",
    "            # Disruptive cleanse\n",
    "            features_2d = calculate_features_2d(features, n_neighbors=n_neighbors)\n",
    "            #plot_features_2d(features_2d, labels_poison, true_poison_indices, legend=True)\n",
    "            predicted_poison_indices_disruptive = kmeans_cleanse(features_2d, means=11, mode=\"distance\")\n",
    "\n",
    "            # Combine cleanses\n",
    "            predicted_poison_indices_final = predicted_poison_indices_nondisruptive | predicted_poison_indices_disruptive\n",
    "\n",
    "            # RECLASSIFICATION\n",
    "            poison_multiclass_classifier_model = train_multiclass_classifier(dataset, predicted_poison_indices_final, 25)\n",
    "            predicted_poison_indices_multiclass_reclassification = multiclass_reclassification(dataset, poison_multiclass_classifier_model, labels_poison)\n",
    "\n",
    "            # Evaluate\n",
    "            poison_rate, _, clean_kept = evaluate_cleanse(predicted_poison_indices_multiclass_reclassification, true_poison_indices)\n",
    "            poison_rates.append(poison_rate)\n",
    "            clean_kepts.append(clean_kept)\n",
    "\n",
    "            # Save\n",
    "            save_name = f\"{dataset_str}-{dataset_index}-{train_str}\"\n",
    "            #save_predicted_indices(predicted_poison_indices_final, save_name)\n",
    "\n",
    "        poison_rate = sum(poison_rates)/len(poison_rates)\n",
    "        clean_kept = sum(clean_kepts)/len(clean_kepts)\n",
    "        \n",
    "        # Print\n",
    "        print(f\"\\tpoison rate: {100*poison_rate: .2f}%\\t(\", end=\"\")\n",
    "        for pr in poison_rates:\n",
    "            print(f\"{100*pr: .2f}, \", end=\"\")\n",
    "        print(\")\")\n",
    "        print(f\"\\tclean kept:  {100*clean_kept: .2f}%\\t(\", end=\"\")\n",
    "        for ck in clean_kepts:\n",
    "            print(f\"{100*ck: .2f}, \", end=\"\")\n",
    "        print(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoisonClassificationDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, poison_indices: np.array) -> None:\n",
    "        self.original_dataset = original_dataset\n",
    "        self.poison_indices = poison_indices\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.original_dataset)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "class ConvolutionalBinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1568, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_binary_classifier(dataset: VisionDataset, predicted_poison_indices: np.array, epochs: int) -> nn.Module:\n",
    "    poison_classification_dataset = PoisonClassificationDataset(dataset, predicted_poison_indices)\n",
    "    # sampler for class imbalance\n",
    "    positives = sum([1 for _, target in poison_classification_dataset if target==1])\n",
    "    total = len(poison_classification_dataset)\n",
    "    positive_weight = 0.5 / positives\n",
    "    negative_weight = 0.5 / (total - positives)\n",
    "    weights = [positive_weight if target==1 else negative_weight for _, target in poison_classification_dataset]\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "    dataloader = DataLoader(poison_classification_dataset, batch_size=128, sampler=sampler)\n",
    "    model = ConvolutionalBinaryClassifier().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1-1e-4)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.forward(inputs).squeeze(-1)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def binary_reclassification(dataset: VisionDataset, model: nn.Module):\n",
    "    predicted_poison_indices = np.zeros((len(dataset)))\n",
    "    batch_size = 128\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    for i, (inputs, _, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(inputs).squeeze(-1)\n",
    "            predictions = (logits>0.5).cpu().numpy()\n",
    "        predicted_poison_indices[i*batch_size : i*batch_size+len(predictions)] = predictions\n",
    "    return predicted_poison_indices==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "badnets1-train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpoison rate:  0.31%\t( 0.11,  0.61,  0.21, )\n",
      "\tclean kept:   63.40%\t( 63.22,  65.00,  61.96, )\n",
      "badnets1-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpoison rate:  0.83%\t( 0.83,  0.75,  0.90, )\n",
      "\tclean kept:   67.23%\t( 73.90,  58.46,  69.32, )\n",
      "badnets10-train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpoison rate:  0.00%\t( 0.01,  0.00,  0.00, )\n",
      "\tclean kept:   70.75%\t( 75.11,  72.15,  64.99, )\n",
      "badnets10-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpoison rate:  0.04%\t( 0.07,  0.00,  0.06, )\n",
      "\tclean kept:   77.07%\t( 83.09,  69.34,  78.79, )\n",
      "sig-train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpoison rate:  0.00%\t( 0.00,  0.00,  0.00, )\n",
      "\tclean kept:   59.67%\t( 63.50,  58.11,  57.40, )\n",
      "sig-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpoison rate:  0.00%\t( 0.00,  0.00,  0.00, )\n",
      "\tclean kept:   47.06%\t( 42.21,  54.20,  44.76, )\n",
      "wanet-train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpoison rate:  2.66%\t( 2.94,  3.06,  1.99, )\n",
      "\tclean kept:   51.70%\t( 53.44,  59.23,  42.43, )\n",
      "wanet-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n",
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_1544\\2447615426.py:11: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpoison rate:  5.30%\t( 5.87,  3.44,  6.60, )\n",
      "\tclean kept:   53.91%\t( 63.84,  33.41,  64.49, )\n"
     ]
    }
   ],
   "source": [
    "for dataset_str in [\"badnets1\", \"badnets10\", \"sig\", \"wanet\"]:\n",
    "    for train in [True, False]:\n",
    "\n",
    "        poison_rates = []\n",
    "        clean_kepts = []\n",
    "\n",
    "        train_str = \"train\" if train else \"test\"\n",
    "        print(f\"{dataset_str}-{train_str}\")\n",
    "\n",
    "        for dataset_index in [0,1,2]:\n",
    "        \n",
    "            dataset_name = f\"{dataset_str}-{dataset_index}\"\n",
    "            simclr_model_name = f\"{dataset_name}-SimCLR.pt\"\n",
    "\n",
    "            dataset, true_poison_indices, _, _ = prepare_poison_dataset(dataset_name, train)\n",
    "            simclr, _ = load_simclr(simclr_model_name)\n",
    "            features, labels_poison, labels_true = extract_simclr_features(simclr, dataset, layer=\"repr\")\n",
    "            num_classes = int(max(labels_poison).item())\n",
    "\n",
    "            n_neighbors = int(len(dataset) / 500)\n",
    "\n",
    "            # Nondisruptive cleanse\n",
    "            predicted_poison_indices_nondisruptive = knn_cleanse(features, labels_poison, n_neighbors=n_neighbors)\n",
    "\n",
    "            # Disruptive cleanse\n",
    "            features_2d = calculate_features_2d(features, n_neighbors=n_neighbors)\n",
    "            #plot_features_2d(features_2d, labels_poison, true_poison_indices, legend=True)\n",
    "            predicted_poison_indices_disruptive = kmeans_cleanse(features_2d, means=11, mode=\"distance\")\n",
    "\n",
    "            # Combine cleanses\n",
    "            predicted_poison_indices_final = predicted_poison_indices_nondisruptive | predicted_poison_indices_disruptive\n",
    "\n",
    "            # RECLASSIFICATION\n",
    "            poison_multiclass_classifier_model = train_binary_classifier(dataset, predicted_poison_indices_final, 10)\n",
    "            predicted_poison_indices_multiclass_reclassification = binary_reclassification(dataset, poison_multiclass_classifier_model)\n",
    "\n",
    "            # Evaluate\n",
    "            poison_rate, _, clean_kept = evaluate_cleanse(predicted_poison_indices_multiclass_reclassification, true_poison_indices)\n",
    "            poison_rates.append(poison_rate)\n",
    "            clean_kepts.append(clean_kept)\n",
    "\n",
    "            # Save\n",
    "            save_name = f\"{dataset_str}-{dataset_index}-{train_str}\"\n",
    "            #save_predicted_indices(predicted_poison_indices_final, save_name)\n",
    "\n",
    "        poison_rate = sum(poison_rates)/len(poison_rates)\n",
    "        clean_kept = sum(clean_kepts)/len(clean_kepts)\n",
    "        \n",
    "        # Print\n",
    "        print(f\"\\tpoison rate: {100*poison_rate: .2f}%\\t(\", end=\"\")\n",
    "        for pr in poison_rates:\n",
    "            print(f\"{100*pr: .2f}, \", end=\"\")\n",
    "        print(\")\")\n",
    "        print(f\"\\tclean kept:  {100*clean_kept: .2f}%\\t(\", end=\"\")\n",
    "        for ck in clean_kepts:\n",
    "            print(f\"{100*ck: .2f}, \", end=\"\")\n",
    "        print(\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
