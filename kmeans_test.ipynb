{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Luka\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from math import ceil, floor\n",
    "\n",
    "import copy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from datasets import BadNetsDataset, WaNetDataset, SIGDataset\n",
    "from simclr import SimClrBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poison dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_poison_dataset(dataset_name: str, train: bool) -> VisionDataset:\n",
    "    clean_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=train, download=True)\n",
    "\n",
    "    if dataset_name == \"badnets\":\n",
    "        poison_dataset = BadNetsDataset(clean_dataset, 1, \"triggers/trigger_10.png\", seed=1)\n",
    "    elif dataset_name == \"wanet\":\n",
    "        poison_dataset = WaNetDataset(clean_dataset, 0, seed=1)\n",
    "    elif dataset_name == \"sig\":\n",
    "        poison_dataset = SIGDataset(clean_dataset, 1, 20, 6, seed=1)\n",
    "    else:\n",
    "        raise Exception(\"Invalid dataset\")\n",
    "\n",
    "    poison_indices = np.array([poison_dataset.is_poison(i) for i in range(len(poison_dataset))])\n",
    "\n",
    "    return poison_dataset, poison_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SimCLR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_simclr(simclr_model_name: str) -> SimClrBackbone:\n",
    "    model = SimClrBackbone()\n",
    "    out = os.path.join('./saved_models/', simclr_model_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract SimCLR features for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_simclr_features(model: SimClrBackbone, dataset: VisionDataset):\n",
    "\n",
    "    simclr_feature_size = 128\n",
    "    num_examples = len(dataset)\n",
    "\n",
    "    features = np.zeros((num_examples, simclr_feature_size))\n",
    "    labels_poison = np.zeros((num_examples))\n",
    "    labels_true = np.zeros((num_examples))\n",
    "\n",
    "    batch_size = 256\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    for i, (img, labels_batch_poison, labels_batch_true) in enumerate(dataloader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features_batch = model(img.to(device)).cpu().data.numpy()\n",
    "            \n",
    "        features[i*batch_size : i*batch_size+len(features_batch)] = features_batch\n",
    "        labels_poison[i*batch_size : i*batch_size+len(labels_batch_poison)] = labels_batch_poison.long()\n",
    "        labels_true[i*batch_size : i*batch_size+len(labels_batch_true)] = labels_batch_true.long()\n",
    "\n",
    "    labels_poison = labels_poison.astype(int)\n",
    "    labels_true = labels_true.astype(int)\n",
    "\n",
    "    return features, labels_poison, labels_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot t-SNE features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tsne_features(features: np.array, perplexity: int = 50) -> np.array:\n",
    "    tsne = TSNE(n_components = 2, perplexity = perplexity)\n",
    "    tsne_features = tsne.fit_transform(features)\n",
    "    return tsne_features\n",
    "\n",
    "def plot_tsne_features(tsne_features: np.array, labels: np.array, poison_indices: np.array, legend: bool = True) -> None:\n",
    "    num_classes = int(max(labels).item())\n",
    "\n",
    "    # label poison examples as 10\n",
    "    labels_10 = copy.deepcopy(labels)\n",
    "    labels_10[poison_indices] = 10\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        plt.scatter(tsne_features[labels_10==i,1], tsne_features[labels_10==i,0])\n",
    "    plt.scatter(tsne_features[labels_10==10,1], tsne_features[labels_10==10,0], c = \"black\", marker= \"x\")\n",
    "\n",
    "    if legend:\n",
    "        plt.legend([str(i) for i in range(num_classes)] + [\"poison\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_and_plot_tsne(features: np.array, labels: np.array, poison_indices: np.array, subset_size: int = None, legend: bool = True) -> np.array:\n",
    "    # Plot only a subset\n",
    "    if subset_size is None:\n",
    "        subset_size = len(features)\n",
    "    features_subset = features[:subset_size]\n",
    "    labels_subset = labels[:subset_size]\n",
    "    poison_indices_subset = poison_indices[:subset_size]\n",
    "    \n",
    "    tsne_features = calculate_tsne_features(features_subset)\n",
    "    plot_tsne_features(tsne_features, labels_subset, poison_indices_subset, legend=legend)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions for all cleanses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cleanse(poison_predicted: np.array, poison_indices: np.array):\n",
    "\n",
    "    tp = (poison_indices & poison_predicted).sum()\n",
    "    fp = (np.invert(poison_indices) & poison_predicted).sum()\n",
    "    fn = (poison_indices & np.invert(poison_predicted)).sum()\n",
    "    tn = (np.invert(poison_indices) & np.invert(poison_predicted)).sum()\n",
    "\n",
    "    fnr = fn/(fn+tp) if fn+tp!=0 else 0\n",
    "    tnr = tn/(tn+fp) if tn+fp!=0 else 0\n",
    "    poison_rate = fn/(fn+tn) if fn+tn!=0 else 0\n",
    "\n",
    "    print(f\"{tp} \\t {fp}\")\n",
    "    print(f\"{fn} \\t {tn}\")\n",
    "    print(f\"Percentage of poisoned images (out of all poisoned) kept: {100*fnr: .2f}%\")\n",
    "    print(f\"Percentage of clean images (out of all clean) kept: {100*tnr: .2f}%\")\n",
    "    print(f\"Percentage of remaining poisoned images (out of all remaining): {100*poison_rate: .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predicted_labels(labels_predicted: np.array, save_name: str):\n",
    "    with open(f\"./cleansed_labels/{save_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(labels_predicted, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_poisoned(values: np.array, poison_indices: np.array = None, is_integer: bool = False, bins_num: int = 100, separation_line: float = None) -> None:\n",
    "    if poison_indices is not None:\n",
    "        values_clean = values[np.invert(poison_indices)]\n",
    "        values_poisoned = values[poison_indices]\n",
    "    else:\n",
    "        values_clean = values[:]\n",
    "        values_poisoned = []\n",
    "\n",
    "    bins = np.linspace(floor(np.min(values)), ceil(np.max(values)), int(np.max(values)) if is_integer else bins_num)\n",
    "    plt.hist(values_clean, bins, alpha=0.5, label='clean')\n",
    "    plt.hist(values_poisoned, bins, alpha=0.5, label='poisoned')\n",
    "\n",
    "    if separation_line:\n",
    "        plt.axvline(separation_line, color='red', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Non-disruptive cleanse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn_cleanse(features: np.array, labels_poison: np.array, num_classes: int) -> np.array:\n",
    "    \n",
    "    examples_per_class = len(features) / num_classes\n",
    "    knn = KNeighborsClassifier(n_neighbors=int(examples_per_class/2))\n",
    "    knn.fit(features, labels_poison)\n",
    "    labels_predicted = knn.predict(features)\n",
    "\n",
    "    return labels_predicted != labels_poison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "\n",
    "CIFAR-10 train \\\n",
    "BadNets \n",
    "\n",
    "    4905 \t 9481\n",
    "    95 \t 35519\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  1.90%\n",
    "    Percentage of clean images (out of all clean) kept:  78.93%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  0.27%\n",
    "\n",
    "CIFAR-10 test \\\n",
    "BadNets \n",
    "\n",
    "    960 \t 2056\n",
    "    40 \t 6944\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  4.00%\n",
    "    Percentage of clean images (out of all clean) kept:  77.16%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  0.57%\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "CIFAR-10 train \\\n",
    "WaNet \n",
    "\n",
    "    3975 \t 14331\n",
    "    1025 \t 30669\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  20.50%\n",
    "    Percentage of clean images (out of all clean) kept:  68.15%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  3.23%\n",
    "\n",
    "CIFAR-10 test \\\n",
    "WaNet \n",
    "\n",
    "    712 \t 3058\n",
    "    288 \t 5942\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  28.80%\n",
    "    Percentage of clean images (out of all clean) kept:  66.02%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  4.62%\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "CIFAR-10 train \\\n",
    "SIG\n",
    "\n",
    "    0 \t 9195\n",
    "    500 \t 40305\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  100.00%\n",
    "    Percentage of clean images (out of all clean) kept:  81.42%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  1.23%\n",
    "\n",
    "CIFAR-10 test \\\n",
    "SIG\n",
    "\n",
    "    0 \t 2179\n",
    "    100 \t 7721\n",
    "    Percentage of poisoned images (out of all poisoned) kept:  100.00%\n",
    "    Percentage of clean images (out of all clean) kept:  77.99%\n",
    "    Percentage of remaining poisoned images (out of all remaining):  1.28%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyClassifier():\n",
    "\n",
    "    def __init__(self, t=1):\n",
    "        self.t = t\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.C = int(np.max(y))\n",
    "        self.Ic = {c:[i for i in range(len(y)) if y[i]==c] for c in range(self.C)}\n",
    "        \n",
    "    def predict_index(self, i):\n",
    "        # consider improving with numpy and batch\n",
    "\n",
    "        xi = self.X[i]\n",
    "\n",
    "        exp_all = np.exp([xi*self.X[k]/self.t for k in range(len(self.X))])\n",
    "        sum_exp_all_except_xi = np.sum([exp_all[k] for k in range(len(self.X)) if k!=i])\n",
    "        mean_exp_c = [np.mean([exp_all[k] for k in self.Ic[c] if k!=i]) for c in range(self.C)]\n",
    "    \n",
    "        Scs = mean_exp_c / sum_exp_all_except_xi\n",
    "        return np.argmax(Scs)\n",
    "\n",
    "    def predict(self):\n",
    "        predicted = np.zeros((len(self.X)))\n",
    "        for i in tqdm(range(len(self.X))):\n",
    "            predicted[i] = self.predict_index(i)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_cleanse(features: np.array, labels_poison: np.array, t: float = 10) -> np.array:\n",
    "    \n",
    "    # if DATASET == \"badnets\":\n",
    "    #     T = 100\n",
    "    # elif DATASET == \"wanet\":\n",
    "    #     T = 10\n",
    "    # elif DATASET == \"sig\":\n",
    "    #     T = 1\n",
    "    # else:\n",
    "    #     raise Exception(\"Invalid dataset\")\n",
    "\n",
    "    energy = EnergyClassifier(t=t)\n",
    "    energy.fit(features, labels_poison)\n",
    "    labels_predicted = energy.predict()\n",
    "\n",
    "    return labels_predicted != labels_poison\n",
    "\n",
    "# if RUN_ENERGY:\n",
    "#     labels_predicted_energy = energy_cleanse(features, labels_poison)\n",
    "#     evaluate_cleanse(labels_predicted_energy != labels_poison, poison_indices)\n",
    "\n",
    "#     save_name = f\"__NEW__{DATASET_NAME}-Energy-{'train' if TRAIN else 'test'}\"\n",
    "#     save_predicted_labels(labels_predicted_knn, save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 train \\\n",
    "BadNets\n",
    "\n",
    "\t50000 / 50000\n",
    "\t\t4618 \t 12732\n",
    "\t\t382 \t 32268\n",
    "\tPercentage of poisoned images (out of all poisoned) kept:  7.64%\n",
    "\tPercentage of clean images (out of all clean) kept:  71.71%\n",
    "\tPercentage of remaining poisoned images (out of all remaining):  1.17%\n",
    "\n",
    "CIFAR-10 test \\\n",
    "BadNets\n",
    "\n",
    "\t10000 / 10000\n",
    "\t\t920 \t 2585\n",
    "\t\t80 \t 6415\n",
    "\tPercentage of poisoned images (out of all poisoned) kept:  8.00%\n",
    "\tPercentage of clean images (out of all clean) kept:  71.28%\n",
    "\tPercentage of remaining poisoned images (out of all remaining):  1.23%\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 train \\\n",
    "WaNet\n",
    "\n",
    "\t50000 / 50000\n",
    "\t\t4725 \t 14096\n",
    "\t\t275 \t 30904\n",
    "\tPercentage of poisoned images (out of all poisoned) kept:  5.50%\n",
    "\tPercentage of clean images (out of all clean) kept:  68.68%\n",
    "\tPercentage of remaining poisoned images (out of all remaining):  0.88%\n",
    "\n",
    "CIFAR-10 test \\\n",
    "WaNet\n",
    "\n",
    "\t10000 / 10000\n",
    "\t\t951 \t 2851\n",
    "\t\t49 \t 6149\n",
    "\tPercentage of poisoned images (out of all poisoned) kept:  4.90%\n",
    "\tPercentage of clean images (out of all clean) kept:  68.32%\n",
    "\tPercentage of remaining poisoned images (out of all remaining):  0.79%\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 train \\\n",
    "SIG\n",
    "\n",
    "\t50000 / 50000\n",
    "\t\t2 \t 15777\n",
    "\t\t498 \t 33723\n",
    "\tPercentage of poisoned images (out of all poisoned) kept:  99.60%\n",
    "\tPercentage of clean images (out of all clean) kept:  68.13%\n",
    "\tPercentage of remaining poisoned images (out of all remaining):  1.46%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO cleanup, delete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.linear(x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimClrFeaturesDataset(VisionDataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if RUN_LOGREG:\n",
    "    \n",
    "#     BATCH_SIZE = 1024\n",
    "#     NUM_MODELS = 10\n",
    "#     EPOCHS = 60\n",
    "\n",
    "#     classifications = np.zeros((NUM_MODELS, len(features)))\n",
    "\n",
    "#     for num_model in range(NUM_MODELS):\n",
    "#         print(f\"Learning logistic regression model {num_model+1}/{NUM_MODELS}\")\n",
    "#         simclr_features_dataset = SimClrFeaturesDataset(features, labels_poison)\n",
    "#         dataloader = DataLoader(dataset=simclr_features_dataset, batch_size=BATCH_SIZE, shuffle=True) \n",
    "#         model = LogReg(128, 10)\n",
    "#         optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-1)\n",
    "#         criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "#         # train\n",
    "#         for epoch in range(EPOCHS):\n",
    "#             correct = 0\n",
    "#             for i, (batch_features, batch_labels) in enumerate(dataloader):\n",
    "#                 batch_features, batch_labels = batch_features.float(), batch_labels.long()\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = model(batch_features)\n",
    "#                 loss = criterion(outputs, batch_labels)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 correct += (predicted == batch_labels).sum()\n",
    "#             accuracy = 100 * (correct.item()) / len(simclr_features_dataset)\n",
    "#         print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "#         # get model classifications\n",
    "#         dataloader = DataLoader(dataset=simclr_features_dataset, batch_size=BATCH_SIZE, shuffle=False) \n",
    "#         for i, (batch_features, batch_labels) in enumerate(dataloader):\n",
    "#             batch_features, batch_labels = batch_features.float(), batch_labels.long()\n",
    "\n",
    "#             outputs = model(batch_features)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             predicted = predicted.numpy()\n",
    "#             classifications[num_model, (i*BATCH_SIZE):(i*BATCH_SIZE+len(predicted))] = predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if RUN_LOGREG:\n",
    "\n",
    "#     labels_poison_repeated = np.tile(labels_poison, (NUM_MODELS,1))\n",
    "#     misclassifications = np.sum(classifications != labels_poison_repeated, axis=0)\n",
    "\n",
    "#     misclassifications_poisoned = misclassifications[poison_indices]\n",
    "#     misclassifications_clean = misclassifications[np.invert(poison_indices)]\n",
    "\n",
    "#     # print(misclassifications_poisoned.shape)\n",
    "#     # print(misclassifications_clean.shape)\n",
    "\n",
    "#     # print(np.average(misclassifications_poisoned))\n",
    "#     # print(np.average(misclassifications_clean))\n",
    "\n",
    "#     bins = np.linspace(0, np.max(misclassifications), np.max(misclassifications))\n",
    "#     plt.hist(misclassifications_clean, bins, alpha=0.5, label='clean')\n",
    "#     plt.hist(misclassifications_poisoned, bins, alpha=0.5, label='poisoned')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if RUN_LOGREG:\n",
    "\n",
    "#     class_num = 1\n",
    "\n",
    "#     misclassifications_class = misclassifications[labels_poison == class_num]\n",
    "#     misclassifications_class_poisoned = misclassifications[np.logical_and(labels_poison == class_num, poison_indices)]\n",
    "#     misclassifications_class_clean = misclassifications[np.logical_and(labels_poison == class_num, np.invert(poison_indices))]\n",
    "\n",
    "#     print((labels_poison == class_num & np.invert(poison_indices)).shape)\n",
    "#     print(misclassifications_class.shape)\n",
    "#     print(misclassifications_class_poisoned.shape)\n",
    "#     print(misclassifications_class_clean.shape)\n",
    "\n",
    "#     bins = np.linspace(0, np.max(misclassifications_class), np.max(misclassifications_class))\n",
    "#     plt.hist(misclassifications_class_clean, bins, alpha=0.5, label='clean')\n",
    "#     plt.hist(misclassifications_class_poisoned, bins, alpha=0.5, label='poisoned')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disruptive detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_cleanse(features: np.array, poison_indices: np.array) -> np.array:\n",
    "    centroid = np.sum(features, axis=0) / features.shape[0]\n",
    "    distances = np.linalg.norm(features - centroid, axis=1)\n",
    "    \n",
    "    plot_histogram_poisoned(distances, poison_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def gauss_cleanse(features: np.array, discard_percentage: float, poison_indices: np.array = None) -> np.array:\n",
    "    mean = np.mean(features, axis=0)\n",
    "    cov = np.cov(features, rowvar=0)\n",
    "\n",
    "    probabilities = multivariate_normal.pdf(features, mean=mean, cov=cov, allow_singular=True)\n",
    "    probabilities[probabilities <= 0] = 1e-100\n",
    "    probabilities = -np.log(probabilities)\n",
    "\n",
    "    discard_line = np.percentile(probabilities, (1-discard_percentage)*100)\n",
    "    plot_histogram_poisoned(probabilities, poison_indices, separation_line=discard_line)\n",
    "\n",
    "    predicted_poison_indices = probabilities > discard_line\n",
    "    return predicted_poison_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poison reclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak binary classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoisonClassificationDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, poison_indices: np.array) -> None:\n",
    "        self.original_dataset = original_dataset\n",
    "        self.poison_indices = poison_indices\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.original_dataset)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.original_dataset[index][0], torch.tensor(self.poison_indices[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "class ConvolutionalBinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1568, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_binary_classifier(dataset: VisionDataset, predicted_poison_indices: np.array) -> nn.Module:\n",
    "    poison_classification_dataset = PoisonClassificationDataset(dataset, predicted_poison_indices)\n",
    "    # sampler for class imbalance\n",
    "    positives = sum([1 for _, target in poison_classification_dataset if target==1])\n",
    "    total = len(poison_classification_dataset)\n",
    "    positive_weight = 0.5 / positives\n",
    "    negative_weight = 0.5 / (total - positives)\n",
    "    weights = [positive_weight if target==1 else negative_weight for _, target in poison_classification_dataset]\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "    dataloader = DataLoader(poison_classification_dataset, batch_size=128, sampler=sampler)\n",
    "    model = ConvolutionalBinaryClassifier().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1-1e-4)\n",
    "    num_epochs = 10\n",
    "\n",
    "    for _ in tqdm(range(num_epochs)):\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.forward(inputs).squeeze(-1)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def binary_reclassification(dataset: VisionDataset, model: nn.Module):\n",
    "    predicted_poison_indices = np.zeros((len(dataset)))\n",
    "    batch_size = 128\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    for i, (inputs, _, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(inputs).squeeze(-1)\n",
    "            predictions = (logits>0.5).cpu().numpy()\n",
    "        predicted_poison_indices[i*batch_size : i*batch_size+len(predictions)] = predictions\n",
    "    return predicted_poison_indices==1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong multiclass classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, predicted_poison: np.array, transforms: torch.nn.Module = None):\n",
    "        self.data = [poison_dataset[i][0] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.labels = [poison_dataset[i][1] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transforms:\n",
    "            item = self.transforms(item)\n",
    "\n",
    "        return item, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiclass_classifier(dataset: VisionDataset, predicted_poison_indices: np.array) -> nn.Module:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    cleansed_dataset = CleansedDataset(dataset, predicted_poison_indices, transform_train)\n",
    "    dataloader = DataLoader(cleansed_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = ResNet18()\n",
    "    model.to(device)\n",
    "\n",
    "    epochs = 35\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100.*correct/total\n",
    "        scheduler.step()\n",
    "        print(epoch, train_loss, acc)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_reclassification(dataset: VisionDataset, model: nn.Module, true_labels: np.array):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_dataset = CleansedDataset(dataset, np.zeros(len(dataset)), transform_train)\n",
    "    \n",
    "    predicted_labels = np.zeros((len(dataset)))\n",
    "    batch_size = 128\n",
    "    dataloader = DataLoader(transform_dataset, batch_size=batch_size, shuffle=False)\n",
    "    for i, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(inputs)\n",
    "            predictions = torch.argmax(logits, 1).cpu().numpy()\n",
    "        predicted_labels[i*batch_size : i*batch_size+len(predictions)] = predictions\n",
    "    return predicted_labels!=true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final cleanse pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "dataset_name = \"wanet\"\n",
    "\n",
    "train = True\n",
    "simclr_model_name = f\"{dataset_name}-SimCLR.pt\"\n",
    "\n",
    "dataset, true_poison_indices = prepare_poison_dataset(dataset_name, train)\n",
    "simclr = load_simclr(simclr_model_name)\n",
    "features, labels_poison, labels_true = extract_simclr_features(simclr, dataset)\n",
    "num_classes = int(max(labels_poison).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparameters\n",
    "\n",
    "gauss_discard = 0.005\n",
    "tsne_perplexity = len(dataset) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_cleanse(features: np.array, discard_percentage: float, poison_indices: np.array = None) -> np.array:\n",
    "    mean = np.mean(features, axis=0)\n",
    "    cov = np.cov(features, rowvar=0)\n",
    "\n",
    "    probabilities = multivariate_normal.pdf(features, mean=mean, cov=cov, allow_singular=True)\n",
    "    probabilities[probabilities <= 0] = 1e-100\n",
    "    probabilities = -np.log(probabilities)\n",
    "\n",
    "    discard_line = np.percentile(probabilities, (1-discard_percentage)*100)\n",
    "    plot_histogram_poisoned(probabilities, poison_indices, separation_line=discard_line)\n",
    "\n",
    "    predicted_poison_indices = probabilities > discard_line\n",
    "    return predicted_poison_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poisons caught: 746/5000\n",
      "Clean caught: 6046/45000\n",
      "0/1\n",
      "Poisons caught: 792/5000\n",
      "Clean caught: 6818/45000\n",
      "0/2\n",
      "Poisons caught: 747/5000\n",
      "Clean caught: 7224/45000\n",
      "0/3\n",
      "Poisons caught: 1045/5000\n",
      "Clean caught: 7318/45000\n",
      "0/4\n",
      "Poisons caught: 870/5000\n",
      "Clean caught: 7221/45000\n",
      "0/5\n",
      "Poisons caught: 938/5000\n",
      "Clean caught: 6664/45000\n",
      "0/6\n",
      "Poisons caught: 786/5000\n",
      "Clean caught: 6141/45000\n",
      "0/7\n",
      "Poisons caught: 891/5000\n",
      "Clean caught: 6394/45000\n",
      "0/8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\multiarray.py:346\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(condition, x, y)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    inner(a, b, /)\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b)\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mwhere)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhere\u001b[39m(condition, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m    where(condition, [x, y], /)\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m           [ 0,  3, -1]])\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, x, y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Luka\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\multiarray.py\", line 346, in where\n",
      "    @array_function_from_c_func_and_dispatcher(_multiarray_umath.where)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poisons caught: 900/5000\n",
      "Clean caught: 6639/45000\n",
      "0/9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from scipy.stats import mode\n",
    "\n",
    "count = 0\n",
    "for i in range(1,50+1):\n",
    "\tkmeans = KMeans(n_clusters=50, init=\"k-means++\")\n",
    "\tkmeans.fit(features)\n",
    "\n",
    "\tpoison_predictions = kmeans.predict(features[true_poison_indices])\n",
    "\tpoison_cluster = mode(poison_predictions)[0][0]\n",
    "\tprint(f\"Poisons caught: {np.count_nonzero(poison_predictions==poison_cluster)}/{len(poison_predictions)}\")\n",
    "\n",
    "\tclean_predictions = kmeans.predict(features[np.invert(true_poison_indices)])\n",
    "\tprint(f\"Clean caught: {np.count_nonzero(clean_predictions==poison_cluster)}/{len(clean_predictions)}\")\n",
    "\n",
    "\t#centroids = {\"clusters\": np.mean(kmeans.cluster_centers_), \"dataset\": np.mean(features)}\n",
    "\tcentroids = {\"dataset\": np.mean(features)}\n",
    "\t#metrics = {\"cosine\": cosine, \"euclidean\": euclidean}\n",
    "\tmetrics = {\"euclidean\": euclidean}\n",
    "\n",
    "\tfor centroid_name, centroid in centroids.items():\n",
    "\t\tfor metric_name, metric in metrics.items():\n",
    "\t\t\tdistances = [metric(center, centroid) for center in kmeans.cluster_centers_]\n",
    "\t\t\t#print(f\"{metric_name}, {centroid_name}\")\n",
    "\t\t\t#print(f\"\\tAverage distance: {np.mean(distances)}\")\n",
    "\t\t\t#print(f\"\\tMax center distance: {max(distances)}\")\n",
    "\t\t\t#print(f\"\\tPoison center distance: {distances[poison_cluster]}\")\n",
    "\t\t\tif distances[poison_cluster] == max(distances):\n",
    "\t\t\t\tcount += 1\n",
    "\t\t\t#else:\n",
    "\t\t\t#\tprint(\"Poison distance is NOT farthest\")\n",
    "\t\t\tprint(f\"{count}/{i}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
