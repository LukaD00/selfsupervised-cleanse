{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import BadNetsDataset, WaNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "CLEANSE = True\n",
    "CLEANSED_LABELS_NAME = \"WaNet-kNN\" \n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "CHECKPOINT = \"\"\n",
    "DATASET = \"wanet\"  \n",
    "\n",
    "if DATASET == \"badnets\":\n",
    "    TARGET_CLASS = 1\n",
    "elif DATASET == \"wanet\":\n",
    "    TARGET_CLASS = 0\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, strategy: str = \"remove\"):\n",
    "        self.poison_dataset = poison_dataset\n",
    "        self.poison_indices = list(range(len(self.poison_dataset)))\n",
    "        \n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            self.predicted_labels = pickle.load(f)\n",
    "\n",
    "        assert strategy in [\"relabel\", \"remove\"]\n",
    "        self.strategy = strategy\n",
    "\n",
    "        poison_labels = [poison_label for _, poison_label in self.poison_dataset]\n",
    "        if self.strategy == \"remove\":\n",
    "            self.indices = [index for index in self.poison_indices if poison_labels[index]==self.predicted_labels[index]]\n",
    "        elif self.strategy == \"relabel\":\n",
    "            self.indices = self.poison_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index > len(self):\n",
    "            return IndexError()\n",
    "        while index not in self.indices:\n",
    "            index += 1\n",
    "\n",
    "        item = self.poison_dataset[index][0]\n",
    "        label = self.predicted_labels[index]\n",
    "\n",
    "        return item, label\n",
    "        \n",
    "\n",
    "class SkipLabelDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, skip_class: int):\n",
    "        self.return_as_pil = type(original_dataset[0][0]) is Image.Image\n",
    "\n",
    "        targets = np.array(original_dataset.targets)\n",
    "        self.data = original_dataset.data[targets != skip_class]\n",
    "        self.targets = targets[targets != skip_class].tolist()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.return_as_pil:\n",
    "            data = Image.fromarray(data)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_train_clean = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=True, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    train_poison_dataset = BadNetsDataset(train_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    train_poison_dataset = WaNetDataset(train_dataset, TARGET_CLASS, seed=1, transform=transform_train, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "if CLEANSE:\n",
    "    train_poison_dataset = CleansedDataset(train_poison_dataset, CLEANSED_LABELS_NAME + \"-train\", strategy=\"remove\")\n",
    "trainloader = DataLoader(train_poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    test_poison_dataset = BadNetsDataset(test_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_test, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    test_poison_dataset = WaNetDataset(test_dataset, TARGET_CLASS, seed=1, transform=transform_test, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "if CLEANSE:\n",
    "    test_poison_dataset = CleansedDataset(test_poison_dataset, CLEANSED_LABELS_NAME + \"-test\", strategy=\"remove\")\n",
    "testloader = DataLoader(test_poison_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "epochs = 35\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/', CHECKPOINT)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")\n",
    "    print(f\"{start_epoch = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "save_name = \"NEW-Resnet-18.pt\"\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, save_name)\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 549.854455947876 Test Loss: 87.19707107543945\n",
      "\tTraining Acc: 30.28741555021184 Test Acc: 42.391304347826086\n",
      "\tTime Taken: 0.8329390565554301 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 376.51978331804276 Test Loss: 67.06172555685043\n",
      "\tTraining Acc: 49.85400206114737 Test Acc: 56.26086956521739\n",
      "\tTime Taken: 0.8239683389663697 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 298.6596665382385 Test Loss: 54.11909508705139\n",
      "\tTraining Acc: 60.259933585251346 Test Acc: 65.8695652173913\n",
      "\tTime Taken: 0.8316778302192688 minutes\n",
      "Epoch [4/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 237.15485471487045 Test Loss: 51.03674840927124\n",
      "\tTraining Acc: 69.0054963929921 Test Acc: 67.30434782608695\n",
      "\tTime Taken: 0.8327959378560384 minutes\n",
      "Epoch [5/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 195.90538731217384 Test Loss: 56.4392209649086\n",
      "\tTraining Acc: 74.66220084736058 Test Acc: 67.79710144927536\n",
      "\tTime Taken: 0.8322150389353434 minutes\n",
      "Epoch [6/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 164.01421770453453 Test Loss: 37.406317591667175\n",
      "\tTraining Acc: 78.79022100080155 Test Acc: 76.69565217391305\n",
      "\tTime Taken: 0.8339649160703023 minutes\n",
      "Epoch [7/35]\t\n",
      "\tTraining Loss: 137.54544532299042 Test Loss: 40.703293800354004\n",
      "\tTraining Acc: 82.27126989579754 Test Acc: 74.76811594202898\n",
      "\tTime Taken: 0.8313085238138834 minutes\n",
      "Epoch [8/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 113.35298185050488 Test Loss: 30.114151656627655\n",
      "\tTraining Acc: 85.33722661170273 Test Acc: 81.4927536231884\n",
      "\tTime Taken: 0.8311576843261719 minutes\n",
      "Epoch [9/35]\t\n",
      "\tTraining Loss: 99.68312679231167 Test Loss: 29.50823250412941\n",
      "\tTraining Acc: 87.24378793083706 Test Acc: 81.28985507246377\n",
      "\tTime Taken: 0.8321264704068502 minutes\n",
      "Epoch [10/35]\t\n",
      "\tTraining Loss: 89.03024338185787 Test Loss: 32.74903982877731\n",
      "\tTraining Acc: 88.48906446810948 Test Acc: 80.92753623188406\n",
      "\tTime Taken: 0.829426630338033 minutes\n",
      "Epoch [11/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 81.02700535953045 Test Loss: 22.44333216547966\n",
      "\tTraining Acc: 89.4652467651437 Test Acc: 86.42028985507247\n",
      "\tTime Taken: 0.8321521441141765 minutes\n",
      "Epoch [12/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 76.35597041249275 Test Loss: 19.34725321829319\n",
      "\tTraining Acc: 90.15515859383946 Test Acc: 87.52173913043478\n",
      "\tTime Taken: 0.8325181841850281 minutes\n",
      "Epoch [13/35]\t\n",
      "\tTraining Loss: 73.79313306510448 Test Loss: 35.89132523536682\n",
      "\tTraining Acc: 90.43570365281117 Test Acc: 79.8840579710145\n",
      "\tTime Taken: 0.8312604784965515 minutes\n",
      "Epoch [14/35]\t\n",
      "\tTraining Loss: 69.16563089191914 Test Loss: 22.50210127234459\n",
      "\tTraining Acc: 91.01396999885492 Test Acc: 86.18840579710145\n",
      "\tTime Taken: 0.8294769605000814 minutes\n",
      "Epoch [15/35]\t\n",
      "\tTraining Loss: 68.21521204710007 Test Loss: 20.518476247787476\n",
      "\tTraining Acc: 91.29737776251002 Test Acc: 87.5072463768116\n",
      "\tTime Taken: 0.8301019827524821 minutes\n",
      "Epoch [16/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 66.53860964626074 Test Loss: 17.571879491209984\n",
      "\tTraining Acc: 91.56933470743158 Test Acc: 88.91304347826087\n",
      "\tTime Taken: 0.831392749150594 minutes\n",
      "Epoch [17/35]\t\n",
      "\tTraining Loss: 65.23311650007963 Test Loss: 27.648854553699493\n",
      "\tTraining Acc: 91.71819535096759 Test Acc: 84.55072463768116\n",
      "\tTime Taken: 0.8294954737027486 minutes\n",
      "Epoch [18/35]\t\n",
      "\tTraining Loss: 62.967292472720146 Test Loss: 24.015665277838707\n",
      "\tTraining Acc: 92.15046375815871 Test Acc: 85.6231884057971\n",
      "\tTime Taken: 0.8312857747077942 minutes\n",
      "Epoch [19/35]\t\n",
      "\tTraining Loss: 61.00304702669382 Test Loss: 22.593591421842575\n",
      "\tTraining Acc: 92.19626703309287 Test Acc: 86.2463768115942\n",
      "\tTime Taken: 0.8307229081789652 minutes\n",
      "Epoch [20/35]\t\n",
      "\tTraining Loss: 60.2498382255435 Test Loss: 25.143224358558655\n",
      "\tTraining Acc: 92.3222260391618 Test Acc: 85.43478260869566\n",
      "\tTime Taken: 0.8399063309033712 minutes\n",
      "Epoch [21/35]\t\n",
      "\tTraining Loss: 58.267008163034916 Test Loss: 22.1733638048172\n",
      "\tTraining Acc: 92.68578953395168 Test Acc: 86.3913043478261\n",
      "\tTime Taken: 0.8360889593760172 minutes\n",
      "Epoch [22/35]\t\n",
      "\tTraining Loss: 58.06489619612694 Test Loss: 27.406519562005997\n",
      "\tTraining Acc: 92.5111645482652 Test Acc: 84.8840579710145\n",
      "\tTime Taken: 0.8363541642824809 minutes\n",
      "Epoch [23/35]\t\n",
      "\tTraining Loss: 57.556077890098095 Test Loss: 19.56734851002693\n",
      "\tTraining Acc: 92.62281003091721 Test Acc: 88.73913043478261\n",
      "\tTime Taken: 0.83767751455307 minutes\n",
      "Epoch [24/35]\t\n",
      "\tTraining Loss: 55.243445456027985 Test Loss: 18.05409674346447\n",
      "\tTraining Acc: 93.06652925684186 Test Acc: 88.82608695652173\n",
      "\tTime Taken: 0.8369989673296611 minutes\n",
      "Epoch [25/35]\t\n",
      "\tTraining Loss: 54.18961451202631 Test Loss: 25.76040391623974\n",
      "\tTraining Acc: 93.09229359899233 Test Acc: 86.01449275362319\n",
      "\tTime Taken: 0.8375764052073161 minutes\n",
      "Epoch [26/35]\t\n",
      "\tTraining Loss: 53.73906219005585 Test Loss: 27.72549442946911\n",
      "\tTraining Acc: 93.20966449101111 Test Acc: 85.0\n",
      "\tTime Taken: 0.838601803779602 minutes\n",
      "Epoch [27/35]\t\n",
      "\tTraining Loss: 54.75666479766369 Test Loss: 21.84932278841734\n",
      "\tTraining Acc: 93.13237146455971 Test Acc: 87.05797101449275\n",
      "\tTime Taken: 0.8368499358495076 minutes\n",
      "Epoch [28/35]\t\n",
      "\tTraining Loss: 52.73826214671135 Test Loss: 20.119207233190536\n",
      "\tTraining Acc: 93.32989808771327 Test Acc: 88.21739130434783\n",
      "\tTime Taken: 0.829905649026235 minutes\n",
      "Epoch [29/35]\t\n",
      "\tTraining Loss: 51.13026054576039 Test Loss: 22.70530593395233\n",
      "\tTraining Acc: 93.61616855605176 Test Acc: 87.1159420289855\n",
      "\tTime Taken: 0.8320047497749329 minutes\n",
      "Epoch [30/35]\t\n",
      "\tTraining Loss: 52.29409138858318 Test Loss: 20.167103052139282\n",
      "\tTraining Acc: 93.42150463758159 Test Acc: 87.79710144927536\n",
      "\tTime Taken: 0.8310959458351135 minutes\n",
      "Epoch [31/35]\t\n",
      "\tTraining Loss: 50.01786778867245 Test Loss: 35.94432529807091\n",
      "\tTraining Acc: 93.65624642161914 Test Acc: 80.68115942028986\n",
      "\tTime Taken: 0.8305326461791992 minutes\n",
      "Epoch [32/35]\t\n",
      "\tTraining Loss: 49.99254958331585 Test Loss: 22.3969519585371\n",
      "\tTraining Acc: 93.67342264971946 Test Acc: 87.10144927536231\n",
      "\tTime Taken: 0.870260481039683 minutes\n",
      "Epoch [33/35]\t\n",
      "\tTraining Loss: 48.80304744839668 Test Loss: 23.353025928139687\n",
      "\tTraining Acc: 93.833734111989 Test Acc: 86.68115942028986\n",
      "\tTime Taken: 0.8916331966718037 minutes\n",
      "Epoch [34/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 49.12863615900278 Test Loss: 15.221084468066692\n",
      "\tTraining Acc: 93.77648001832131 Test Acc: 90.26086956521739\n",
      "\tTime Taken: 0.809667150179545 minutes\n",
      "Epoch [35/35]\t\n",
      "\tTraining Loss: 47.956674344837666 Test Loss: 26.30203242599964\n",
      "\tTraining Acc: 93.89098820565671 Test Acc: 86.10144927536231\n",
      "\tTime Taken: 0.8093791524569194 minutes\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "        stime = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "        test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "        print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "        time_taken = (time.time()-stime)/60\n",
    "        print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # loading the best model checkpoint from training before testing\n",
    "    out = os.path.join('./saved_models/', save_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "test_dataset = SkipLabelDataset(test_dataset, TARGET_CLASS)\n",
    "if DATASET == \"badnets\":\n",
    "    full_poisoned_test_dataset = BadNetsDataset(test_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_poison, poisoning_rate=1.0, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    full_poisoned_test_dataset = WaNetDataset(test_dataset, TARGET_CLASS, seed=1, transform=transform_poison, poisoning_rate=1.0, noise_rate=0.0, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "\n",
    "\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 79.18\n",
      "Attack Success Rate (ASR): 4.544444444444444\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 90.49\n",
    "    Attack Success Rate (ASR): 0.33 \n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.88\n",
    "    Attack Success Rate (ASR): 99.66\n",
    "\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "kNN \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.76\n",
    "    Attack Success Rate (ASR): 0.92\n",
    "\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "Energy \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.49\n",
    "    Attack Success Rate (ASR): 1.91\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "ResNet-18 \\\n",
    "\n",
    "    Clean Accuracy (C-Acc): 80.93\n",
    "    Attack Success Rate (ASR): 79.58\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "kNN \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.18\n",
    "    Attack Success Rate (ASR): 4.54\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "Energy \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.52\n",
    "    Attack Success Rate (ASR): 1.73"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
