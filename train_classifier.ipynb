{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import BadNetsDataset, WaNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128\n",
    "epochs = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, strategy: str = \"remove\"):\n",
    "        self.poison_dataset = poison_dataset\n",
    "        self.poison_indices = list(range(len(self.poison_dataset)))\n",
    "        \n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            self.predicted_labels = pickle.load(f)\n",
    "\n",
    "        assert strategy in [\"relabel\", \"remove\"]\n",
    "        self.strategy = strategy\n",
    "\n",
    "        poison_labels = [poison_label for _, poison_label in self.poison_dataset]\n",
    "        if self.strategy == \"remove\":\n",
    "            self.indices = [index for index in self.poison_indices if poison_labels[index]==self.predicted_labels[index]]\n",
    "        elif self.strategy == \"relabel\":\n",
    "            self.indices = self.poison_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index > len(self):\n",
    "            return IndexError()\n",
    "        while index not in self.indices:\n",
    "            index += 1\n",
    "\n",
    "        item = self.poison_dataset[index][0]\n",
    "        label = self.predicted_labels[index]\n",
    "\n",
    "        return item, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_train_clean = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=True, download=False)\n",
    "#train_dataset_transforms = torchvision.datasets.CIFAR10(root='C:/Datasets', train=True, download=False, transform=transform_train_clean)\n",
    "#train_poison_dataset = BadNetsDataset(train_dataset, 0, \"triggers/trigger_white.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "#train_poison_dataset = WaNetDataset(train_dataset, 0, seed=1, transform=transform_train, return_original_label=False)\n",
    "train_poison_dataset = BadNetsDataset(train_dataset, 1, \"triggers/trigger_10.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "train_cleansed_dataset = CleansedDataset(train_poison_dataset, \"BadNets2-SimCLR-train\", strategy=\"remove\")\n",
    "trainloader = DataLoader(train_cleansed_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "#test_dataset_transforms = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_test_clean)\n",
    "#test_poison_dataset = BadNetsDataset(test_dataset, 0, \"triggers/trigger_white.png\", seed=1, transform=transform_test, return_original_label=False)\n",
    "test_poison_dataset = WaNetDataset(test_dataset, 0, seed=1, transform=transform_test, return_original_label=False)\n",
    "test_poison_dataset = BadNetsDataset(test_dataset, 1, \"triggers/trigger_10.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "test_cleansed_dataset = CleansedDataset(test_poison_dataset, \"BadNets2-SimCLR-test\", strategy=\"remove\")\n",
    "testloader = DataLoader(test_cleansed_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "load_checkpoint = False\n",
    "checkpoint_name = \"Resnet-18.pt\"\n",
    "\n",
    "if load_checkpoint:\n",
    "    out = os.path.join('./saved_models/', checkpoint_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "best_model = None\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, f\"Resnet-18.pt\")\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 518.6843427419662 Test Loss: 77.31429040431976\n",
      "\tTraining Acc: 34.97757847533632 Test Acc: 48.799204206337926\n",
      "\tTime Taken: 1.2405133883158366 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 344.84946662187576 Test Loss: 63.43118244409561\n",
      "\tTraining Acc: 54.99159192825112 Test Acc: 58.7750461844536\n",
      "\tTime Taken: 1.2282502373059592 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 258.39552080631256 Test Loss: 52.13862818479538\n",
      "\tTraining Acc: 67.07399103139014 Test Acc: 64.61560324001705\n",
      "\tTime Taken: 1.22998206615448 minutes\n",
      "Epoch [4/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 198.731981664896 Test Loss: 36.54022166132927\n",
      "\tTraining Acc: 74.83464125560538 Test Acc: 76.31092795225238\n",
      "\tTime Taken: 1.2199222922325135 minutes\n",
      "Epoch [5/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 159.42823618650436 Test Loss: 34.74977269768715\n",
      "\tTraining Acc: 79.74495515695067 Test Acc: 77.90251527639619\n",
      "\tTime Taken: 1.0851686199506123 minutes\n",
      "Epoch [6/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 125.69885623455048 Test Loss: 31.714689701795578\n",
      "\tTraining Acc: 84.13116591928251 Test Acc: 80.50305527923831\n",
      "\tTime Taken: 0.7939562280972798 minutes\n",
      "Epoch [7/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 104.43616212904453 Test Loss: 22.288285732269287\n",
      "\tTraining Acc: 86.80773542600897 Test Acc: 85.56202927383828\n",
      "\tTime Taken: 0.7944571057955424 minutes\n",
      "Epoch [8/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 87.16702850162983 Test Loss: 21.857883617281914\n",
      "\tTraining Acc: 88.99663677130044 Test Acc: 85.61887167827199\n",
      "\tTime Taken: 0.7889689405759176 minutes\n",
      "Epoch [9/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 80.63361385464668 Test Loss: 21.483380883932114\n",
      "\tTraining Acc: 89.85706278026906 Test Acc: 86.6278243569703\n",
      "\tTime Taken: 0.8067665259043376 minutes\n",
      "Epoch [10/35]\t\n",
      "\tTraining Loss: 72.66278118640184 Test Loss: 21.513006031513214\n",
      "\tTraining Acc: 90.92208520179372 Test Acc: 86.49992894699446\n",
      "\tTime Taken: 0.7849822839101156 minutes\n",
      "Epoch [11/35]\t\n",
      "\tTraining Loss: 67.43471022695303 Test Loss: 22.96212200820446\n",
      "\tTraining Acc: 91.69843049327355 Test Acc: 86.10203211595851\n",
      "\tTime Taken: 0.785721504688263 minutes\n",
      "Epoch [12/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 65.28948047757149 Test Loss: 21.7024048268795\n",
      "\tTraining Acc: 91.81053811659193 Test Acc: 87.06835299133154\n",
      "\tTime Taken: 0.7867055217425029 minutes\n",
      "Epoch [13/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 63.25470998138189 Test Loss: 17.70744927227497\n",
      "\tTraining Acc: 92.27298206278027 Test Acc: 88.61730851215006\n",
      "\tTime Taken: 0.7867851098378499 minutes\n",
      "Epoch [14/35]\t\n",
      "\tTraining Loss: 59.55307240784168 Test Loss: 37.75647187232971\n",
      "\tTraining Acc: 92.6317264573991 Test Acc: 79.42304959499786\n",
      "\tTime Taken: 0.7868890007336934 minutes\n",
      "Epoch [15/35]\t\n",
      "\tTraining Loss: 58.65931595116854 Test Loss: 23.298100098967552\n",
      "\tTraining Acc: 92.67096412556054 Test Acc: 86.40045473923547\n",
      "\tTime Taken: 0.7974692185719808 minutes\n",
      "Epoch [16/35]\t\n",
      "\tTraining Loss: 58.81448461860418 Test Loss: 18.78835316002369\n",
      "\tTraining Acc: 92.7914798206278 Test Acc: 88.48941310217423\n",
      "\tTime Taken: 0.7864249149958292 minutes\n",
      "Epoch [17/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 55.55715002864599 Test Loss: 16.929775588214397\n",
      "\tTraining Acc: 93.10257847533633 Test Acc: 89.68310359528208\n",
      "\tTime Taken: 0.7888645052909851 minutes\n",
      "Epoch [18/35]\t\n",
      "\tTraining Loss: 52.930378176271915 Test Loss: 27.288694500923157\n",
      "\tTraining Acc: 93.54820627802691 Test Acc: 85.07886883615177\n",
      "\tTime Taken: 0.8133624911308288 minutes\n",
      "Epoch [19/35]\t\n",
      "\tTraining Loss: 54.46209263801575 Test Loss: 23.590235471725464\n",
      "\tTraining Acc: 93.27634529147981 Test Acc: 86.47150774477761\n",
      "\tTime Taken: 0.7567057053248087 minutes\n",
      "Epoch [20/35]\t\n",
      "\tTraining Loss: 52.397922679781914 Test Loss: 17.52827349305153\n",
      "\tTraining Acc: 93.39686098654708 Test Acc: 88.78783572545119\n",
      "\tTime Taken: 0.7535236318906148 minutes\n",
      "Epoch [21/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 50.85157661512494 Test Loss: 16.915076598525047\n",
      "\tTraining Acc: 93.78923766816143 Test Acc: 89.96731561745062\n",
      "\tTime Taken: 0.786070175965627 minutes\n",
      "Epoch [22/35]\t\n",
      "\tTraining Loss: 47.93659022822976 Test Loss: 17.50057227909565\n",
      "\tTraining Acc: 94.19002242152466 Test Acc: 88.95836293875232\n",
      "\tTime Taken: 0.7806967258453369 minutes\n",
      "Epoch [23/35]\t\n",
      "\tTraining Loss: 47.14911051094532 Test Loss: 19.122094944119453\n",
      "\tTraining Acc: 94.29652466367713 Test Acc: 88.92994173653545\n",
      "\tTime Taken: 0.7805871327718099 minutes\n",
      "Epoch [24/35]\t\n",
      "\tTraining Loss: 50.859326511621475 Test Loss: 16.906641647219658\n",
      "\tTraining Acc: 93.82286995515695 Test Acc: 89.54099758419781\n",
      "\tTime Taken: 0.7800468802452087 minutes\n",
      "Epoch [25/35]\t\n",
      "\tTraining Loss: 48.48886403813958 Test Loss: 21.6445721834898\n",
      "\tTraining Acc: 93.99103139013452 Test Acc: 86.55677135142817\n",
      "\tTime Taken: 0.7816293398539226 minutes\n",
      "Epoch [26/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 47.09040022268891 Test Loss: 16.39932158589363\n",
      "\tTraining Acc: 94.2432735426009 Test Acc: 90.20889583629388\n",
      "\tTime Taken: 0.7822823723157247 minutes\n",
      "Epoch [27/35]\t\n",
      "\tTraining Loss: 44.427294567227364 Test Loss: 21.037054106593132\n",
      "\tTraining Acc: 94.52914798206278 Test Acc: 87.5230922268012\n",
      "\tTime Taken: 0.780905568599701 minutes\n",
      "Epoch [28/35]\t\n",
      "\tTraining Loss: 45.60698988288641 Test Loss: 29.428501814603806\n",
      "\tTraining Acc: 94.375 Test Acc: 83.10359528208043\n",
      "\tTime Taken: 0.7811022559801738 minutes\n",
      "Epoch [29/35]\t\n",
      "\tTraining Loss: 45.89264253154397 Test Loss: 18.71851748228073\n",
      "\tTraining Acc: 94.27970852017937 Test Acc: 89.38468097200511\n",
      "\tTime Taken: 0.77977454662323 minutes\n",
      "Epoch [30/35]\t\n",
      "\tTraining Loss: 45.24286475777626 Test Loss: 22.273142501711845\n",
      "\tTraining Acc: 94.32455156950672 Test Acc: 87.06835299133154\n",
      "\tTime Taken: 0.7917054812113444 minutes\n",
      "Epoch [31/35]\t\n",
      "\tTraining Loss: 43.39039781689644 Test Loss: 17.847330659627914\n",
      "\tTraining Acc: 94.73934977578476 Test Acc: 88.88730993321018\n",
      "\tTime Taken: 0.8177850246429443 minutes\n",
      "Epoch [32/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 43.60759276896715 Test Loss: 14.051767989993095\n",
      "\tTraining Acc: 94.75056053811659 Test Acc: 91.7578513571124\n",
      "\tTime Taken: 0.8231435179710388 minutes\n",
      "Epoch [33/35]\t\n",
      "\tTraining Loss: 41.4751549102366 Test Loss: 18.92601391673088\n",
      "\tTraining Acc: 94.87387892376681 Test Acc: 89.01520534318601\n",
      "\tTime Taken: 0.7823874195416768 minutes\n",
      "Epoch [34/35]\t\n",
      "\tTraining Loss: 44.07116971164942 Test Loss: 17.50060448795557\n",
      "\tTraining Acc: 94.54316143497758 Test Acc: 89.65468239306523\n",
      "\tTime Taken: 0.789412812391917 minutes\n",
      "Epoch [35/35]\t\n",
      "\tTraining Loss: 42.523839704692364 Test Loss: 16.76936488598585\n",
      "\tTraining Acc: 94.80381165919283 Test Acc: 90.09521102742646\n",
      "\tTime Taken: 0.7818596680959066 minutes\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs+1):\n",
    "    print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "    stime = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "    test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "    print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "    time_taken = (time.time()-stime)/60\n",
    "    print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider loading the best model checkpoint before running\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "#full_poisoned_test_dataset = BadNetsDataset(test_dataset, 0, \"triggers/trigger_white.png\", seed=1, transform=transform_train, poisoning_rate=1.0, return_original_label=False)\n",
    "#full_poisoned_test_dataset = WaNetDataset(test_dataset, 0, seed=1, transform=transform_poison, poisoning_rate=1.0, noise_rate=0.0, return_original_label=False)\n",
    "full_poisoned_test_dataset = BadNetsDataset(test_dataset, 1, \"triggers/trigger_10.png\", seed=1, transform=transform_poison, poisoning_rate=1.0, return_original_label=False)\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 77.89\n",
      "Attack Success Rate (ASR): 10.92\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 90.49\n",
    "    Attack Success Rate (ASR): 10.63\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 89.19\n",
    "    Attack Success Rate (ASR): 62.01\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "SimCLR_300 \\\n",
    "ResNet-18 \\\n",
    "\n",
    "    Clean Accuracy (C-Acc): 84.16\n",
    "    Attack Success Rate (ASR): 9.25\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets2 \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.06\n",
    "    Attack Success Rate (ASR): 98.9\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets2 \\\n",
    "SimCLR_300 \\\n",
    "ResNet-18 \\\n",
    "\n",
    "    Clean Accuracy (C-Acc): 77.89\n",
    "    Attack Success Rate (ASR): 10.92\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "ResNet-18 \\\n",
    "\n",
    "    Clean Accuracy (C-Acc): 83.32\n",
    "    Attack Success Rate (ASR): 83.46\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "SimCLR_300 \\\n",
    "ResNet-18 \\\n",
    "\n",
    "    Clean Accuracy (C-Acc): 83.08\n",
    "    Attack Success Rate (ASR): 12.50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
