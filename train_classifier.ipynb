{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import BadNetsDataset, WaNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "CHECKPOINT = \"WaNet-Resnet-kNN.pt\"\n",
    "DATASET = \"wanet\"  # badnets, wanet\n",
    "\n",
    "if DATASET == \"badnets\":\n",
    "    TARGET_CLASS = 1\n",
    "elif DATASET == \"wanet\":\n",
    "    TARGET_CLASS = 0\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "\n",
    "CLEANSED_LABELS_NAME = \"WaNet-Energy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, strategy: str = \"remove\"):\n",
    "        self.poison_dataset = poison_dataset\n",
    "        self.poison_indices = list(range(len(self.poison_dataset)))\n",
    "        \n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            self.predicted_labels = pickle.load(f)\n",
    "\n",
    "        assert strategy in [\"relabel\", \"remove\"]\n",
    "        self.strategy = strategy\n",
    "\n",
    "        poison_labels = [poison_label for _, poison_label in self.poison_dataset]\n",
    "        if self.strategy == \"remove\":\n",
    "            self.indices = [index for index in self.poison_indices if poison_labels[index]==self.predicted_labels[index]]\n",
    "        elif self.strategy == \"relabel\":\n",
    "            self.indices = self.poison_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index > len(self):\n",
    "            return IndexError()\n",
    "        while index not in self.indices:\n",
    "            index += 1\n",
    "\n",
    "        item = self.poison_dataset[index][0]\n",
    "        label = self.predicted_labels[index]\n",
    "\n",
    "        return item, label\n",
    "        \n",
    "\n",
    "class SkipLabelDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, skip_class: int):\n",
    "        self.return_as_pil = type(original_dataset[0][0]) is Image.Image\n",
    "\n",
    "        targets = np.array(original_dataset.targets)\n",
    "        self.data = original_dataset.data[targets != skip_class]\n",
    "        self.targets = targets[targets != skip_class].tolist()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.return_as_pil:\n",
    "            data = Image.fromarray(data)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_train_clean = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=True, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    train_poison_dataset = BadNetsDataset(train_dataset, 1, \"triggers/trigger_10.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    train_poison_dataset = WaNetDataset(train_dataset, 0, seed=1, transform=transform_train, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "train_cleansed_dataset = CleansedDataset(train_poison_dataset, CLEANSED_LABELS_NAME + \"-train\", strategy=\"remove\")\n",
    "trainloader = DataLoader(train_cleansed_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    test_poison_dataset = BadNetsDataset(test_dataset, 1, \"triggers/trigger_10.png\", seed=1, transform=transform_test, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    test_poison_dataset = WaNetDataset(test_dataset, 0, seed=1, transform=transform_test, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "test_cleansed_dataset = CleansedDataset(test_poison_dataset, CLEANSED_LABELS_NAME + \"-test\", strategy=\"remove\")\n",
    "testloader = DataLoader(test_cleansed_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "epochs = 35\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint\n",
      "start_epoch = 27\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "load_checkpoint = True\n",
    "checkpoint_name = CHECKPOINT\n",
    "\n",
    "if load_checkpoint:\n",
    "    out = os.path.join('./saved_models/', checkpoint_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")\n",
    "    print(f\"{start_epoch = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "best_model = None\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, f\"Resnet-18.pt\")\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "        stime = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "        test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "        print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "        time_taken = (time.time()-stime)/60\n",
    "        print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider loading the best model checkpoint before running\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "test_dataset = SkipLabelDataset(test_dataset, TARGET_CLASS)\n",
    "if DATASET == \"badnets\":\n",
    "    full_poisoned_test_dataset = BadNetsDataset(test_dataset, 1, \"triggers/trigger_10.png\", seed=1, transform=transform_poison, poisoning_rate=1.0, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    full_poisoned_test_dataset = WaNetDataset(test_dataset, 0, seed=1, transform=transform_poison, poisoning_rate=1.0, noise_rate=0.0, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "\n",
    "\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 83.08\n",
      "Attack Success Rate (ASR): 57.11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 90.49\n",
    "    Attack Success Rate (ASR): 0.33 \n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.88\n",
    "    Attack Success Rate (ASR): 99.66\n",
    "\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "kNN \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.76\n",
    "    Attack Success Rate (ASR): 0.92\n",
    "\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "Energy \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.49\n",
    "    Attack Success Rate (ASR): 1.91\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "ResNet-18 \\\n",
    "\n",
    "    Clean Accuracy (C-Acc): 83.32\n",
    "    Attack Success Rate (ASR): 83.46 #td\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "kNN \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 83.08\n",
    "    Attack Success Rate (ASR): 12.50 #td\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "Energy \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.52\n",
    "    Attack Success Rate (ASR): 1.73"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
