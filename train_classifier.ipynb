{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import BadNetsDataset, WaNetDataset, SIGDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "CLEANSE = False\n",
    "CLEANSED_LABELS_NAME = \"WaNet-kNN\" \n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "CHECKPOINT = \"\"\n",
    "DATASET = \"sig\"  \n",
    "\n",
    "if DATASET == \"badnets\":\n",
    "    TARGET_CLASS = 1\n",
    "elif DATASET == \"wanet\":\n",
    "    TARGET_CLASS = 0\n",
    "elif DATASET == \"sig\":\n",
    "    TARGET_CLASS = 0\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, strategy: str = \"remove\"):\n",
    "        self.poison_dataset = poison_dataset\n",
    "        self.poison_indices = list(range(len(self.poison_dataset)))\n",
    "        \n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            self.predicted_labels = pickle.load(f)\n",
    "\n",
    "        assert strategy in [\"relabel\", \"remove\"]\n",
    "        self.strategy = strategy\n",
    "\n",
    "        poison_labels = [poison_label for _, poison_label in self.poison_dataset]\n",
    "        if self.strategy == \"remove\":\n",
    "            self.indices = [index for index in self.poison_indices if poison_labels[index]==self.predicted_labels[index]]\n",
    "        elif self.strategy == \"relabel\":\n",
    "            self.indices = self.poison_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index > len(self):\n",
    "            return IndexError()\n",
    "        while index not in self.indices:\n",
    "            index += 1\n",
    "\n",
    "        item = self.poison_dataset[index][0]\n",
    "        label = self.predicted_labels[index]\n",
    "\n",
    "        return item, label\n",
    "        \n",
    "\n",
    "class SkipLabelDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, skip_class: int):\n",
    "        self.return_as_pil = type(original_dataset[0][0]) is Image.Image\n",
    "\n",
    "        targets = np.array(original_dataset.targets)\n",
    "        self.data = original_dataset.data[targets != skip_class]\n",
    "        self.targets = targets[targets != skip_class].tolist()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.return_as_pil:\n",
    "            data = Image.fromarray(data)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_train_clean = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=True, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    train_poison_dataset = BadNetsDataset(train_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    train_poison_dataset = WaNetDataset(train_dataset, TARGET_CLASS, seed=1, transform=transform_train, return_original_label=False)\n",
    "elif DATASET == \"sig\":\n",
    "    train_poison_dataset = SIGDataset(train_dataset, TARGET_CLASS, 20, 6, seed=1, transform=transform_train, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "if CLEANSE:\n",
    "    train_poison_dataset = CleansedDataset(train_poison_dataset, CLEANSED_LABELS_NAME + \"-train\", strategy=\"remove\")\n",
    "trainloader = DataLoader(train_poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    test_poison_dataset = BadNetsDataset(test_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_test, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    test_poison_dataset = WaNetDataset(test_dataset, TARGET_CLASS, seed=1, transform=transform_test, return_original_label=False)\n",
    "elif DATASET == \"sig\":\n",
    "    test_poison_dataset = SIGDataset(test_dataset, TARGET_CLASS, 20, 6, seed=1, transform=transform_test, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "if CLEANSE:\n",
    "    test_poison_dataset = CleansedDataset(test_poison_dataset, CLEANSED_LABELS_NAME + \"-test\", strategy=\"remove\")\n",
    "testloader = DataLoader(test_poison_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "epochs = 35\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/', CHECKPOINT)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")\n",
    "    print(f\"{start_epoch = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "save_name = \"NEW-Resnet-18.pt\"\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, save_name)\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 693.2328153848648 Test Loss: 117.69663679599762\n",
      "\tTraining Acc: 35.258 Test Acc: 44.53\n",
      "\tTime Taken: 0.8992165843645732 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 522.3284000754356 Test Loss: 97.87184262275696\n",
      "\tTraining Acc: 51.338 Test Acc: 56.16\n",
      "\tTime Taken: 0.8778336763381958 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 430.0914623737335 Test Loss: 89.58713412284851\n",
      "\tTraining Acc: 60.554 Test Acc: 59.88\n",
      "\tTime Taken: 0.8775244792302449 minutes\n",
      "Epoch [4/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 364.902844786644 Test Loss: 79.27815181016922\n",
      "\tTraining Acc: 67.07 Test Acc: 64.73\n",
      "\tTime Taken: 0.8766696055730184 minutes\n",
      "Epoch [5/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 308.99410396814346 Test Loss: 62.929565250873566\n",
      "\tTraining Acc: 72.45 Test Acc: 73.25\n",
      "\tTime Taken: 0.8771889209747314 minutes\n",
      "Epoch [6/35]\t\n",
      "\tTraining Loss: 263.76606944203377 Test Loss: 64.74453246593475\n",
      "\tTraining Acc: 76.634 Test Acc: 72.82\n",
      "\tTime Taken: 0.8756924827893575 minutes\n",
      "Epoch [7/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 236.02055916190147 Test Loss: 53.25117173790932\n",
      "\tTraining Acc: 79.132 Test Acc: 77.19\n",
      "\tTime Taken: 0.8787582397460938 minutes\n",
      "Epoch [8/35]\t\n",
      "\tTraining Loss: 216.85909456014633 Test Loss: 87.486099421978\n",
      "\tTraining Acc: 80.922 Test Acc: 68.24\n",
      "\tTime Taken: 0.8765993158022563 minutes\n",
      "Epoch [9/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 206.51370710134506 Test Loss: 50.51835063099861\n",
      "\tTraining Acc: 81.962 Test Acc: 78.52\n",
      "\tTime Taken: 0.8782958984375 minutes\n",
      "Epoch [10/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 195.8168549835682 Test Loss: 48.258328169584274\n",
      "\tTraining Acc: 82.918 Test Acc: 79.56\n",
      "\tTime Taken: 0.8774768273035686 minutes\n",
      "Epoch [11/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 188.79318779706955 Test Loss: 45.40992343425751\n",
      "\tTraining Acc: 83.408 Test Acc: 80.64\n",
      "\tTime Taken: 0.8784586946169536 minutes\n",
      "Epoch [12/35]\t\n",
      "\tTraining Loss: 181.82174330949783 Test Loss: 61.73583281040192\n",
      "\tTraining Acc: 83.978 Test Acc: 74.98\n",
      "\tTime Taken: 0.8746561527252197 minutes\n",
      "Epoch [13/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 175.82456105947495 Test Loss: 44.322149872779846\n",
      "\tTraining Acc: 84.632 Test Acc: 81.58\n",
      "\tTime Taken: 0.877205506960551 minutes\n",
      "Epoch [14/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 170.39039722085 Test Loss: 40.46476083993912\n",
      "\tTraining Acc: 85.006 Test Acc: 82.96\n",
      "\tTime Taken: 0.8761740922927856 minutes\n",
      "Epoch [15/35]\t\n",
      "\tTraining Loss: 165.84859718382359 Test Loss: 42.752157777547836\n",
      "\tTraining Acc: 85.58 Test Acc: 81.62\n",
      "\tTime Taken: 0.8755926489830017 minutes\n",
      "Epoch [16/35]\t\n",
      "\tTraining Loss: 162.30063338577747 Test Loss: 47.03594774007797\n",
      "\tTraining Acc: 85.82 Test Acc: 79.96\n",
      "\tTime Taken: 0.8763957778612773 minutes\n",
      "Epoch [17/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 156.15620428323746 Test Loss: 35.4472703486681\n",
      "\tTraining Acc: 86.384 Test Acc: 84.28\n",
      "\tTime Taken: 0.8767371733983358 minutes\n",
      "Epoch [18/35]\t\n",
      "\tTraining Loss: 155.41058617830276 Test Loss: 63.065440982580185\n",
      "\tTraining Acc: 86.356 Test Acc: 75.35\n",
      "\tTime Taken: 0.8743197719256083 minutes\n",
      "Epoch [19/35]\t\n",
      "\tTraining Loss: 153.99002386629581 Test Loss: 38.15934154391289\n",
      "\tTraining Acc: 86.376 Test Acc: 84.0\n",
      "\tTime Taken: 0.8757343610127767 minutes\n",
      "Epoch [20/35]\t\n",
      "\tTraining Loss: 150.88720014691353 Test Loss: 42.51886188983917\n",
      "\tTraining Acc: 86.812 Test Acc: 82.05\n",
      "\tTime Taken: 0.8743733008702596 minutes\n",
      "Epoch [21/35]\t\n",
      "\tTraining Loss: 147.2418800741434 Test Loss: 40.59208530187607\n",
      "\tTraining Acc: 87.136 Test Acc: 83.07\n",
      "\tTime Taken: 0.8759659171104431 minutes\n",
      "Epoch [22/35]\t\n",
      "\tTraining Loss: 146.77899533510208 Test Loss: 39.80145055055618\n",
      "\tTraining Acc: 87.074 Test Acc: 83.37\n",
      "\tTime Taken: 0.8757541378339132 minutes\n",
      "Epoch [23/35]\t\n",
      "\tTraining Loss: 144.34990680217743 Test Loss: 41.50300344824791\n",
      "\tTraining Acc: 87.452 Test Acc: 82.39\n",
      "\tTime Taken: 0.8746704459190369 minutes\n",
      "Epoch [24/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 143.34062245488167 Test Loss: 37.13626638054848\n",
      "\tTraining Acc: 87.562 Test Acc: 84.65\n",
      "\tTime Taken: 0.8770127932230631 minutes\n",
      "Epoch [25/35]\t\n",
      "\tTraining Loss: 141.45315225422382 Test Loss: 42.33939105272293\n",
      "\tTraining Acc: 87.74 Test Acc: 82.25\n",
      "\tTime Taken: 0.8752405405044555 minutes\n",
      "Epoch [26/35]\t\n",
      "\tTraining Loss: 142.25365281105042 Test Loss: 46.80912193655968\n",
      "\tTraining Acc: 87.688 Test Acc: 80.76\n",
      "\tTime Taken: 0.8782143672307332 minutes\n",
      "Epoch [27/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 137.97325260937214 Test Loss: 30.548483699560165\n",
      "\tTraining Acc: 88.088 Test Acc: 87.01\n",
      "\tTime Taken: 0.8782677928606669 minutes\n",
      "Epoch [28/35]\t\n",
      "\tTraining Loss: 135.31372113525867 Test Loss: 35.80795595049858\n",
      "\tTraining Acc: 88.338 Test Acc: 85.13\n",
      "\tTime Taken: 0.8965507944424947 minutes\n",
      "Epoch [29/35]\t\n",
      "\tTraining Loss: 135.47091932594776 Test Loss: 38.414543241262436\n",
      "\tTraining Acc: 88.248 Test Acc: 83.77\n",
      "\tTime Taken: 0.8511019547780355 minutes\n",
      "Epoch [30/35]\t\n",
      "\tTraining Loss: 135.03965856134892 Test Loss: 38.71901188790798\n",
      "\tTraining Acc: 88.134 Test Acc: 84.28\n",
      "\tTime Taken: 0.8494224627812703 minutes\n",
      "Epoch [31/35]\t\n",
      "\tTraining Loss: 131.49925687909126 Test Loss: 34.23271228373051\n",
      "\tTraining Acc: 88.416 Test Acc: 85.37\n",
      "\tTime Taken: 0.8514452377955118 minutes\n",
      "Epoch [32/35]\t\n",
      "\tTraining Loss: 133.74421076476574 Test Loss: 44.11075320839882\n",
      "\tTraining Acc: 88.23 Test Acc: 82.26\n",
      "\tTime Taken: 0.8499280730883281 minutes\n",
      "Epoch [33/35]\t\n",
      "\tTraining Loss: 133.33810856938362 Test Loss: 33.57734780013561\n",
      "\tTraining Acc: 88.404 Test Acc: 85.29\n",
      "\tTime Taken: 0.8778785824775696 minutes\n",
      "Epoch [34/35]\t\n",
      "\tTraining Loss: 128.8494517058134 Test Loss: 38.54144102334976\n",
      "\tTraining Acc: 88.66 Test Acc: 83.86\n",
      "\tTime Taken: 0.8713369488716125 minutes\n",
      "Epoch [35/35]\t\n",
      "\tTraining Loss: 128.46243166923523 Test Loss: 41.795736223459244\n",
      "\tTraining Acc: 88.772 Test Acc: 83.34\n",
      "\tTime Taken: 0.8589028596878052 minutes\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "        stime = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "        test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "        print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "        time_taken = (time.time()-stime)/60\n",
    "        print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # loading the best model checkpoint from training before testing\n",
    "    out = os.path.join('./saved_models/', save_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "test_dataset = SkipLabelDataset(test_dataset, TARGET_CLASS)\n",
    "if DATASET == \"badnets\":\n",
    "    full_poisoned_test_dataset = BadNetsDataset(test_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_poison, poisoning_rate=1.0, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    full_poisoned_test_dataset = WaNetDataset(test_dataset, TARGET_CLASS, seed=1, transform=transform_poison, poisoning_rate=1.0, noise_rate=0.0, return_original_label=False)\n",
    "elif DATASET == \"sig\":\n",
    "    full_poisoned_test_dataset = SIGDataset(test_dataset, TARGET_CLASS, 20, 6, seed=1, transform=transform_poison, return_original_label=False, attack_test=True)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "\n",
    "\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 86.84\n",
      "Attack Success Rate (ASR): 100.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No attack\n",
    "\n",
    "    Clean Accuracy (C-Acc): 90.49\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "SIG\n",
    "\n",
    "    Clean Accuracy (C-Acc): 86.84\n",
    "    Attack Success Rate (ASR): 100.0\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "BadNets\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.88\n",
    "    Attack Success Rate (ASR): 99.66\n",
    "\n",
    "BadNets \\\n",
    "kNN \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.76\n",
    "    Attack Success Rate (ASR): 0.92\n",
    "\n",
    "BadNets \\\n",
    "Energy\n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.49\n",
    "    Attack Success Rate (ASR): 1.91\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "WaNet\n",
    "\n",
    "    Clean Accuracy (C-Acc): 80.93\n",
    "    Attack Success Rate (ASR): 79.58\n",
    "\n",
    "WaNet \\\n",
    "kNN\n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.18\n",
    "    Attack Success Rate (ASR): 4.54\n",
    "\n",
    "WaNet \\\n",
    "Energy\n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.52\n",
    "    Attack Success Rate (ASR): 1.73"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
