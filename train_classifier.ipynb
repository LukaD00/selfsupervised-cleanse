{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import BadNetsDataset, WaNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "CLEANSE = False\n",
    "CLEANSED_LABELS_NAME = \"\" \n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "CHECKPOINT = \"\"\n",
    "DATASET = \"wanet\"  # badnets, wanet\n",
    "\n",
    "if DATASET == \"badnets\":\n",
    "    TARGET_CLASS = 1\n",
    "elif DATASET == \"wanet\":\n",
    "    TARGET_CLASS = 0\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, strategy: str = \"remove\"):\n",
    "        self.poison_dataset = poison_dataset\n",
    "        self.poison_indices = list(range(len(self.poison_dataset)))\n",
    "        \n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            self.predicted_labels = pickle.load(f)\n",
    "\n",
    "        assert strategy in [\"relabel\", \"remove\"]\n",
    "        self.strategy = strategy\n",
    "\n",
    "        poison_labels = [poison_label for _, poison_label in self.poison_dataset]\n",
    "        if self.strategy == \"remove\":\n",
    "            self.indices = [index for index in self.poison_indices if poison_labels[index]==self.predicted_labels[index]]\n",
    "        elif self.strategy == \"relabel\":\n",
    "            self.indices = self.poison_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index > len(self):\n",
    "            return IndexError()\n",
    "        while index not in self.indices:\n",
    "            index += 1\n",
    "\n",
    "        item = self.poison_dataset[index][0]\n",
    "        label = self.predicted_labels[index]\n",
    "\n",
    "        return item, label\n",
    "        \n",
    "\n",
    "class SkipLabelDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, skip_class: int):\n",
    "        self.return_as_pil = type(original_dataset[0][0]) is Image.Image\n",
    "\n",
    "        targets = np.array(original_dataset.targets)\n",
    "        self.data = original_dataset.data[targets != skip_class]\n",
    "        self.targets = targets[targets != skip_class].tolist()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.return_as_pil:\n",
    "            data = Image.fromarray(data)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_train_clean = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=True, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    train_poison_dataset = BadNetsDataset(train_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    train_poison_dataset = WaNetDataset(train_dataset, TARGET_CLASS, seed=1, transform=transform_train, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "if CLEANSE:\n",
    "    train_poison_dataset = CleansedDataset(train_poison_dataset, CLEANSED_LABELS_NAME + \"-train\", strategy=\"remove\")\n",
    "trainloader = DataLoader(train_poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    test_poison_dataset = BadNetsDataset(test_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_test, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    test_poison_dataset = WaNetDataset(test_dataset, TARGET_CLASS, seed=1, transform=transform_test, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "if CLEANSE:\n",
    "    test_poison_dataset = CleansedDataset(test_poison_dataset, CLEANSED_LABELS_NAME + \"-test\", strategy=\"remove\")\n",
    "testloader = DataLoader(test_poison_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "epochs = 35\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/', CHECKPOINT)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")\n",
    "    print(f\"{start_epoch = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "save_name = \"NEW-Resnet-18.pt\"\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, save_name)\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 867.4217586517334 Test Loss: 143.48915302753448\n",
      "\tTraining Acc: 23.134 Test Acc: 30.99\n",
      "\tTime Taken: 1.021517018477122 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 694.1532179117203 Test Loss: 132.72671401500702\n",
      "\tTraining Acc: 32.69 Test Acc: 37.12\n",
      "\tTime Taken: 1.0150614738464356 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 639.9889330863953 Test Loss: 122.50028693675995\n",
      "\tTraining Acc: 38.766 Test Acc: 42.55\n",
      "\tTime Taken: 1.0302222530047098 minutes\n",
      "Epoch [4/35]\t\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "        stime = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "        test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "        print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "        time_taken = (time.time()-stime)/60\n",
    "        print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the best model checkpoint before running\n",
    "out = os.path.join('./saved_models/', save_name)\n",
    "checkpoint = torch.load(out, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "test_dataset = SkipLabelDataset(test_dataset, TARGET_CLASS)\n",
    "if DATASET == \"badnets\":\n",
    "    full_poisoned_test_dataset = BadNetsDataset(test_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_poison, poisoning_rate=1.0, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    full_poisoned_test_dataset = WaNetDataset(test_dataset, TARGET_CLASS, seed=1, transform=transform_poison, poisoning_rate=1.0, noise_rate=0.0, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "\n",
    "\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 78.31\n",
      "Attack Success Rate (ASR): 1.711111111111111\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 90.49\n",
    "    Attack Success Rate (ASR): 0.33 \n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.88\n",
    "    Attack Success Rate (ASR): 99.66\n",
    "\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "kNN \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.76\n",
    "    Attack Success Rate (ASR): 0.92\n",
    "\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "Energy \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.49\n",
    "    Attack Success Rate (ASR): 1.91\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "ResNet-18 \\\n",
    "\n",
    "    Clean Accuracy (C-Acc): 83.32\n",
    "    Attack Success Rate (ASR): 83.46 #td\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "kNN \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 78.31\n",
    "    Attack Success Rate (ASR): 1.71\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "Energy \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.52\n",
    "    Attack Success Rate (ASR): 1.73"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
