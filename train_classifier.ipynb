{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import prepare_poison_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"wanet1\"  \n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "CHECKPOINT = \"\"\n",
    "\n",
    "TRAIN = True\n",
    "CLEANSE = False\n",
    "CLEANSED_LABELS_NAME = f\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, transforms: torch.nn.Module = None):\n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            predicted_poison = pickle.load(f)\n",
    "\n",
    "        self.data = [poison_dataset[i][0] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.labels = [poison_dataset[i][1] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transforms:\n",
    "            item = self.transforms(item)\n",
    "\n",
    "        return item, label\n",
    "\n",
    "\n",
    "class SkipLabelDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, skip_class: int):\n",
    "        self.return_as_pil = type(original_dataset[0][0]) is Image.Image\n",
    "\n",
    "        targets = np.array(original_dataset.targets)\n",
    "        self.data = original_dataset.data[targets != skip_class]\n",
    "        self.targets = targets[targets != skip_class].tolist()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.return_as_pil:\n",
    "            data = Image.fromarray(data)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ProbTransform(torch.nn.Module):\n",
    "    def __init__(self, f, p=1):\n",
    "        super(ProbTransform, self).__init__()\n",
    "        self.f = f\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if random.random() < self.p:\n",
    "            return self.f(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    ProbTransform(transforms.RandomCrop(32, padding=5), p=0.8),\n",
    "    ProbTransform(transforms.transforms.RandomRotation(10), p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_poison_dataset, _, target_class, train_dataset = prepare_poison_dataset(DATASET, train=True, transform=transform_train, return_original_label=False)\n",
    "if CLEANSE:\n",
    "    train_poison_dataset = CleansedDataset(train_poison_dataset, CLEANSED_LABELS_NAME + \"-train\")\n",
    "trainloader = DataLoader(train_poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "test_poison_dataset, _, _, test_dataset = prepare_poison_dataset(DATASET, train=False, transform=transform_test, return_original_label=False)\n",
    "if CLEANSE:\n",
    "    test_poison_dataset = CleansedDataset(test_poison_dataset, CLEANSED_LABELS_NAME + \"-test\")\n",
    "testloader = DataLoader(test_poison_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a PreAct-Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActBlock(nn.Module):\n",
    "    \"\"\"Pre-activation version of the BasicBlock.\"\"\"\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.ind = None\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        if self.ind is not None:\n",
    "            out += shortcut[:, self.ind, :, :]\n",
    "        else:\n",
    "            out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"Pre-activation version of the original Bottleneck module.\"\"\"\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18(num_classes=10):\n",
    "    return PreActResNet(PreActBlock, [2, 2, 2, 2], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/preact-resnet18', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PreActResNet18()\n",
    "model.to(device)\n",
    "\n",
    "epochs = 35\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [100,200,300,400], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/preact-resnet18/', CHECKPOINT)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")\n",
    "    print(f\"{start_epoch = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "save_name = \"NEW-Resnet-18.pt\"\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, save_name)\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 636.0883057117462 Test Loss: 124.57472097873688\n",
      "\tTraining Acc: 40.484 Test Acc: 46.51\n",
      "\tTime Taken: 1.309550114472707 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 486.2538110613823 Test Loss: 103.51094579696655\n",
      "\tTraining Acc: 55.742 Test Acc: 54.49\n",
      "\tTime Taken: 1.3054253935813904 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 415.21736991405487 Test Loss: 96.17593902349472\n",
      "\tTraining Acc: 62.25 Test Acc: 59.3\n",
      "\tTime Taken: 1.3060154755910238 minutes\n",
      "Epoch [4/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 361.32583940029144 Test Loss: 71.97968864440918\n",
      "\tTraining Acc: 67.376 Test Acc: 67.11\n",
      "\tTime Taken: 1.298178482055664 minutes\n",
      "Epoch [5/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 321.75435185432434 Test Loss: 69.88287270069122\n",
      "\tTraining Acc: 70.88 Test Acc: 70.07\n",
      "\tTime Taken: 1.2060081243515015 minutes\n",
      "Epoch [6/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 291.0328929424286 Test Loss: 58.711282163858414\n",
      "\tTraining Acc: 73.682 Test Acc: 74.43\n",
      "\tTime Taken: 1.2038743217786154 minutes\n",
      "Epoch [7/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 268.04400485754013 Test Loss: 54.591162234544754\n",
      "\tTraining Acc: 75.662 Test Acc: 76.2\n",
      "\tTime Taken: 1.2023842771848043 minutes\n",
      "Epoch [8/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 253.97943380475044 Test Loss: 49.80117020010948\n",
      "\tTraining Acc: 76.928 Test Acc: 77.81\n",
      "\tTime Taken: 1.2077840288480124 minutes\n",
      "Epoch [9/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 237.826052993536 Test Loss: 49.809024810791016\n",
      "\tTraining Acc: 78.532 Test Acc: 78.1\n",
      "\tTime Taken: 1.2049023350079855 minutes\n",
      "Epoch [10/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 223.52744686603546 Test Loss: 47.016544342041016\n",
      "\tTraining Acc: 79.68 Test Acc: 79.86\n",
      "\tTime Taken: 1.2018255670865376 minutes\n",
      "Epoch [11/35]\t\n",
      "\tTraining Loss: 211.03724816441536 Test Loss: 57.04575818777084\n",
      "\tTraining Acc: 80.934 Test Acc: 75.29\n",
      "\tTime Taken: 1.202048619588216 minutes\n",
      "Epoch [12/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 199.1515173614025 Test Loss: 45.87224358320236\n",
      "\tTraining Acc: 81.98 Test Acc: 80.68\n",
      "\tTime Taken: 1.2043723940849305 minutes\n",
      "Epoch [13/35]\t\n",
      "\tTraining Loss: 187.3164400756359 Test Loss: 51.34962013363838\n",
      "\tTraining Acc: 83.138 Test Acc: 79.08\n",
      "\tTime Taken: 1.200020933151245 minutes\n",
      "Epoch [14/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 178.75192561745644 Test Loss: 41.4673517793417\n",
      "\tTraining Acc: 84.042 Test Acc: 81.91\n",
      "\tTime Taken: 1.2024215658505757 minutes\n",
      "Epoch [15/35]\t\n",
      "\tTraining Loss: 169.65849949419498 Test Loss: 45.297589272260666\n",
      "\tTraining Acc: 84.896 Test Acc: 80.25\n",
      "\tTime Taken: 1.1968029459317526 minutes\n",
      "Epoch [16/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 164.3662108182907 Test Loss: 42.61717036366463\n",
      "\tTraining Acc: 85.168 Test Acc: 82.34\n",
      "\tTime Taken: 1.1997754375139873 minutes\n",
      "Epoch [17/35]\t\n",
      "\tTraining Loss: 155.49833272397518 Test Loss: 45.285189390182495\n",
      "\tTraining Acc: 86.062 Test Acc: 80.86\n",
      "\tTime Taken: 1.1989939173062643 minutes\n",
      "Epoch [18/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 148.30744263529778 Test Loss: 43.360573679208755\n",
      "\tTraining Acc: 86.592 Test Acc: 82.93\n",
      "\tTime Taken: 1.2019270459810893 minutes\n",
      "Epoch [19/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 140.84950767457485 Test Loss: 37.471232026815414\n",
      "\tTraining Acc: 87.488 Test Acc: 84.59\n",
      "\tTime Taken: 1.204107900460561 minutes\n",
      "Epoch [20/35]\t\n",
      "\tTraining Loss: 136.47770896553993 Test Loss: 40.50644314289093\n",
      "\tTraining Acc: 87.82 Test Acc: 84.29\n",
      "\tTime Taken: 1.2033165216445922 minutes\n",
      "Epoch [21/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 126.63084734976292 Test Loss: 32.8742209225893\n",
      "\tTraining Acc: 88.468 Test Acc: 85.98\n",
      "\tTime Taken: 1.2005801637967428 minutes\n",
      "Epoch [22/35]\t\n",
      "\tTraining Loss: 126.03816282749176 Test Loss: 35.481823831796646\n",
      "\tTraining Acc: 88.462 Test Acc: 85.04\n",
      "\tTime Taken: 1.1964245001475016 minutes\n",
      "Epoch [23/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 119.37392820417881 Test Loss: 33.76281712949276\n",
      "\tTraining Acc: 89.218 Test Acc: 86.46\n",
      "\tTime Taken: 1.2021218617757161 minutes\n",
      "Epoch [24/35]\t\n",
      "\tTraining Loss: 119.21394410729408 Test Loss: 37.03118938207626\n",
      "\tTraining Acc: 89.226 Test Acc: 85.55\n",
      "\tTime Taken: 1.1985482772191365 minutes\n",
      "Epoch [25/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 114.25117833912373 Test Loss: 30.529595524072647\n",
      "\tTraining Acc: 89.608 Test Acc: 87.76\n",
      "\tTime Taken: 1.202758002281189 minutes\n",
      "Epoch [26/35]\t\n",
      "\tTraining Loss: 110.2733150050044 Test Loss: 32.702004224061966\n",
      "\tTraining Acc: 90.122 Test Acc: 86.97\n",
      "\tTime Taken: 1.2023659149805705 minutes\n",
      "Epoch [27/35]\t\n",
      "\tTraining Loss: 104.64001380652189 Test Loss: 33.14429958164692\n",
      "\tTraining Acc: 90.522 Test Acc: 86.45\n",
      "\tTime Taken: 1.199847730000814 minutes\n",
      "Epoch [28/35]\t\n",
      "\tTraining Loss: 103.6228620633483 Test Loss: 31.614463485777378\n",
      "\tTraining Acc: 90.704 Test Acc: 87.35\n",
      "\tTime Taken: 1.1992340167363484 minutes\n",
      "Epoch [29/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 99.40159969776869 Test Loss: 29.474445298314095\n",
      "\tTraining Acc: 90.958 Test Acc: 88.48\n",
      "\tTime Taken: 1.19999041557312 minutes\n",
      "Epoch [30/35]\t\n",
      "\tTraining Loss: 95.44192779064178 Test Loss: 32.33837601542473\n",
      "\tTraining Acc: 91.438 Test Acc: 87.49\n",
      "\tTime Taken: 1.202359708150228 minutes\n",
      "Epoch [31/35]\t\n",
      "\tTraining Loss: 93.11694046109915 Test Loss: 32.04366132616997\n",
      "\tTraining Acc: 91.604 Test Acc: 87.76\n",
      "\tTime Taken: 1.2031471689542135 minutes\n",
      "Epoch [32/35]\t\n",
      "\tTraining Loss: 92.33888284116983 Test Loss: 30.385162234306335\n",
      "\tTraining Acc: 91.596 Test Acc: 88.04\n",
      "\tTime Taken: 1.201047452290853 minutes\n",
      "Epoch [33/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 89.51158559322357 Test Loss: 30.823642574250698\n",
      "\tTraining Acc: 91.874 Test Acc: 88.65\n",
      "\tTime Taken: 1.1998071352640787 minutes\n",
      "Epoch [34/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW-Resnet-18.pt\n",
      "\tTraining Loss: 86.48263987898827 Test Loss: 28.78394041955471\n",
      "\tTraining Acc: 92.174 Test Acc: 89.0\n",
      "\tTime Taken: 1.2047852396965026 minutes\n",
      "Epoch [35/35]\t\n",
      "\tTraining Loss: 85.16493648290634 Test Loss: 28.13176853209734\n",
      "\tTraining Acc: 92.338 Test Acc: 88.9\n",
      "\tTime Taken: 1.2034081856409709 minutes\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "        stime = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "        test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "        print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "        time_taken = (time.time()-stime)/60\n",
    "        print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # loading the best model checkpoint from training before testing\n",
    "    out = os.path.join('./saved_models/preact-resnet18/', save_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "test_dataset = SkipLabelDataset(test_dataset, target_class)\n",
    "full_poisoned_test_dataset, _, _, _ = prepare_poison_dataset(DATASET, train=False, transform=transform_poison, return_original_label=False, clean_dataset=test_dataset)\n",
    "# if DATASET == \"badnets\":\n",
    "#     full_poisoned_test_dataset = BadNetsDataset(test_dataset, target_class, \"triggers/trigger_10.png\", seed=1, transform=transform_poison, poisoning_rate=1.0, return_original_label=False)\n",
    "# elif DATASET == \"wanet\":\n",
    "#     full_poisoned_test_dataset = WaNetDataset(test_dataset, target_class, seed=1, transform=transform_poison, poisoning_rate=1.0, noise_rate=0.0, return_original_label=False)\n",
    "# elif DATASET == \"sig\":\n",
    "#     full_poisoned_test_dataset = SIGDataset(test_dataset, target_class, 20, 6, seed=1, transform=transform_poison, return_original_label=False, attack_test=True)\n",
    "# else:\n",
    "#     raise Exception(\"Invalid dataset\")\n",
    "\n",
    "\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 89.14\n",
      "Attack Success Rate (ASR): 87.93333333333334\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    No attack\n",
    "    Clean Accuracy (C-Acc): 91.07\n",
    "    Attack Success Rate (ASR): 91.07\n",
    "\n",
    "    WaNet0\n",
    "    Clean Accuracy (C-Acc): 87.25\n",
    "    Attack Success Rate (ASR): 87.26666666666667\n",
    "\n",
    "    WaNet1\n",
    "    Clean Accuracy (C-Acc): 89.14\n",
    "    Attack Success Rate (ASR): 87.93333333333334"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
