{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import prepare_poison_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"sig-0\"  \n",
    "save_name = f\"{DATASET}-NEW.pt\"\n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "CHECKPOINT = \"\"\n",
    "\n",
    "TRAIN = True\n",
    "CLEANSE = False\n",
    "CLEANSED_LABELS_NAME = f\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, transforms: torch.nn.Module = None):\n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            predicted_poison = pickle.load(f)\n",
    "\n",
    "        self.data = [poison_dataset[i][0] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.labels = [poison_dataset[i][1] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transforms:\n",
    "            item = self.transforms(item)\n",
    "\n",
    "        return item, label\n",
    "\n",
    "\n",
    "class SkipLabelDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, skip_class: int):\n",
    "        self.return_as_pil = type(original_dataset[0][0]) is Image.Image\n",
    "\n",
    "        targets = np.array(original_dataset.targets)\n",
    "        self.data = original_dataset.data[targets != skip_class]\n",
    "        self.targets = targets[targets != skip_class].tolist()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.return_as_pil:\n",
    "            data = Image.fromarray(data)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ProbTransform(torch.nn.Module):\n",
    "    def __init__(self, f, p=1):\n",
    "        super(ProbTransform, self).__init__()\n",
    "        self.f = f\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if random.random() < self.p:\n",
    "            return self.f(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    ProbTransform(transforms.RandomCrop(32, padding=5), p=0.8),\n",
    "    ProbTransform(transforms.transforms.RandomRotation(10), p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_poison_dataset, _, target_class, train_dataset = prepare_poison_dataset(DATASET, train=True, transform=transform_train, return_original_label=False)\n",
    "if CLEANSE:\n",
    "    train_poison_dataset = CleansedDataset(train_poison_dataset, CLEANSED_LABELS_NAME + \"-train\")\n",
    "trainloader = DataLoader(train_poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "test_poison_dataset, _, _, test_dataset = prepare_poison_dataset(DATASET, train=False, transform=transform_test, return_original_label=False)\n",
    "if CLEANSE:\n",
    "    test_poison_dataset = CleansedDataset(test_poison_dataset, CLEANSED_LABELS_NAME + \"-test\")\n",
    "testloader = DataLoader(test_poison_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a PreAct-Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActBlock(nn.Module):\n",
    "    \"\"\"Pre-activation version of the BasicBlock.\"\"\"\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.ind = None\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        if self.ind is not None:\n",
    "            out += shortcut[:, self.ind, :, :]\n",
    "        else:\n",
    "            out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"Pre-activation version of the original Bottleneck module.\"\"\"\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18(num_classes=10):\n",
    "    return PreActResNet(PreActBlock, [2, 2, 2, 2], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/preact-resnet18', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PreActResNet18()\n",
    "model.to(device)\n",
    "\n",
    "epochs = 35\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [100,200,300,400], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/preact-resnet18/', CHECKPOINT)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")\n",
    "    print(f\"{start_epoch = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "save_name = f\"{DATASET}-NEW.pt\"\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, save_name)\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 570.8289043903351 Test Loss: 94.98973053693771\n",
      "\tTraining Acc: 46.196 Test Acc: 57.51\n",
      "\tTime Taken: 1.1589024504025778 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 385.96001183986664 Test Loss: 66.0149696469307\n",
      "\tTraining Acc: 64.844 Test Acc: 70.97\n",
      "\tTime Taken: 1.630881957213084 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 297.4491183459759 Test Loss: 60.33340725302696\n",
      "\tTraining Acc: 73.392 Test Acc: 73.17\n",
      "\tTime Taken: 1.68278458515803 minutes\n",
      "Epoch [4/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 254.56908702850342 Test Loss: 48.94113662838936\n",
      "\tTraining Acc: 77.578 Test Acc: 78.97\n",
      "\tTime Taken: 1.6798864205678303 minutes\n",
      "Epoch [5/35]\t\n",
      "\tTraining Loss: 222.0759895145893 Test Loss: 60.91204231977463\n",
      "\tTraining Acc: 80.222 Test Acc: 74.66\n",
      "\tTime Taken: 1.614560345808665 minutes\n",
      "Epoch [6/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 201.7030549943447 Test Loss: 44.506353467702866\n",
      "\tTraining Acc: 82.054 Test Acc: 81.46\n",
      "\tTime Taken: 1.6367361863454184 minutes\n",
      "Epoch [7/35]\t\n",
      "\tTraining Loss: 182.05849324166775 Test Loss: 45.14572098851204\n",
      "\tTraining Acc: 83.898 Test Acc: 81.07\n",
      "\tTime Taken: 1.470130205154419 minutes\n",
      "Epoch [8/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 166.2764966338873 Test Loss: 44.208730310201645\n",
      "\tTraining Acc: 85.224 Test Acc: 81.83\n",
      "\tTime Taken: 0.9860348105430603 minutes\n",
      "Epoch [9/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 154.6768361479044 Test Loss: 34.317561998963356\n",
      "\tTraining Acc: 86.506 Test Acc: 85.57\n",
      "\tTime Taken: 0.9877583742141723 minutes\n",
      "Epoch [10/35]\t\n",
      "\tTraining Loss: 147.29657672345638 Test Loss: 40.53665056824684\n",
      "\tTraining Acc: 86.912 Test Acc: 83.76\n",
      "\tTime Taken: 0.9886319756507873 minutes\n",
      "Epoch [11/35]\t\n",
      "\tTraining Loss: 137.57942600548267 Test Loss: 35.04411019384861\n",
      "\tTraining Acc: 87.832 Test Acc: 85.36\n",
      "\tTime Taken: 0.9901426911354065 minutes\n",
      "Epoch [12/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 130.26138503849506 Test Loss: 31.97646913677454\n",
      "\tTraining Acc: 88.418 Test Acc: 86.91\n",
      "\tTime Taken: 0.9915416598320007 minutes\n",
      "Epoch [13/35]\t\n",
      "\tTraining Loss: 123.4769446849823 Test Loss: 35.03245013952255\n",
      "\tTraining Acc: 89.05 Test Acc: 85.45\n",
      "\tTime Taken: 0.9889538963635762 minutes\n",
      "Epoch [14/35]\t\n",
      "\tTraining Loss: 115.66437900066376 Test Loss: 34.870473980903625\n",
      "\tTraining Acc: 89.668 Test Acc: 86.14\n",
      "\tTime Taken: 0.9864327987035115 minutes\n",
      "Epoch [15/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 112.16518448293209 Test Loss: 32.558259673416615\n",
      "\tTraining Acc: 90.07 Test Acc: 87.19\n",
      "\tTime Taken: 0.9860584100087484 minutes\n",
      "Epoch [16/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 105.12285543978214 Test Loss: 30.626669511198997\n",
      "\tTraining Acc: 90.62 Test Acc: 87.27\n",
      "\tTime Taken: 0.9897132635116577 minutes\n",
      "Epoch [17/35]\t\n",
      "\tTraining Loss: 103.0305807441473 Test Loss: 34.18547512590885\n",
      "\tTraining Acc: 90.696 Test Acc: 86.16\n",
      "\tTime Taken: 0.988878341515859 minutes\n",
      "Epoch [18/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 98.1013630926609 Test Loss: 27.433608703315258\n",
      "\tTraining Acc: 91.114 Test Acc: 88.82\n",
      "\tTime Taken: 0.9895931164423625 minutes\n",
      "Epoch [19/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 93.8843152448535 Test Loss: 26.60037586838007\n",
      "\tTraining Acc: 91.626 Test Acc: 88.96\n",
      "\tTime Taken: 0.9879958510398865 minutes\n",
      "Epoch [20/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 90.56835174560547 Test Loss: 26.110718473792076\n",
      "\tTraining Acc: 91.936 Test Acc: 89.16\n",
      "\tTime Taken: 0.9904916723569234 minutes\n",
      "Epoch [21/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 86.74802701175213 Test Loss: 23.63245166838169\n",
      "\tTraining Acc: 92.26 Test Acc: 90.08\n",
      "\tTime Taken: 1.0328418533007304 minutes\n",
      "Epoch [22/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\NEW.pt\n",
      "\tTraining Loss: 83.54625121504068 Test Loss: 24.263935662806034\n",
      "\tTraining Acc: 92.652 Test Acc: 90.13\n",
      "\tTime Taken: 1.1903173526128132 minutes\n",
      "Epoch [23/35]\t\n",
      "\tTraining Loss: 81.58011627942324 Test Loss: 25.497499488294125\n",
      "\tTraining Acc: 92.662 Test Acc: 89.94\n",
      "\tTime Taken: 1.4728958090146382 minutes\n",
      "Epoch [24/35]\t\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "        stime = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "        test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "        print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "        time_taken = (time.time()-stime)/60\n",
    "        print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # loading the best model checkpoint from training before testing\n",
    "    out = os.path.join('./saved_models/preact-resnet18/', save_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "test_dataset = SkipLabelDataset(test_dataset, target_class)\n",
    "full_poisoned_test_dataset, _, _, _ = prepare_poison_dataset(DATASET, train=False, transform=transform_poison, return_original_label=False, clean_dataset=test_dataset)\n",
    "\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 88.33\n",
      "Attack Success Rate (ASR): 88.88888888888889\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    No attack\n",
    "    Clean Accuracy (C-Acc): 91.07\n",
    "    Attack Success Rate (ASR): 91.07\n",
    "\n",
    "    WaNet0\n",
    "    Clean Accuracy (C-Acc): 87.25\n",
    "    Attack Success Rate (ASR): 87.26666666666667\n",
    "\n",
    "    WaNet1\n",
    "    Clean Accuracy (C-Acc): 89.14\n",
    "    Attack Success Rate (ASR): 87.93333333333334\n",
    "\n",
    "    WaNet2\n",
    "    Clean Accuracy (C-Acc): 88.33\n",
    "    Attack Success Rate (ASR): 88.88888888888889"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
