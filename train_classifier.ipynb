{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import BadNetsDataset, WaNetDataset, SIGDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "CLEANSE = False\n",
    "CLEANSED_LABELS_NAME = \"WaNet-kNN\" \n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "CHECKPOINT = \"\"\n",
    "DATASET = \"sig\"  \n",
    "\n",
    "if DATASET == \"badnets\":\n",
    "    TARGET_CLASS = 1\n",
    "elif DATASET == \"wanet\":\n",
    "    TARGET_CLASS = 0\n",
    "elif DATASET == \"sig\":\n",
    "    TARGET_CLASS = 0\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, strategy: str = \"remove\"):\n",
    "        self.poison_dataset = poison_dataset\n",
    "        self.poison_indices = list(range(len(self.poison_dataset)))\n",
    "        \n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            self.predicted_labels = pickle.load(f)\n",
    "\n",
    "        assert strategy in [\"relabel\", \"remove\"]\n",
    "        self.strategy = strategy\n",
    "\n",
    "        poison_labels = [poison_label for _, poison_label in self.poison_dataset]\n",
    "        if self.strategy == \"remove\":\n",
    "            self.indices = [index for index in self.poison_indices if poison_labels[index]==self.predicted_labels[index]]\n",
    "        elif self.strategy == \"relabel\":\n",
    "            self.indices = self.poison_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index > len(self):\n",
    "            return IndexError()\n",
    "        while index not in self.indices:\n",
    "            index += 1\n",
    "\n",
    "        item = self.poison_dataset[index][0]\n",
    "        label = self.predicted_labels[index]\n",
    "\n",
    "        return item, label\n",
    "        \n",
    "\n",
    "class SkipLabelDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, skip_class: int):\n",
    "        self.return_as_pil = type(original_dataset[0][0]) is Image.Image\n",
    "\n",
    "        targets = np.array(original_dataset.targets)\n",
    "        self.data = original_dataset.data[targets != skip_class]\n",
    "        self.targets = targets[targets != skip_class].tolist()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.return_as_pil:\n",
    "            data = Image.fromarray(data)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_train_clean = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=True, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    train_poison_dataset = BadNetsDataset(train_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    train_poison_dataset = WaNetDataset(train_dataset, TARGET_CLASS, seed=1, transform=transform_train, return_original_label=False)\n",
    "elif DATASET == \"sig\":\n",
    "    train_poison_dataset = SIGDataset(train_dataset, TARGET_CLASS, 20, 6, seed=1, transform=transform_train, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "if CLEANSE:\n",
    "    train_poison_dataset = CleansedDataset(train_poison_dataset, CLEANSED_LABELS_NAME + \"-train\", strategy=\"remove\")\n",
    "trainloader = DataLoader(train_poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    test_poison_dataset = BadNetsDataset(test_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_test, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    test_poison_dataset = WaNetDataset(test_dataset, TARGET_CLASS, seed=1, transform=transform_test, return_original_label=False)\n",
    "elif DATASET == \"sig\":\n",
    "    test_poison_dataset = SIGDataset(test_dataset, TARGET_CLASS, 20, 6, seed=1, transform=transform_test, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "if CLEANSE:\n",
    "    test_poison_dataset = CleansedDataset(test_poison_dataset, CLEANSED_LABELS_NAME + \"-test\", strategy=\"remove\")\n",
    "testloader = DataLoader(test_poison_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "epochs = 35\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/', CHECKPOINT)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")\n",
    "    print(f\"{start_epoch = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "save_name = \"NEW-Resnet-18.pt\"\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, save_name)\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 771.0979427099228 Test Loss: 127.27564215660095\n",
      "\tTraining Acc: 29.702 Test Acc: 40.88\n",
      "\tTime Taken: 0.8966806968053181 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 580.4249113798141 Test Loss: 105.19109261035919\n",
      "\tTraining Acc: 45.236 Test Acc: 50.61\n",
      "\tTime Taken: 0.8622506737709046 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 479.68746691942215 Test Loss: 85.8911521434784\n",
      "\tTraining Acc: 55.672 Test Acc: 60.71\n",
      "\tTime Taken: 0.8623854200045268 minutes\n",
      "Epoch [4/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 401.0405304431915 Test Loss: 78.34409028291702\n",
      "\tTraining Acc: 63.43 Test Acc: 64.34\n",
      "\tTime Taken: 0.863690972328186 minutes\n",
      "Epoch [5/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 350.4799196124077 Test Loss: 77.10587954521179\n",
      "\tTraining Acc: 68.066 Test Acc: 66.28\n",
      "\tTime Taken: 0.87409588098526 minutes\n",
      "Epoch [6/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 306.3301497101784 Test Loss: 68.12933224439621\n",
      "\tTraining Acc: 72.476 Test Acc: 70.52\n",
      "\tTime Taken: 0.8616222977638245 minutes\n",
      "Epoch [7/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 268.5810075700283 Test Loss: 53.701324820518494\n",
      "\tTraining Acc: 76.118 Test Acc: 76.72\n",
      "\tTime Taken: 0.862429694334666 minutes\n",
      "Epoch [8/35]\t\n",
      "\tTraining Loss: 244.46863609552383 Test Loss: 53.09805291891098\n",
      "\tTraining Acc: 78.19 Test Acc: 76.71\n",
      "\tTime Taken: 0.8598500291506449 minutes\n",
      "Epoch [9/35]\t\n",
      "\tTraining Loss: 224.80628871917725 Test Loss: 57.03426215052605\n",
      "\tTraining Acc: 80.164 Test Acc: 75.69\n",
      "\tTime Taken: 0.8625087976455689 minutes\n",
      "Epoch [10/35]\t\n",
      "\tTraining Loss: 214.30639526247978 Test Loss: 62.980678617954254\n",
      "\tTraining Acc: 81.266 Test Acc: 73.33\n",
      "\tTime Taken: 0.8631526549657186 minutes\n",
      "Epoch [11/35]\t\n",
      "\tTraining Loss: 202.2332618534565 Test Loss: 62.82907170057297\n",
      "\tTraining Acc: 82.084 Test Acc: 73.05\n",
      "\tTime Taken: 0.8583896199862162 minutes\n",
      "Epoch [12/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 196.90505227446556 Test Loss: 46.160673916339874\n",
      "\tTraining Acc: 82.758 Test Acc: 80.08\n",
      "\tTime Taken: 0.8507145126660665 minutes\n",
      "Epoch [13/35]\t\n",
      "\tTraining Loss: 187.40385538339615 Test Loss: 47.161173433065414\n",
      "\tTraining Acc: 83.504 Test Acc: 79.49\n",
      "\tTime Taken: 0.8921303153038025 minutes\n",
      "Epoch [14/35]\t\n",
      "\tTraining Loss: 182.39912851154804 Test Loss: 47.68377694487572\n",
      "\tTraining Acc: 83.966 Test Acc: 79.38\n",
      "\tTime Taken: 0.8640946944554647 minutes\n",
      "Epoch [15/35]\t\n",
      "\tTraining Loss: 174.7066489160061 Test Loss: 55.17185565829277\n",
      "\tTraining Acc: 84.604 Test Acc: 77.7\n",
      "\tTime Taken: 0.8606108903884888 minutes\n",
      "Epoch [16/35]\t\n",
      "\tTraining Loss: 171.19309717416763 Test Loss: 49.39017716050148\n",
      "\tTraining Acc: 84.978 Test Acc: 80.03\n",
      "\tTime Taken: 0.8931192080179851 minutes\n",
      "Epoch [17/35]\t\n",
      "\tTraining Loss: 169.0248397141695 Test Loss: 47.306735545396805\n",
      "\tTraining Acc: 85.22 Test Acc: 79.84\n",
      "\tTime Taken: 0.9226835489273071 minutes\n",
      "Epoch [18/35]\t\n",
      "\tTraining Loss: 163.54991753399372 Test Loss: 48.31177303195\n",
      "\tTraining Acc: 85.672 Test Acc: 79.32\n",
      "\tTime Taken: 0.9415335655212402 minutes\n",
      "Epoch [19/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 160.40913306176662 Test Loss: 46.82030168175697\n",
      "\tTraining Acc: 85.848 Test Acc: 80.87\n",
      "\tTime Taken: 0.8707832932472229 minutes\n",
      "Epoch [20/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 157.1114862859249 Test Loss: 41.304845333099365\n",
      "\tTraining Acc: 86.212 Test Acc: 83.32\n",
      "\tTime Taken: 0.8582701603571574 minutes\n",
      "Epoch [21/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 155.8103981912136 Test Loss: 37.637254416942596\n",
      "\tTraining Acc: 86.29 Test Acc: 84.33\n",
      "\tTime Taken: 0.8405326882998149 minutes\n",
      "Epoch [22/35]\t\n",
      "\tTraining Loss: 149.79391123354435 Test Loss: 36.884787529706955\n",
      "\tTraining Acc: 86.87 Test Acc: 84.11\n",
      "\tTime Taken: 0.8388186573982239 minutes\n",
      "Epoch [23/35]\t\n",
      "\tTraining Loss: 149.30396883189678 Test Loss: 39.317431569099426\n",
      "\tTraining Acc: 86.872 Test Acc: 83.68\n",
      "\tTime Taken: 0.8392687360445659 minutes\n",
      "Epoch [24/35]\t\n",
      "\tTraining Loss: 149.03932078182697 Test Loss: 49.271561831235886\n",
      "\tTraining Acc: 87.048 Test Acc: 79.95\n",
      "\tTime Taken: 0.8402456601460775 minutes\n",
      "Epoch [25/35]\t\n",
      "\tTraining Loss: 145.33836820721626 Test Loss: 47.970327615737915\n",
      "\tTraining Acc: 87.186 Test Acc: 81.26\n",
      "\tTime Taken: 0.8429338137308756 minutes\n",
      "Epoch [26/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 143.9832562059164 Test Loss: 36.38906529545784\n",
      "\tTraining Acc: 87.392 Test Acc: 84.76\n",
      "\tTime Taken: 0.9327645977338155 minutes\n",
      "Epoch [27/35]\t\n",
      "\tTraining Loss: 142.66539853811264 Test Loss: 36.552994817495346\n",
      "\tTraining Acc: 87.414 Test Acc: 84.38\n",
      "\tTime Taken: 0.9009063800175985 minutes\n",
      "Epoch [28/35]\t\n",
      "\tTraining Loss: 142.0451328754425 Test Loss: 37.803032636642456\n",
      "\tTraining Acc: 87.616 Test Acc: 83.8\n",
      "\tTime Taken: 0.8893702228864034 minutes\n",
      "Epoch [29/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 141.62905095517635 Test Loss: 34.74149677157402\n",
      "\tTraining Acc: 87.66 Test Acc: 85.48\n",
      "\tTime Taken: 0.8911396185557048 minutes\n",
      "Epoch [30/35]\t\n",
      "\tTraining Loss: 138.0339252203703 Test Loss: 40.90907260775566\n",
      "\tTraining Acc: 87.946 Test Acc: 83.3\n",
      "\tTime Taken: 0.9084002216657002 minutes\n",
      "Epoch [31/35]\t\n",
      "\tTraining Loss: 137.35846136510372 Test Loss: 44.387817054986954\n",
      "\tTraining Acc: 87.952 Test Acc: 81.39\n",
      "\tTime Taken: 0.8901338656743367 minutes\n",
      "Epoch [32/35]\t\n",
      "\tTraining Loss: 135.30051650106907 Test Loss: 36.47617715597153\n",
      "\tTraining Acc: 88.036 Test Acc: 84.02\n",
      "\tTime Taken: 0.8900155981381734 minutes\n",
      "Epoch [33/35]\t\n",
      "\tTraining Loss: 135.31318272650242 Test Loss: 37.18614496290684\n",
      "\tTraining Acc: 88.1 Test Acc: 84.08\n",
      "\tTime Taken: 0.8892579793930053 minutes\n",
      "Epoch [34/35]\t\n",
      "\tTraining Loss: 133.13557025790215 Test Loss: 43.788783460855484\n",
      "\tTraining Acc: 88.386 Test Acc: 80.8\n",
      "\tTime Taken: 0.8903223792711894 minutes\n",
      "Epoch [35/35]\t\n",
      "\tTraining Loss: 132.34394662082195 Test Loss: 35.07364892959595\n",
      "\tTraining Acc: 88.516 Test Acc: 84.89\n",
      "\tTime Taken: 0.8899413307507833 minutes\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "        stime = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "        test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "        print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "        time_taken = (time.time()-stime)/60\n",
    "        print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # loading the best model checkpoint from training before testing\n",
    "    out = os.path.join('./saved_models/', save_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "test_dataset = SkipLabelDataset(test_dataset, TARGET_CLASS)\n",
    "if DATASET == \"badnets\":\n",
    "    full_poisoned_test_dataset = BadNetsDataset(test_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_poison, poisoning_rate=1.0, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    full_poisoned_test_dataset = WaNetDataset(test_dataset, TARGET_CLASS, seed=1, transform=transform_poison, poisoning_rate=1.0, noise_rate=0.0, return_original_label=False)\n",
    "elif DATASET == \"sig\":\n",
    "    full_poisoned_test_dataset = SIGDataset(test_dataset, TARGET_CLASS, 20, 6, seed=1, transform=transform_poison, return_original_label=False, attack_test=True)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "\n",
    "\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 85.38\n",
      "Attack Success Rate (ASR): 99.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No attack\n",
    "\n",
    "    Clean Accuracy (C-Acc): 90.49\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "SIG\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.38\n",
    "    Attack Success Rate (ASR): 99.5\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "BadNets\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.88\n",
    "    Attack Success Rate (ASR): 99.66\n",
    "\n",
    "BadNets \\\n",
    "kNN \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.76\n",
    "    Attack Success Rate (ASR): 0.92\n",
    "\n",
    "BadNets \\\n",
    "Energy\n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.49\n",
    "    Attack Success Rate (ASR): 1.91\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "WaNet\n",
    "\n",
    "    Clean Accuracy (C-Acc): 80.93\n",
    "    Attack Success Rate (ASR): 79.58\n",
    "\n",
    "WaNet \\\n",
    "kNN\n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.18\n",
    "    Attack Success Rate (ASR): 4.54\n",
    "\n",
    "WaNet \\\n",
    "Energy\n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.52\n",
    "    Attack Success Rate (ASR): 1.73"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
