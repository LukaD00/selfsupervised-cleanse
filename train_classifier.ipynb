{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import prepare_poison_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"sig-2\"  \n",
    "save_name = f\"{DATASET}-cleansed-NEW.pt\"\n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "CHECKPOINT = f\"{DATASET}.pt\"\n",
    "\n",
    "TRAIN = True\n",
    "CLEANSE = True\n",
    "CLEANSED_LABELS_NAME = f\"{DATASET}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, transforms: torch.nn.Module = None):\n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            predicted_poison = pickle.load(f)\n",
    "\n",
    "        self.data = [poison_dataset[i][0] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.labels = [poison_dataset[i][1] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transforms:\n",
    "            item = self.transforms(item)\n",
    "\n",
    "        return item, label\n",
    "\n",
    "\n",
    "class SkipLabelDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, skip_class: int):\n",
    "        self.return_as_pil = type(original_dataset[0][0]) is Image.Image\n",
    "\n",
    "        targets = np.array(original_dataset.targets)\n",
    "        self.data = original_dataset.data[targets != skip_class]\n",
    "        self.targets = targets[targets != skip_class].tolist()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.return_as_pil:\n",
    "            data = Image.fromarray(data)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ProbTransform(torch.nn.Module):\n",
    "    def __init__(self, f, p=1):\n",
    "        super(ProbTransform, self).__init__()\n",
    "        self.f = f\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if random.random() < self.p:\n",
    "            return self.f(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    ProbTransform(transforms.RandomCrop(32, padding=5), p=0.8),\n",
    "    ProbTransform(transforms.transforms.RandomRotation(10), p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_poison_dataset, _, target_class, train_dataset = prepare_poison_dataset(DATASET, train=True, transform=transform_train, return_original_label=False)\n",
    "if CLEANSE:\n",
    "    train_poison_dataset = CleansedDataset(train_poison_dataset, CLEANSED_LABELS_NAME + \"-train\")\n",
    "trainloader = DataLoader(train_poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "test_poison_dataset, _, _, test_dataset = prepare_poison_dataset(DATASET, train=False, transform=transform_test, return_original_label=False)\n",
    "if CLEANSE:\n",
    "    test_poison_dataset = CleansedDataset(test_poison_dataset, CLEANSED_LABELS_NAME + \"-test\")\n",
    "testloader = DataLoader(test_poison_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a PreAct-Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActBlock(nn.Module):\n",
    "    \"\"\"Pre-activation version of the BasicBlock.\"\"\"\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.ind = None\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        if self.ind is not None:\n",
    "            out += shortcut[:, self.ind, :, :]\n",
    "        else:\n",
    "            out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"Pre-activation version of the original Bottleneck module.\"\"\"\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18(num_classes=10):\n",
    "    return PreActResNet(PreActBlock, [2, 2, 2, 2], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/preact-resnet18', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PreActResNet18()\n",
    "model.to(device)\n",
    "\n",
    "epochs = 35\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [100,200,300,400], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/preact-resnet18/', CHECKPOINT)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")\n",
    "    print(f\"{start_epoch = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, save_name)\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 429.4554075598717 Test Loss: 82.82050740718842\n",
      "\tTraining Acc: 51.27607391758335 Test Acc: 59.032802701398936\n",
      "\tTime Taken: 0.49063427845637003 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 231.68622317910194 Test Loss: 77.09313905239105\n",
      "\tTraining Acc: 74.76506155751439 Test Acc: 67.47467438494935\n",
      "\tTime Taken: 0.4526587049166361 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 152.1083374619484 Test Loss: 36.77268907427788\n",
      "\tTraining Acc: 83.44867778830043 Test Acc: 82.16353111432706\n",
      "\tTime Taken: 0.44227985143661497 minutes\n",
      "Epoch [4/35]\t\n",
      "\tTraining Loss: 110.67691747844219 Test Loss: 44.2239747941494\n",
      "\tTraining Acc: 87.81476894684442 Test Acc: 79.34153400868307\n",
      "\tTime Taken: 0.4421314756075541 minutes\n",
      "Epoch [5/35]\t\n",
      "\tTraining Loss: 77.66851775348186 Test Loss: 42.94603055715561\n",
      "\tTraining Acc: 91.44751220222918 Test Acc: 81.64495899662325\n",
      "\tTime Taken: 0.4420996030171712 minutes\n",
      "Epoch [6/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 60.48312225937843 Test Loss: 49.47419413924217\n",
      "\tTraining Acc: 93.3828707413613 Test Acc: 82.34442836468885\n",
      "\tTime Taken: 0.44419782161712645 minutes\n",
      "Epoch [7/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 44.27882478572428 Test Loss: 45.890744388103485\n",
      "\tTraining Acc: 95.2696631941915 Test Acc: 83.20067534973468\n",
      "\tTime Taken: 0.4323569138844808 minutes\n",
      "Epoch [8/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 31.559781724587083 Test Loss: 42.91693192720413\n",
      "\tTraining Acc: 96.66593817537213 Test Acc: 85.34732272069465\n",
      "\tTime Taken: 0.4324057300885518 minutes\n",
      "Epoch [9/35]\t\n",
      "\tTraining Loss: 28.40624535549432 Test Loss: 42.68575567007065\n",
      "\tTraining Acc: 96.89905538962142 Test Acc: 85.32320308731308\n",
      "\tTime Taken: 0.43073039849599204 minutes\n",
      "Epoch [10/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 21.60032483935356 Test Loss: 37.262758895754814\n",
      "\tTraining Acc: 97.61540516257497 Test Acc: 87.72310660877955\n",
      "\tTime Taken: 0.4325794577598572 minutes\n",
      "Epoch [11/35]\t\n",
      "\tTraining Loss: 18.197822482325137 Test Loss: 43.137982338666916\n",
      "\tTraining Acc: 98.01364707025084 Test Acc: 86.4327062228654\n",
      "\tTime Taken: 0.43089313904444376 minutes\n",
      "Epoch [12/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 15.75041845208034 Test Loss: 35.58759990334511\n",
      "\tTraining Acc: 98.37303610888516 Test Acc: 88.21755909310178\n",
      "\tTime Taken: 0.43267983198165894 minutes\n",
      "Epoch [13/35]\t\n",
      "\tTraining Loss: 11.677678211592138 Test Loss: 41.993602722883224\n",
      "\tTraining Acc: 98.77613462519123 Test Acc: 87.72310660877955\n",
      "\tTime Taken: 0.43089661598205564 minutes\n",
      "Epoch [14/35]\t\n",
      "\tTraining Loss: 14.561848081182688 Test Loss: 41.65339635312557\n",
      "\tTraining Acc: 98.49202302032491 Test Acc: 87.19247467438495\n",
      "\tTime Taken: 0.4309298872947693 minutes\n",
      "Epoch [15/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 8.072976493509486 Test Loss: 38.586967930197716\n",
      "\tTraining Acc: 99.15252179403123 Test Acc: 88.5552339604438\n",
      "\tTime Taken: 0.43307774464289345 minutes\n",
      "Epoch [16/35]\t\n",
      "\tTraining Loss: 7.963864377699792 Test Loss: 39.419465601444244\n",
      "\tTraining Acc: 99.18894635875768 Test Acc: 87.74722624216112\n",
      "\tTime Taken: 0.43390265305836995 minutes\n",
      "Epoch [17/35]\t\n",
      "\tTraining Loss: 7.864004548406228 Test Loss: 98.5572481751442\n",
      "\tTraining Acc: 99.15980670697652 Test Acc: 78.65412445730824\n",
      "\tTime Taken: 0.4323960741360982 minutes\n",
      "Epoch [18/35]\t\n",
      "\tTraining Loss: 12.956537273013964 Test Loss: 36.16521157324314\n",
      "\tTraining Acc: 98.63043636628542 Test Acc: 88.2899179932465\n",
      "\tTime Taken: 0.4325136661529541 minutes\n",
      "Epoch [19/35]\t\n",
      "\tTraining Loss: 7.630608887178823 Test Loss: 40.4870650023222\n",
      "\tTraining Acc: 99.21808601053884 Test Acc: 87.97636275928606\n",
      "\tTime Taken: 0.4322800556818644 minutes\n",
      "Epoch [20/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 4.828102711006068 Test Loss: 32.24726502597332\n",
      "\tTraining Acc: 99.51433913698065 Test Acc: 89.6044380125422\n",
      "\tTime Taken: 0.4335627237955729 minutes\n",
      "Epoch [21/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 4.054606582969427 Test Loss: 29.593452125787735\n",
      "\tTraining Acc: 99.58718826643354 Test Acc: 90.41244573082489\n",
      "\tTime Taken: 0.43269789616266885 minutes\n",
      "Epoch [22/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 1.6754319804458646 Test Loss: 30.26556433737278\n",
      "\tTraining Acc: 99.87129987129987 Test Acc: 90.4968644476604\n",
      "\tTime Taken: 0.4328142921129862 minutes\n",
      "Epoch [23/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 0.9625182134914212 Test Loss: 26.834425672888756\n",
      "\tTraining Acc: 99.93686408780748 Test Acc: 91.36517124939701\n",
      "\tTime Taken: 0.4326804081598918 minutes\n",
      "Epoch [24/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 0.28778655648056883 Test Loss: 24.473146483302116\n",
      "\tTraining Acc: 99.99514339136981 Test Acc: 91.79932465026532\n",
      "\tTime Taken: 0.43271275361378986 minutes\n",
      "Epoch [25/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 0.14100885880179703 Test Loss: 23.752101331949234\n",
      "\tTraining Acc: 100.0 Test Acc: 91.98022190062711\n",
      "\tTime Taken: 0.43255908886591593 minutes\n",
      "Epoch [26/35]\t\n",
      "\tTraining Loss: 0.11689987238059985 Test Loss: 23.18486177176237\n",
      "\tTraining Acc: 100.0 Test Acc: 91.95610226724554\n",
      "\tTime Taken: 0.43091277678807577 minutes\n",
      "Epoch [27/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 0.11025861153757432 Test Loss: 22.71988397091627\n",
      "\tTraining Acc: 100.0 Test Acc: 92.10082006753497\n",
      "\tTime Taken: 0.43244707187016806 minutes\n",
      "Epoch [28/35]\t\n",
      "\tTraining Loss: 0.12157019793085055 Test Loss: 21.89876314997673\n",
      "\tTraining Acc: 100.0 Test Acc: 92.02846116739026\n",
      "\tTime Taken: 0.4360172231992086 minutes\n",
      "Epoch [29/35]\t\n",
      "\tTraining Loss: 0.12164619743998628 Test Loss: 21.621092475950718\n",
      "\tTraining Acc: 100.0 Test Acc: 92.05258080077184\n",
      "\tTime Taken: 0.4702437480290731 minutes\n",
      "Epoch [30/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 0.127043301021331 Test Loss: 21.275484144687653\n",
      "\tTraining Acc: 100.0 Test Acc: 92.17317896767969\n",
      "\tTime Taken: 0.4496631741523743 minutes\n",
      "Epoch [31/35]\t\n",
      "\tTraining Loss: 0.13577059294766514 Test Loss: 20.83854568004608\n",
      "\tTraining Acc: 100.0 Test Acc: 92.06464061746262\n",
      "\tTime Taken: 0.4348631978034973 minutes\n",
      "Epoch [32/35]\t\n",
      "\tTraining Loss: 0.13982235304138158 Test Loss: 20.464584477245808\n",
      "\tTraining Acc: 100.0 Test Acc: 92.14905933429812\n",
      "\tTime Taken: 0.4705129305521647 minutes\n",
      "Epoch [33/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 0.14980287225625943 Test Loss: 20.322289638221264\n",
      "\tTraining Acc: 100.0 Test Acc: 92.25759768451519\n",
      "\tTime Taken: 0.45387513637542726 minutes\n",
      "Epoch [34/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\sig-2-cleansed-NEW.pt\n",
      "\tTraining Loss: 0.15649730208679102 Test Loss: 20.01623345911503\n",
      "\tTraining Acc: 100.0 Test Acc: 92.26965750120598\n",
      "\tTime Taken: 0.4527624527613322 minutes\n",
      "Epoch [35/35]\t\n",
      "\tTraining Loss: 0.162062588671688 Test Loss: 19.766529880464077\n",
      "\tTraining Acc: 100.0 Test Acc: 92.08876025084419\n",
      "\tTime Taken: 0.45424639781316123 minutes\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "        stime = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "        test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "        print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "        time_taken = (time.time()-stime)/60\n",
    "        print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # loading the best model checkpoint from training before testing\n",
    "    out = os.path.join('./saved_models/preact-resnet18/', save_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "test_dataset = SkipLabelDataset(test_dataset, target_class)\n",
    "full_poisoned_test_dataset, _, _, _ = prepare_poison_dataset(DATASET, train=False, transform=transform_poison, return_original_label=False, clean_dataset=test_dataset, full_poison=True)\n",
    "\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 83.99\n",
      "Attack Success Rate (ASR): 0.6444444444444445\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    No attack\n",
    "    Clean Accuracy (C-Acc): 91.07\n",
    "    Attack Success Rate (ASR): 91.07\n",
    "\n",
    "    Badnets1-0\n",
    "    Clean Accuracy (C-Acc): 90.85\n",
    "    Attack Success Rate (ASR): 98.65555555555555\n",
    "\n",
    "    Badnets1-1\n",
    "    Clean Accuracy (C-Acc): 91.18\n",
    "    Attack Success Rate (ASR): 95.54444444444445\n",
    "\n",
    "    Badnets1-2\n",
    "    Clean Accuracy (C-Acc): 90.84\n",
    "    Attack Success Rate (ASR): 91.63333333333334\n",
    "\n",
    "    Badnets10-0\n",
    "    Clean Accuracy (C-Acc): 90.79\n",
    "    Attack Success Rate (ASR): 99.9888888888889\n",
    "\n",
    "    Badnets10-1\n",
    "    Clean Accuracy (C-Acc): 90.71\n",
    "    Attack Success Rate (ASR): 99.93333333333334\n",
    "\n",
    "    Badnets10-2\n",
    "    Clean Accuracy (C-Acc): 89.94\n",
    "    Attack Success Rate (ASR): 99.9888888888889\n",
    "\n",
    "    SIG-0\n",
    "    Clean Accuracy (C-Acc): 91.31\n",
    "    Attack Success Rate (ASR): 100.0\n",
    "\n",
    "    SIG-1\n",
    "    Clean Accuracy (C-Acc): 91.0\n",
    "    Attack Success Rate (ASR): 99.61111111111111\n",
    "\n",
    "    SIG-2\n",
    "    Clean Accuracy (C-Acc): 91.39\n",
    "    Attack Success Rate (ASR): 99.9888888888889\n",
    "\n",
    "    WaNet0\n",
    "    Clean Accuracy (C-Acc): 87.25\n",
    "    Attack Success Rate (ASR): 95.5\n",
    "\n",
    "    WaNet1\n",
    "    Clean Accuracy (C-Acc): 89.14\n",
    "    Attack Success Rate (ASR): 91.53333333333333\n",
    "\n",
    "    WaNet2\n",
    "    Clean Accuracy (C-Acc): 88.26\n",
    "    Attack Success Rate (ASR): 93.3\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "    WITH OUR DEFENSE:\n",
    "\n",
    "    Badnets1-0\n",
    "    Clean Accuracy (C-Acc): 78.45\n",
    "    Attack Success Rate (ASR): 3.1666666666666665\n",
    "\n",
    "    Badnets1-1\n",
    "    Clean Accuracy (C-Acc): 77.65\n",
    "    Attack Success Rate (ASR): 1.5444444444444445\n",
    "\n",
    "    Badnets1-2\n",
    "    Clean Accuracy (C-Acc): 81.88\n",
    "    Attack Success Rate (ASR): 2.0\n",
    "\n",
    "    Badnets10-0\n",
    "    Clean Accuracy (C-Acc): 76.86\n",
    "    Attack Success Rate (ASR): 4.088888888888889\n",
    "\n",
    "    Badnets10-1\n",
    "    Clean Accuracy (C-Acc): 77.52\n",
    "    Attack Success Rate (ASR): 0.3111111111111111\n",
    "\n",
    "    Badnets10-2\n",
    "    Clean Accuracy (C-Acc): 76.43\n",
    "    Attack Success Rate (ASR): 3.4\n",
    "\n",
    "    SIG-0\n",
    "    Clean Accuracy (C-Acc): 82.16\n",
    "    Attack Success Rate (ASR): 0.0\n",
    "\n",
    "    SIG-1\n",
    "    Clean Accuracy (C-Acc): 84.0\n",
    "    Attack Success Rate (ASR): 0.0\n",
    "\n",
    "    SIG-2\n",
    "    Clean Accuracy (C-Acc): 83.99\n",
    "    Attack Success Rate (ASR): 0.6444444444444445\n",
    "\n",
    "    WaNet0\n",
    "    Clean Accuracy (C-Acc): 79.48\n",
    "    Attack Success Rate (ASR): 3.7444444444444445\n",
    "\n",
    "    WaNet1\n",
    "    Clean Accuracy (C-Acc): 81.71\n",
    "    Attack Success Rate (ASR): 1.3\n",
    "\n",
    "    WaNet2\n",
    "    Clean Accuracy (C-Acc): 81.47\n",
    "    Attack Success Rate (ASR): 5.266666666666667"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
