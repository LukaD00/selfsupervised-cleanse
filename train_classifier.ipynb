{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import prepare_poison_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"wanet-2\"  \n",
    "save_name = f\"{DATASET}-NEW.pt\"\n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "CHECKPOINT = f\"{DATASET}.pt\"\n",
    "\n",
    "TRAIN = True\n",
    "CLEANSE = False\n",
    "CLEANSED_LABELS_NAME = f\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, transforms: torch.nn.Module = None):\n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            predicted_poison = pickle.load(f)\n",
    "\n",
    "        self.data = [poison_dataset[i][0] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.labels = [poison_dataset[i][1] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transforms:\n",
    "            item = self.transforms(item)\n",
    "\n",
    "        return item, label\n",
    "\n",
    "\n",
    "class SkipLabelDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, skip_class: int):\n",
    "        self.return_as_pil = type(original_dataset[0][0]) is Image.Image\n",
    "\n",
    "        targets = np.array(original_dataset.targets)\n",
    "        self.data = original_dataset.data[targets != skip_class]\n",
    "        self.targets = targets[targets != skip_class].tolist()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.return_as_pil:\n",
    "            data = Image.fromarray(data)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ProbTransform(torch.nn.Module):\n",
    "    def __init__(self, f, p=1):\n",
    "        super(ProbTransform, self).__init__()\n",
    "        self.f = f\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if random.random() < self.p:\n",
    "            return self.f(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    ProbTransform(transforms.RandomCrop(32, padding=5), p=0.8),\n",
    "    ProbTransform(transforms.transforms.RandomRotation(10), p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_poison_dataset, _, target_class, train_dataset = prepare_poison_dataset(DATASET, train=True, transform=transform_train, return_original_label=False)\n",
    "if CLEANSE:\n",
    "    train_poison_dataset = CleansedDataset(train_poison_dataset, CLEANSED_LABELS_NAME + \"-train\")\n",
    "trainloader = DataLoader(train_poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "test_poison_dataset, _, _, test_dataset = prepare_poison_dataset(DATASET, train=False, transform=transform_test, return_original_label=False)\n",
    "if CLEANSE:\n",
    "    test_poison_dataset = CleansedDataset(test_poison_dataset, CLEANSED_LABELS_NAME + \"-test\")\n",
    "testloader = DataLoader(test_poison_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a PreAct-Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActBlock(nn.Module):\n",
    "    \"\"\"Pre-activation version of the BasicBlock.\"\"\"\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.ind = None\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        if self.ind is not None:\n",
    "            out += shortcut[:, self.ind, :, :]\n",
    "        else:\n",
    "            out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"Pre-activation version of the original Bottleneck module.\"\"\"\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18(num_classes=10):\n",
    "    return PreActResNet(PreActBlock, [2, 2, 2, 2], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/preact-resnet18', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PreActResNet18()\n",
    "model.to(device)\n",
    "\n",
    "epochs = 35\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [100,200,300,400], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/preact-resnet18/', CHECKPOINT)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")\n",
    "    print(f\"{start_epoch = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, save_name)\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 619.7898663282394 Test Loss: 107.60147047042847\n",
      "\tTraining Acc: 40.542 Test Acc: 50.07\n",
      "\tTime Taken: 1.2289062460263571 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 472.93381744623184 Test Loss: 101.3351708650589\n",
      "\tTraining Acc: 55.728 Test Acc: 54.66\n",
      "\tTime Taken: 1.220379106203715 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 402.37851852178574 Test Loss: 75.69884467124939\n",
      "\tTraining Acc: 62.636 Test Acc: 64.92\n",
      "\tTime Taken: 1.219195059935252 minutes\n",
      "Epoch [4/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 356.5334636569023 Test Loss: 73.29535222053528\n",
      "\tTraining Acc: 67.076 Test Acc: 67.42\n",
      "\tTime Taken: 1.2310031334559122 minutes\n",
      "Epoch [5/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 320.27137702703476 Test Loss: 62.85719746351242\n",
      "\tTraining Acc: 70.4 Test Acc: 71.01\n",
      "\tTime Taken: 1.2263896822929383 minutes\n",
      "Epoch [6/35]\t\n",
      "\tTraining Loss: 291.79531395435333 Test Loss: 74.805957198143\n",
      "\tTraining Acc: 73.092 Test Acc: 67.93\n",
      "\tTime Taken: 1.219990030924479 minutes\n",
      "Epoch [7/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 272.4299621284008 Test Loss: 60.29033666849136\n",
      "\tTraining Acc: 74.772 Test Acc: 73.17\n",
      "\tTime Taken: 1.2189402818679809 minutes\n",
      "Epoch [8/35]\t\n",
      "\tTraining Loss: 257.21774688363075 Test Loss: 63.32727289199829\n",
      "\tTraining Acc: 76.412 Test Acc: 70.55\n",
      "\tTime Taken: 1.2200681726137796 minutes\n",
      "Epoch [9/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 239.5409136712551 Test Loss: 50.39006760716438\n",
      "\tTraining Acc: 77.892 Test Acc: 77.45\n",
      "\tTime Taken: 1.217739967505137 minutes\n",
      "Epoch [10/35]\t\n",
      "\tTraining Loss: 224.5783398449421 Test Loss: 55.23412489891052\n",
      "\tTraining Acc: 79.388 Test Acc: 75.0\n",
      "\tTime Taken: 1.220008381207784 minutes\n",
      "Epoch [11/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 213.62514647841454 Test Loss: 53.27137267589569\n",
      "\tTraining Acc: 80.418 Test Acc: 77.7\n",
      "\tTime Taken: 1.2176466147104898 minutes\n",
      "Epoch [12/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 199.5470328629017 Test Loss: 48.86504876613617\n",
      "\tTraining Acc: 81.848 Test Acc: 78.8\n",
      "\tTime Taken: 1.2220827221870423 minutes\n",
      "Epoch [13/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 190.75577861070633 Test Loss: 40.45430105924606\n",
      "\tTraining Acc: 82.67 Test Acc: 82.71\n",
      "\tTime Taken: 1.2188434918721518 minutes\n",
      "Epoch [14/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 179.69046023488045 Test Loss: 41.10784646868706\n",
      "\tTraining Acc: 83.576 Test Acc: 82.79\n",
      "\tTime Taken: 1.2172571619351704 minutes\n",
      "Epoch [15/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 169.30532184243202 Test Loss: 39.06379544734955\n",
      "\tTraining Acc: 84.644 Test Acc: 83.57\n",
      "\tTime Taken: 1.2178333322207133 minutes\n",
      "Epoch [16/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 163.10810801386833 Test Loss: 34.116226345300674\n",
      "\tTraining Acc: 85.032 Test Acc: 85.41\n",
      "\tTime Taken: 1.2173400123914082 minutes\n",
      "Epoch [17/35]\t\n",
      "\tTraining Loss: 155.82097126543522 Test Loss: 36.6322680413723\n",
      "\tTraining Acc: 85.806 Test Acc: 83.92\n",
      "\tTime Taken: 1.2226253588994345 minutes\n",
      "Epoch [18/35]\t\n",
      "\tTraining Loss: 150.87586778402328 Test Loss: 45.848870903253555\n",
      "\tTraining Acc: 86.356 Test Acc: 82.46\n",
      "\tTime Taken: 1.2167640368143717 minutes\n",
      "Epoch [19/35]\t\n",
      "\tTraining Loss: 143.64056354761124 Test Loss: 35.93511724472046\n",
      "\tTraining Acc: 86.962 Test Acc: 85.09\n",
      "\tTime Taken: 1.238294816017151 minutes\n",
      "Epoch [20/35]\t\n",
      "\tTraining Loss: 136.90378084778786 Test Loss: 36.04661697149277\n",
      "\tTraining Acc: 87.548 Test Acc: 85.29\n",
      "\tTime Taken: 1.226728888352712 minutes\n",
      "Epoch [21/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 131.63695508241653 Test Loss: 34.11381983757019\n",
      "\tTraining Acc: 88.022 Test Acc: 85.87\n",
      "\tTime Taken: 1.2281236449877422 minutes\n",
      "Epoch [22/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 127.38551819324493 Test Loss: 32.10606923699379\n",
      "\tTraining Acc: 88.428 Test Acc: 86.58\n",
      "\tTime Taken: 1.2257924358050027 minutes\n",
      "Epoch [23/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 121.99121825397015 Test Loss: 34.39763158559799\n",
      "\tTraining Acc: 89.1 Test Acc: 86.6\n",
      "\tTime Taken: 1.2259726166725158 minutes\n",
      "Epoch [24/35]\t\n",
      "\tTraining Loss: 116.77372844517231 Test Loss: 35.429901987314224\n",
      "\tTraining Acc: 89.438 Test Acc: 86.05\n",
      "\tTime Taken: 1.226427670319875 minutes\n",
      "Epoch [25/35]\t\n",
      "\tTraining Loss: 114.78344996273518 Test Loss: 37.654707074165344\n",
      "\tTraining Acc: 89.456 Test Acc: 85.14\n",
      "\tTime Taken: 1.2254713455835977 minutes\n",
      "Epoch [26/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 111.99933941662312 Test Loss: 30.537833482027054\n",
      "\tTraining Acc: 89.924 Test Acc: 87.28\n",
      "\tTime Taken: 1.228059470653534 minutes\n",
      "Epoch [27/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 105.30430971086025 Test Loss: 29.797031313180923\n",
      "\tTraining Acc: 90.468 Test Acc: 88.07\n",
      "\tTime Taken: 1.2262841542561849 minutes\n",
      "Epoch [28/35]\t\n",
      "\tTraining Loss: 103.34247501194477 Test Loss: 33.518891870975494\n",
      "\tTraining Acc: 90.598 Test Acc: 86.93\n",
      "\tTime Taken: 1.2234974225362143 minutes\n",
      "Epoch [29/35]\t\n",
      "\tTraining Loss: 101.94279230386019 Test Loss: 33.82483513653278\n",
      "\tTraining Acc: 90.782 Test Acc: 86.74\n",
      "\tTime Taken: 1.227064275741577 minutes\n",
      "Epoch [30/35]\t\n",
      "\tTraining Loss: 97.19752266258001 Test Loss: 36.48360860347748\n",
      "\tTraining Acc: 91.284 Test Acc: 86.36\n",
      "\tTime Taken: 1.2200560490290324 minutes\n",
      "Epoch [31/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 95.14947450906038 Test Loss: 32.02802327275276\n",
      "\tTraining Acc: 91.284 Test Acc: 88.09\n",
      "\tTime Taken: 1.221456563472748 minutes\n",
      "Epoch [32/35]\t\n",
      "\tTraining Loss: 92.39025386422873 Test Loss: 35.45042419433594\n",
      "\tTraining Acc: 91.584 Test Acc: 86.27\n",
      "\tTime Taken: 1.221560017267863 minutes\n",
      "Epoch [33/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 89.90055081248283 Test Loss: 30.02404823899269\n",
      "\tTraining Acc: 91.73 Test Acc: 88.19\n",
      "\tTime Taken: 1.2219385504722595 minutes\n",
      "Epoch [34/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/preact-resnet18\\wanet-2-NEW.pt\n",
      "\tTraining Loss: 87.8522062972188 Test Loss: 31.694619089365005\n",
      "\tTraining Acc: 92.082 Test Acc: 88.23\n",
      "\tTime Taken: 1.2269396901130676 minutes\n",
      "Epoch [35/35]\t\n",
      "\tTraining Loss: 84.85757502913475 Test Loss: 31.50342558324337\n",
      "\tTraining Acc: 92.28 Test Acc: 88.13\n",
      "\tTime Taken: 1.2262275616327922 minutes\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "        stime = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "        test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "        print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "        time_taken = (time.time()-stime)/60\n",
    "        print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # loading the best model checkpoint from training before testing\n",
    "    out = os.path.join('./saved_models/preact-resnet18/', save_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "test_dataset = SkipLabelDataset(test_dataset, target_class)\n",
    "full_poisoned_test_dataset, _, _, _ = prepare_poison_dataset(DATASET, train=False, transform=transform_poison, return_original_label=False, clean_dataset=test_dataset, full_poison=True)\n",
    "\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 88.26\n",
      "Attack Success Rate (ASR): 93.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    No attack\n",
    "    Clean Accuracy (C-Acc): 91.07\n",
    "    Attack Success Rate (ASR): 91.07\n",
    "\n",
    "    Badnets1-0\n",
    "    Clean Accuracy (C-Acc): 90.85\n",
    "    Attack Success Rate (ASR): 98.65555555555555\n",
    "\n",
    "    Badnets1-1\n",
    "    Clean Accuracy (C-Acc): 91.18\n",
    "    Attack Success Rate (ASR): 95.54444444444445\n",
    "\n",
    "    Badnets1-2\n",
    "    Clean Accuracy (C-Acc): 90.84\n",
    "    Attack Success Rate (ASR): 91.63333333333334\n",
    "\n",
    "    Badnets10-0\n",
    "    Clean Accuracy (C-Acc): 90.79\n",
    "    Attack Success Rate (ASR): 99.9888888888889\n",
    "\n",
    "    Badnets10-1\n",
    "    Clean Accuracy (C-Acc): 90.71\n",
    "    Attack Success Rate (ASR): 99.93333333333334\n",
    "\n",
    "    Badnets10-2\n",
    "    Clean Accuracy (C-Acc): 89.94\n",
    "    Attack Success Rate (ASR): 99.9888888888889\n",
    "\n",
    "    SIG-0\n",
    "    Clean Accuracy (C-Acc): 91.31\n",
    "    Attack Success Rate (ASR): 100.0\n",
    "\n",
    "    SIG-1\n",
    "    Clean Accuracy (C-Acc): 91.0\n",
    "    Attack Success Rate (ASR): 99.61111111111111\n",
    "\n",
    "    SIG-2\n",
    "    Clean Accuracy (C-Acc): 91.39\n",
    "    Attack Success Rate (ASR): 99.9888888888889\n",
    "\n",
    "    WaNet0\n",
    "    Clean Accuracy (C-Acc): 87.25\n",
    "    Attack Success Rate (ASR): 95.5\n",
    "\n",
    "    WaNet1\n",
    "    Clean Accuracy (C-Acc): 89.14\n",
    "    Attack Success Rate (ASR): 91.53333333333333\n",
    "\n",
    "    WaNet2\n",
    "    Clean Accuracy (C-Acc): 88.26\n",
    "    Attack Success Rate (ASR): 93.3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
