{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import BadNetsDataset, WaNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128\n",
    "epochs = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, strategy: str = \"remove\"):\n",
    "        self.poison_dataset = poison_dataset\n",
    "        self.poison_indices = list(range(len(self.poison_dataset)))\n",
    "        \n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            self.predicted_labels = pickle.load(f)\n",
    "\n",
    "        assert strategy in [\"relabel\", \"remove\"]\n",
    "        self.strategy = strategy\n",
    "\n",
    "        poison_labels = [poison_label for _, poison_label in self.poison_dataset]\n",
    "        if self.strategy == \"remove\":\n",
    "            self.indices = [index for index in self.poison_indices if poison_labels[index]==self.predicted_labels[index]]\n",
    "        elif self.strategy == \"relabel\":\n",
    "            self.indices = self.poison_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index > len(self):\n",
    "            return IndexError()\n",
    "        while index not in self.indices:\n",
    "            index += 1\n",
    "\n",
    "        item = self.poison_dataset[index][0]\n",
    "        label = self.predicted_labels[index]\n",
    "\n",
    "        return item, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_train_clean = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=True, download=False)\n",
    "#train_dataset_transforms = torchvision.datasets.CIFAR10(root='C:/Datasets', train=True, download=False, transform=transform_train_clean)\n",
    "#train_poison_dataset = BadNetsDataset(train_dataset, 0, \"triggers/trigger_white.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "#train_poison_dataset = WaNetDataset(train_dataset, 0, seed=1, transform=transform_train, return_original_label=False)\n",
    "train_poison_dataset = BadNetsDataset(train_dataset, 1, \"triggers/trigger_10.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "train_cleansed_dataset = CleansedDataset(train_poison_dataset, \"BadNets2-Energy-train\", strategy=\"remove\")\n",
    "trainloader = DataLoader(train_cleansed_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "#test_dataset_transforms = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_test_clean)\n",
    "#test_poison_dataset = BadNetsDataset(test_dataset, 0, \"triggers/trigger_white.png\", seed=1, transform=transform_test, return_original_label=False)\n",
    "#test_poison_dataset = WaNetDataset(test_dataset, 0, seed=1, transform=transform_test, return_original_label=False)\n",
    "test_poison_dataset = BadNetsDataset(test_dataset, 1, \"triggers/trigger_10.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "test_cleansed_dataset = CleansedDataset(test_poison_dataset, \"BadNets2-Energy-test\", strategy=\"remove\")\n",
    "testloader = DataLoader(test_cleansed_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "load_checkpoint = True\n",
    "checkpoint_name = \"BadNets2-Resnet-Energy.pt\"\n",
    "\n",
    "if load_checkpoint:\n",
    "    out = os.path.join('./saved_models/', checkpoint_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "best_model = None\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, f\"Resnet-18.pt\")\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 550.7471189498901 Test Loss: 83.51505696773529\n",
      "\tTraining Acc: 33.22002045159329 Test Acc: 46.04036186499652\n",
      "\tTime Taken: 0.8384736021359761 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 375.1319259405136 Test Loss: 68.04603880643845\n",
      "\tTraining Acc: 51.55459746289694 Test Acc: 57.60612386917189\n",
      "\tTime Taken: 0.8173271775245666 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 289.09011709690094 Test Loss: 58.55984503030777\n",
      "\tTraining Acc: 63.22306055329851 Test Acc: 63.799582463465555\n",
      "\tTime Taken: 0.9311299562454224 minutes\n",
      "Epoch [4/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 225.76541778445244 Test Loss: 51.563322842121124\n",
      "\tTraining Acc: 71.65796092087444 Test Acc: 68.99095337508699\n",
      "\tTime Taken: 0.8742744366327921 minutes\n",
      "Epoch [5/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 181.56660789251328 Test Loss: 48.37181907892227\n",
      "\tTraining Acc: 77.49495619489815 Test Acc: 71.51009046624912\n",
      "\tTime Taken: 0.8909124811490376 minutes\n",
      "Epoch [6/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 151.3561986386776 Test Loss: 37.430979281663895\n",
      "\tTraining Acc: 81.2149351905591 Test Acc: 77.27209464161447\n",
      "\tTime Taken: 0.8672182520230611 minutes\n",
      "Epoch [7/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 122.89005549252033 Test Loss: 38.14008009433746\n",
      "\tTraining Acc: 84.90174944034491 Test Acc: 77.3277661795407\n",
      "\tTime Taken: 0.9628637274106343 minutes\n",
      "Epoch [8/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 100.21425811946392 Test Loss: 32.60120275616646\n",
      "\tTraining Acc: 87.61849487328303 Test Acc: 80.62630480167014\n",
      "\tTime Taken: 0.9307744026184082 minutes\n",
      "Epoch [9/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 87.97183732688427 Test Loss: 32.547620713710785\n",
      "\tTraining Acc: 88.99759555592405 Test Acc: 81.26652748782185\n",
      "\tTime Taken: 0.8924262960751851 minutes\n",
      "Epoch [10/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 77.95389081537724 Test Loss: 26.083150804042816\n",
      "\tTraining Acc: 90.41538844208607 Test Acc: 84.50939457202506\n",
      "\tTime Taken: 0.8854422370592753 minutes\n",
      "Epoch [11/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 72.9897979348898 Test Loss: 22.366736084222794\n",
      "\tTraining Acc: 91.05933725782826 Test Acc: 86.4857341684064\n",
      "\tTime Taken: 0.8445661902427674 minutes\n",
      "Epoch [12/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 69.3499262407422 Test Loss: 21.283771380782127\n",
      "\tTraining Acc: 91.33571014012105 Test Acc: 87.23729993041057\n",
      "\tTime Taken: 0.8572547992070516 minutes\n",
      "Epoch [13/35]\t\n",
      "\tTraining Loss: 68.15598196536303 Test Loss: 29.50906689465046\n",
      "\tTraining Acc: 91.56233590360114 Test Acc: 82.9366736256089\n",
      "\tTime Taken: 0.8927453358968099 minutes\n",
      "Epoch [14/35]\t\n",
      "\tTraining Loss: 64.58356278389692 Test Loss: 27.24286150187254\n",
      "\tTraining Acc: 92.0929718376033 Test Acc: 83.1454418928323\n",
      "\tTime Taken: 0.9029122551282247 minutes\n",
      "Epoch [15/35]\t\n",
      "\tTraining Loss: 62.815600983798504 Test Loss: 34.23730206489563\n",
      "\tTraining Acc: 92.40250946577122 Test Acc: 82.64439805149617\n",
      "\tTime Taken: 0.8534251133600871 minutes\n",
      "Epoch [16/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 59.8553980961442 Test Loss: 20.479992359876633\n",
      "\tTraining Acc: 92.58491556808445 Test Acc: 88.07237299930411\n",
      "\tTime Taken: 0.8528358181317647 minutes\n",
      "Epoch [17/35]\t\n",
      "\tTraining Loss: 59.349858574569225 Test Loss: 23.518378242850304\n",
      "\tTraining Acc: 92.79219522980405 Test Acc: 86.34655532359082\n",
      "\tTime Taken: 0.8939682881037394 minutes\n",
      "Epoch [18/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 56.59623822569847 Test Loss: 18.45030826330185\n",
      "\tTraining Acc: 93.16806234972225 Test Acc: 89.0883785664579\n",
      "\tTime Taken: 0.8902943015098572 minutes\n",
      "Epoch [19/35]\t\n",
      "\tTraining Loss: 56.38798886537552 Test Loss: 35.57114416360855\n",
      "\tTraining Acc: 93.1929359091286 Test Acc: 80.69589422407795\n",
      "\tTime Taken: 0.8227338234583537 minutes\n",
      "Epoch [20/35]\t\n",
      "\tTraining Loss: 54.238206923007965 Test Loss: 23.942771032452583\n",
      "\tTraining Acc: 93.47207252024431 Test Acc: 86.63883089770354\n",
      "\tTime Taken: 0.8018160899480183 minutes\n",
      "Epoch [21/35]\t\n",
      "\tTraining Loss: 52.450833950191736 Test Loss: 30.02575047314167\n",
      "\tTraining Acc: 93.60473150374486 Test Acc: 84.4258872651357\n",
      "\tTime Taken: 0.7760932207107544 minutes\n",
      "Epoch [22/35]\t\n",
      "\tTraining Loss: 50.90936153382063 Test Loss: 19.13811695575714\n",
      "\tTraining Acc: 93.7981925213498 Test Acc: 88.40640222686152\n",
      "\tTime Taken: 0.7722159226735433 minutes\n",
      "Epoch [23/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 52.810899414122105 Test Loss: 17.526645079255104\n",
      "\tTraining Acc: 93.55222065610923 Test Acc: 89.50591510090466\n",
      "\tTime Taken: 0.9211637576421102 minutes\n",
      "Epoch [24/35]\t\n",
      "\tTraining Loss: 50.722015880048275 Test Loss: 31.909639552235603\n",
      "\tTraining Acc: 93.78990133488102 Test Acc: 82.92275574112735\n",
      "\tTime Taken: 1.093705157438914 minutes\n",
      "Epoch [25/35]\t\n",
      "\tTraining Loss: 50.60535651817918 Test Loss: 17.980013445019722\n",
      "\tTraining Acc: 93.74291794489125 Test Acc: 89.24147529575505\n",
      "\tTime Taken: 1.0492000857988992 minutes\n",
      "Epoch [26/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/Resnet-18.pt\n",
      "\tTraining Loss: 48.43217074126005 Test Loss: 17.277040615677834\n",
      "\tTraining Acc: 94.12983998010115 Test Acc: 90.22964509394572\n",
      "\tTime Taken: 1.0129051804542542 minutes\n",
      "Epoch [27/35]\t\n",
      "\tTraining Loss: 47.780298333615065 Test Loss: 18.470293149352074\n",
      "\tTraining Acc: 94.4117403200398 Test Acc: 89.0187891440501\n",
      "\tTime Taken: 0.8366599361101786 minutes\n",
      "Epoch [28/35]\t\n",
      "\tTraining Loss: 48.900896318256855 Test Loss: 16.259859412908554\n",
      "\tTraining Acc: 94.11049387834066 Test Acc: 90.11830201809325\n",
      "\tTime Taken: 0.8430108944574992 minutes\n",
      "Epoch [29/35]\t\n",
      "\tTraining Loss: 48.38596234843135 Test Loss: 20.623003035783768\n",
      "\tTraining Acc: 94.15747726833042 Test Acc: 88.33681280445373\n",
      "\tTime Taken: 0.7983147501945496 minutes\n",
      "Epoch [30/35]\t\n",
      "\tTraining Loss: 44.584925036877394 Test Loss: 18.286606580018997\n",
      "\tTraining Acc: 94.68811320233259 Test Acc: 89.7981906750174\n",
      "\tTime Taken: 0.796167004108429 minutes\n",
      "Epoch [31/35]\t\n",
      "\tTraining Loss: 44.59357751905918 Test Loss: 23.471123442053795\n",
      "\tTraining Acc: 94.56098167647791 Test Acc: 87.07028531663187\n",
      "\tTime Taken: 0.8019409060478211 minutes\n",
      "Epoch [32/35]\t\n",
      "\tTraining Loss: 45.149260964244604 Test Loss: 19.150675401091576\n",
      "\tTraining Acc: 94.60243760882182 Test Acc: 89.17188587334725\n",
      "\tTime Taken: 1.0111775000890095 minutes\n",
      "Epoch [33/35]\t\n",
      "\tTraining Loss: 45.65033978223801 Test Loss: 20.01294869184494\n",
      "\tTraining Acc: 94.44490506591494 Test Acc: 87.96102992345163\n",
      "\tTime Taken: 0.9205599268277486 minutes\n",
      "Epoch [34/35]\t\n",
      "\tTraining Loss: 43.02994490414858 Test Loss: 19.20781460404396\n",
      "\tTraining Acc: 94.73233286349944 Test Acc: 88.97703549060543\n",
      "\tTime Taken: 0.8213315010070801 minutes\n",
      "Epoch [35/35]\t\n",
      "\tTraining Loss: 41.93512301892042 Test Loss: 22.023921087384224\n",
      "\tTraining Acc: 95.07779896636542 Test Acc: 87.12595685455811\n",
      "\tTime Taken: 0.8414888461430867 minutes\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs+1):\n",
    "    print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "    stime = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "    test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "    print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "    time_taken = (time.time()-stime)/60\n",
    "    print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider loading the best model checkpoint before running\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "#full_poisoned_test_dataset = BadNetsDataset(test_dataset, 0, \"triggers/trigger_white.png\", seed=1, transform=transform_train, poisoning_rate=1.0, return_original_label=False)\n",
    "#full_poisoned_test_dataset = WaNetDataset(test_dataset, 0, seed=1, transform=transform_poison, poisoning_rate=1.0, noise_rate=0.0, return_original_label=False)\n",
    "full_poisoned_test_dataset = BadNetsDataset(test_dataset, 1, \"triggers/trigger_10.png\", seed=1, transform=transform_poison, poisoning_rate=1.0, return_original_label=False)\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 79.49\n",
      "Attack Success Rate (ASR): 11.28\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 90.49\n",
    "    Attack Success Rate (ASR): 10.63\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 89.19\n",
    "    Attack Success Rate (ASR): 62.01\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets \\\n",
    "kNN \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 84.16\n",
    "    Attack Success Rate (ASR): 9.25\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets2 \\\n",
    "ResNet-18\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.06\n",
    "    Attack Success Rate (ASR): 98.9\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets2 \\\n",
    "kNN \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 77.89\n",
    "    Attack Success Rate (ASR): 10.92\n",
    "\n",
    "\n",
    "CIFAR-10 \\\n",
    "BadNets2 \\\n",
    "Energy \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.49\n",
    "    Attack Success Rate (ASR): 11.28\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "ResNet-18 \\\n",
    "\n",
    "    Clean Accuracy (C-Acc): 83.32\n",
    "    Attack Success Rate (ASR): 83.46\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "kNN \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 83.08\n",
    "    Attack Success Rate (ASR): 12.50\n",
    "\n",
    "CIFAR-10 \\\n",
    "WaNet \\\n",
    "Energy \\\n",
    "ResNet-18 \n",
    "\n",
    "    Clean Accuracy (C-Acc): 79.52\n",
    "    Attack Success Rate (ASR): 9.52"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
