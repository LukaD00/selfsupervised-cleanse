{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import BadNetsDataset, WaNetDataset, SIGDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "CLEANSE = True\n",
    "CLEANSED_LABELS_NAME = \"BadNets-Energy\" \n",
    "\n",
    "LOAD_CHECKPOINT = False\n",
    "CHECKPOINT = \"\"\n",
    "DATASET = \"badnets\"  \n",
    "\n",
    "if DATASET == \"badnets\":\n",
    "    TARGET_CLASS = 1\n",
    "elif DATASET == \"wanet\":\n",
    "    TARGET_CLASS = 0\n",
    "elif DATASET == \"sig\":\n",
    "    TARGET_CLASS = 0\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, strategy: str = \"remove\"):\n",
    "        self.poison_dataset = poison_dataset\n",
    "        self.poison_indices = list(range(len(self.poison_dataset)))\n",
    "        \n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            self.predicted_labels = pickle.load(f)\n",
    "\n",
    "        assert strategy in [\"relabel\", \"remove\"]\n",
    "        self.strategy = strategy\n",
    "\n",
    "        poison_labels = [poison_label for _, poison_label in self.poison_dataset]\n",
    "        if self.strategy == \"remove\":\n",
    "            self.indices = [index for index in self.poison_indices if poison_labels[index]==self.predicted_labels[index]]\n",
    "        elif self.strategy == \"relabel\":\n",
    "            self.indices = self.poison_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index > len(self):\n",
    "            return IndexError()\n",
    "        while index not in self.indices:\n",
    "            index += 1\n",
    "\n",
    "        item = self.poison_dataset[index][0]\n",
    "        label = self.predicted_labels[index]\n",
    "\n",
    "        return item, label\n",
    "        \n",
    "\n",
    "class SkipLabelDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, skip_class: int):\n",
    "        self.return_as_pil = type(original_dataset[0][0]) is Image.Image\n",
    "\n",
    "        targets = np.array(original_dataset.targets)\n",
    "        self.data = original_dataset.data[targets != skip_class]\n",
    "        self.targets = targets[targets != skip_class].tolist()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.return_as_pil:\n",
    "            data = Image.fromarray(data)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_train_clean = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=True, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    train_poison_dataset = BadNetsDataset(train_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_train, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    train_poison_dataset = WaNetDataset(train_dataset, TARGET_CLASS, seed=1, transform=transform_train, return_original_label=False)\n",
    "elif DATASET == \"sig\":\n",
    "    train_poison_dataset = SIGDataset(train_dataset, TARGET_CLASS, 20, 6, seed=1, transform=transform_train, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "if CLEANSE:\n",
    "    train_poison_dataset = CleansedDataset(train_poison_dataset, CLEANSED_LABELS_NAME + \"-train\", strategy=\"remove\")\n",
    "trainloader = DataLoader(train_poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "if DATASET == \"badnets\":\n",
    "    test_poison_dataset = BadNetsDataset(test_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_test, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    test_poison_dataset = WaNetDataset(test_dataset, TARGET_CLASS, seed=1, transform=transform_test, return_original_label=False)\n",
    "elif DATASET == \"sig\":\n",
    "    test_poison_dataset = SIGDataset(test_dataset, TARGET_CLASS, 20, 6, seed=1, transform=transform_test, return_original_label=False)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "if CLEANSE:\n",
    "    test_poison_dataset = CleansedDataset(test_poison_dataset, CLEANSED_LABELS_NAME + \"-test\", strategy=\"remove\")\n",
    "testloader = DataLoader(test_poison_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "epochs = 35\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/', CHECKPOINT)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")\n",
    "    print(f\"{start_epoch = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "save_name = \"NEW-Resnet-18.pt\"\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, save_name)\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 489.8726637363434 Test Loss: 80.97724997997284\n",
      "\tTraining Acc: 33.63246554364472 Test Acc: 42.21709006928406\n",
      "\tTime Taken: 0.6835403760274251 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 351.64136493206024 Test Loss: 64.79570960998535\n",
      "\tTraining Acc: 49.14241960183767 Test Acc: 55.996920708237106\n",
      "\tTime Taken: 0.6778564373652141 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 283.69019907712936 Test Loss: 68.53185576200485\n",
      "\tTraining Acc: 59.742725880551305 Test Acc: 58.06004618937644\n",
      "\tTime Taken: 0.7012741446495057 minutes\n",
      "Epoch [4/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 230.0230113863945 Test Loss: 45.38444423675537\n",
      "\tTraining Acc: 67.75497702909648 Test Acc: 69.56120092378752\n",
      "\tTime Taken: 0.7433783014615377 minutes\n",
      "Epoch [5/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 181.42560276389122 Test Loss: 39.35888212919235\n",
      "\tTraining Acc: 74.76263399693721 Test Acc: 73.50269438029254\n",
      "\tTime Taken: 0.6934492309888204 minutes\n",
      "Epoch [6/35]\t\n",
      "\tTraining Loss: 153.9956452548504 Test Loss: 42.85059779882431\n",
      "\tTraining Acc: 78.82695252679939 Test Acc: 71.67051578137028\n",
      "\tTime Taken: 0.6878434578577678 minutes\n",
      "Epoch [7/35]\t\n",
      "\tTraining Loss: 120.4624935388565 Test Loss: 54.13431388139725\n",
      "\tTraining Acc: 83.45788667687596 Test Acc: 73.30254041570439\n",
      "\tTime Taken: 0.6849577744801839 minutes\n",
      "Epoch [8/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 102.04518911242485 Test Loss: 24.653270810842514\n",
      "\tTraining Acc: 86.01225114854518 Test Acc: 83.01770592763664\n",
      "\tTime Taken: 0.6897823492685954 minutes\n",
      "Epoch [9/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 83.38447777926922 Test Loss: 24.999240174889565\n",
      "\tTraining Acc: 88.44410413476264 Test Acc: 83.74133949191686\n",
      "\tTime Taken: 0.6858480374018351 minutes\n",
      "Epoch [10/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 71.03994043916464 Test Loss: 25.018772527575493\n",
      "\tTraining Acc: 90.31240428790198 Test Acc: 84.26481909160893\n",
      "\tTime Taken: 0.6859522541364034 minutes\n",
      "Epoch [11/35]\t\n",
      "\tTraining Loss: 65.65876293927431 Test Loss: 42.42247346043587\n",
      "\tTraining Acc: 91.15467075038285 Test Acc: 76.08929946112394\n",
      "\tTime Taken: 0.6832184632619221 minutes\n",
      "Epoch [12/35]\t\n",
      "\tTraining Loss: 65.81310807913542 Test Loss: 36.53143090009689\n",
      "\tTraining Acc: 91.24961715160796 Test Acc: 80.24634334103156\n",
      "\tTime Taken: 0.6821991602579752 minutes\n",
      "Epoch [13/35]\t\n",
      "\tTraining Loss: 67.35310136526823 Test Loss: 28.435166910290718\n",
      "\tTraining Acc: 90.97702909647779 Test Acc: 83.24865280985374\n",
      "\tTime Taken: 0.6836682081222534 minutes\n",
      "Epoch [14/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 62.72939972579479 Test Loss: 16.635351464152336\n",
      "\tTraining Acc: 91.50382848392037 Test Acc: 88.85296381832178\n",
      "\tTime Taken: 0.6845796585083008 minutes\n",
      "Epoch [15/35]\t\n",
      "\tTraining Loss: 51.58019156754017 Test Loss: 33.80280816555023\n",
      "\tTraining Acc: 93.18529862174579 Test Acc: 81.32409545804465\n",
      "\tTime Taken: 0.684074866771698 minutes\n",
      "Epoch [16/35]\t\n",
      "\tTraining Loss: 67.9786102026701 Test Loss: 22.814752981066704\n",
      "\tTraining Acc: 90.84532924961715 Test Acc: 86.37413394919169\n",
      "\tTime Taken: 0.6831919034322103 minutes\n",
      "Epoch [17/35]\t\n",
      "\tTraining Loss: 49.0726407058537 Test Loss: 29.440488040447235\n",
      "\tTraining Acc: 93.5375191424196 Test Acc: 83.18706697459584\n",
      "\tTime Taken: 0.6840231498082479 minutes\n",
      "Epoch [18/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 50.874253302812576 Test Loss: 17.26690022647381\n",
      "\tTraining Acc: 93.0781010719755 Test Acc: 89.22247882986913\n",
      "\tTime Taken: 0.6837843497594197 minutes\n",
      "Epoch [19/35]\t\n",
      "\tTraining Loss: 48.43908612430096 Test Loss: 38.53805163502693\n",
      "\tTraining Acc: 93.6416539050536 Test Acc: 78.29099307159353\n",
      "\tTime Taken: 0.6837560613950093 minutes\n",
      "Epoch [20/35]\t\n",
      "\tTraining Loss: 57.31113436818123 Test Loss: 19.59388054907322\n",
      "\tTraining Acc: 92.5635528330781 Test Acc: 87.79060816012317\n",
      "\tTime Taken: 0.6846242388089497 minutes\n",
      "Epoch [21/35]\t\n",
      "\tTraining Loss: 48.84758139401674 Test Loss: 18.0764222741127\n",
      "\tTraining Acc: 93.4885145482389 Test Acc: 89.0685142417244\n",
      "\tTime Taken: 0.6805654088656108 minutes\n",
      "Epoch [22/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 55.74495047330856 Test Loss: 14.452340371906757\n",
      "\tTraining Acc: 92.59724349157733 Test Acc: 90.79291762894535\n",
      "\tTime Taken: 0.6832866986592611 minutes\n",
      "Epoch [23/35]\t\n",
      "\tTraining Loss: 45.360329665243626 Test Loss: 22.91645184159279\n",
      "\tTraining Acc: 93.98774885145482 Test Acc: 86.55889145496536\n",
      "\tTime Taken: 0.6818174322446188 minutes\n",
      "Epoch [24/35]\t\n",
      "\tTraining Loss: 53.319846369326115 Test Loss: 27.465581819415092\n",
      "\tTraining Acc: 92.8698315467075 Test Acc: 84.63433410315628\n",
      "\tTime Taken: 0.6838329354921977 minutes\n",
      "Epoch [25/35]\t\n",
      "\tTraining Loss: 41.55990144237876 Test Loss: 18.4561348259449\n",
      "\tTraining Acc: 94.59111791730474 Test Acc: 87.97536566589685\n",
      "\tTime Taken: 0.6809713125228882 minutes\n",
      "Epoch [26/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 46.666586223989725 Test Loss: 14.540834091603756\n",
      "\tTraining Acc: 93.739663093415 Test Acc: 90.83910700538875\n",
      "\tTime Taken: 0.6868738214174906 minutes\n",
      "Epoch [27/35]\t\n",
      "\tTraining Loss: 40.206135377287865 Test Loss: 25.12369628250599\n",
      "\tTraining Acc: 94.55742725880552 Test Acc: 85.41955350269438\n",
      "\tTime Taken: 0.6841012835502625 minutes\n",
      "Epoch [28/35]\t\n",
      "\tTraining Loss: 47.81947027519345 Test Loss: 22.358460679650307\n",
      "\tTraining Acc: 93.82542113323125 Test Acc: 86.91301000769823\n",
      "\tTime Taken: 0.6830517331759135 minutes\n",
      "Epoch [29/35]\t\n",
      "\tTraining Loss: 55.82298643141985 Test Loss: 19.63616482913494\n",
      "\tTraining Acc: 92.47166921898928 Test Acc: 87.85219399538106\n",
      "\tTime Taken: 0.6816526254018148 minutes\n",
      "Epoch [30/35]\t\n",
      "\tTraining Loss: 43.302049558609724 Test Loss: 22.697446554899216\n",
      "\tTraining Acc: 94.24502297090352 Test Acc: 86.46651270207852\n",
      "\tTime Taken: 0.6837279478708903 minutes\n",
      "Epoch [31/35]\t\n",
      "\tTraining Loss: 38.95458411052823 Test Loss: 16.911164358258247\n",
      "\tTraining Acc: 94.74732006125575 Test Acc: 89.76135488837568\n",
      "\tTime Taken: 0.6826825181643168 minutes\n",
      "Epoch [32/35]\t\n",
      "\tTraining Loss: 40.71949769183993 Test Loss: 43.70379948616028\n",
      "\tTraining Acc: 94.54517611026034 Test Acc: 78.04464973056197\n",
      "\tTime Taken: 0.6821276823679606 minutes\n",
      "Epoch [33/35]\t\n",
      "\tTraining Loss: 54.776527769863605 Test Loss: 16.868715301156044\n",
      "\tTraining Acc: 92.70750382848392 Test Acc: 89.09930715935334\n",
      "\tTime Taken: 0.6841514070828756 minutes\n",
      "Epoch [34/35]\t\n",
      "\tTraining Loss: 39.21557775884867 Test Loss: 24.72130073606968\n",
      "\tTraining Acc: 94.75344563552834 Test Acc: 86.05080831408776\n",
      "\tTime Taken: 0.6843663295110066 minutes\n",
      "Epoch [35/35]\t\n",
      "\tTraining Loss: 43.551664002239704 Test Loss: 19.20875471830368\n",
      "\tTraining Acc: 94.2113323124043 Test Acc: 88.06774441878368\n",
      "\tTime Taken: 0.6846081455548604 minutes\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "        stime = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "        test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "        print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "        time_taken = (time.time()-stime)/60\n",
    "        print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # loading the best model checkpoint from training before testing\n",
    "    out = os.path.join('./saved_models/', save_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "test_dataset = SkipLabelDataset(test_dataset, TARGET_CLASS)\n",
    "if DATASET == \"badnets\":\n",
    "    full_poisoned_test_dataset = BadNetsDataset(test_dataset, TARGET_CLASS, \"triggers/trigger_10.png\", seed=1, transform=transform_poison, poisoning_rate=1.0, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    full_poisoned_test_dataset = WaNetDataset(test_dataset, TARGET_CLASS, seed=1, transform=transform_poison, poisoning_rate=1.0, noise_rate=0.0, return_original_label=False)\n",
    "elif DATASET == \"sig\":\n",
    "    full_poisoned_test_dataset = SIGDataset(test_dataset, TARGET_CLASS, 20, 6, seed=1, transform=transform_poison, return_original_label=False, attack_test=True)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "\n",
    "\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 71.44\n",
      "Attack Success Rate (ASR): 8.822222222222223\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No attack\n",
    "\n",
    "    Clean Accuracy (C-Acc): 90.49\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "BadNets\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.88\n",
    "    Attack Success Rate (ASR): 99.66\n",
    "\n",
    "BadNets \\\n",
    "kNN \n",
    "\n",
    "    Clean Accuracy (C-Acc): 80.82\n",
    "    Attack Success Rate (ASR): 1.2\n",
    "\n",
    "BadNets \\\n",
    "Energy\n",
    "\n",
    "    Clean Accuracy (C-Acc): 71.44\n",
    "    Attack Success Rate (ASR): 8.82\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "WaNet\n",
    "\n",
    "    Clean Accuracy (C-Acc): 80.93\n",
    "    Attack Success Rate (ASR): 79.58\n",
    "\n",
    "WaNet \\\n",
    "kNN\n",
    "\n",
    "    Clean Accuracy (C-Acc): 74.84\n",
    "    Attack Success Rate (ASR): 29.07\n",
    "\n",
    "WaNet \\\n",
    "Energy\n",
    "\n",
    "    Clean Accuracy (C-Acc): 70.97\n",
    "    Attack Success Rate (ASR): 6.28\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "SIG\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.38\n",
    "    Attack Success Rate (ASR): 99.5\n",
    "\n",
    "-----------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
