{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from datasets import prepare_poison_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_CHECKPOINT = False\n",
    "CHECKPOINT = \"\"\n",
    "DATASET = \"wanet\"  \n",
    "\n",
    "TRAIN = True\n",
    "CLEANSE = True\n",
    "CLEANSED_LABELS_NAME = f\"{DATASET}-(tsne+knn+kmeans)\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleansedDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, poison_dataset: VisionDataset, cleansed_labels_name: str, transforms: torch.nn.Module = None):\n",
    "        with open(f\"./cleansed_labels/{cleansed_labels_name}.pkl\", 'rb') as f:\n",
    "            predicted_poison = pickle.load(f)\n",
    "\n",
    "        self.data = [poison_dataset[i][0] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.labels = [poison_dataset[i][1] for i in range(len(poison_dataset)) if not predicted_poison[i]]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transforms:\n",
    "            item = self.transforms(item)\n",
    "\n",
    "        return item, label\n",
    "\n",
    "\n",
    "class SkipLabelDataset(VisionDataset):\n",
    "\n",
    "    def __init__(self, original_dataset: VisionDataset, skip_class: int):\n",
    "        self.return_as_pil = type(original_dataset[0][0]) is Image.Image\n",
    "\n",
    "        targets = np.array(original_dataset.targets)\n",
    "        self.data = original_dataset.data[targets != skip_class]\n",
    "        self.targets = targets[targets != skip_class].tolist()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.return_as_pil:\n",
    "            data = Image.fromarray(data)\n",
    "        \n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# TODO - setup \"clean\" dataset?\n",
    "# transform_train_clean = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "# ])\n",
    "\n",
    "# transform_test_clean = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "# ])\n",
    "\n",
    "train_poison_dataset, _, target_class, train_dataset = prepare_poison_dataset(DATASET, train=True, transform=transform_train, return_original_label=False)\n",
    "if CLEANSE:\n",
    "    train_poison_dataset = CleansedDataset(train_poison_dataset, CLEANSED_LABELS_NAME + \"-train\")\n",
    "trainloader = DataLoader(train_poison_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "test_poison_dataset, _, _, test_dataset = prepare_poison_dataset(DATASET, train=False, transform=transform_test, return_original_label=False)\n",
    "if CLEANSE:\n",
    "    test_poison_dataset = CleansedDataset(test_poison_dataset, CLEANSED_LABELS_NAME + \"-test\")\n",
    "testloader = DataLoader(test_poison_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Resnet-18 classifier on the cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, name):\n",
    "    out = os.path.join('./saved_models/', name)\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch\n",
    "                }, out)\n",
    "\n",
    "    print(f\"\\tSaved model, optimizer, scheduler and epoch info to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "epochs = 35\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    out = os.path.join('./saved_models/', CHECKPOINT)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    print(\"Loaded checkpoint\")\n",
    "    print(f\"{start_epoch = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "save_name = \"NEW-Resnet-18.pt\"\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, dataloader, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if len(targets.shape)>1:\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    return train_loss, acc\n",
    "\n",
    "def test(epoch, model, dataloader, criterion, optimizer, save=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if len(targets.shape)>1:\n",
    "                targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        if save: save_model(model, optimizer, scheduler, epoch, save_name)\n",
    "        best_acc = acc\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 495.92829418182373 Test Loss: 78.31521475315094\n",
      "\tTraining Acc: 30.759593828300144 Test Acc: 40.856911883589326\n",
      "\tTime Taken: 0.3500399708747864 minutes\n",
      "Epoch [2/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 346.327880859375 Test Loss: 60.558019518852234\n",
      "\tTraining Acc: 46.831728867202955 Test Acc: 54.71301535974131\n",
      "\tTime Taken: 0.33948331673940024 minutes\n",
      "Epoch [3/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 281.5254981517792 Test Loss: 59.28797709941864\n",
      "\tTraining Acc: 57.055255176051695 Test Acc: 58.7712206952304\n",
      "\tTime Taken: 0.3404612501462301 minutes\n",
      "Epoch [4/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 224.52453923225403 Test Loss: 41.895970702171326\n",
      "\tTraining Acc: 66.41500725306607 Test Acc: 69.50687146321746\n",
      "\tTime Taken: 0.3407525102297465 minutes\n",
      "Epoch [5/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 181.06241416931152 Test Loss: 40.63791221380234\n",
      "\tTraining Acc: 73.12079651852828 Test Acc: 71.96443007275667\n",
      "\tTime Taken: 0.34123053550720217 minutes\n",
      "Epoch [6/35]\t\n",
      "\tTraining Loss: 153.86357176303864 Test Loss: 41.8772959113121\n",
      "\tTraining Acc: 77.30449690096268 Test Acc: 71.49555375909458\n",
      "\tTime Taken: 0.33984996875127155 minutes\n",
      "Epoch [7/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 127.03498211503029 Test Loss: 43.17287826538086\n",
      "\tTraining Acc: 81.13213767638138 Test Acc: 71.98059822150364\n",
      "\tTime Taken: 0.3414972901344299 minutes\n",
      "Epoch [8/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 107.29806533455849 Test Loss: 32.48260435461998\n",
      "\tTraining Acc: 84.2146907556376 Test Acc: 78.75505254648343\n",
      "\tTime Taken: 0.3417666633923849 minutes\n",
      "Epoch [9/35]\t\n",
      "\tTraining Loss: 88.98010021448135 Test Loss: 36.95633941888809\n",
      "\tTraining Acc: 86.62468679941975 Test Acc: 75.92562651576395\n",
      "\tTime Taken: 0.3399999817212423 minutes\n",
      "Epoch [10/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 74.15817065536976 Test Loss: 30.52493667602539\n",
      "\tTraining Acc: 89.10391665567717 Test Acc: 80.95392077607114\n",
      "\tTime Taken: 0.34171673059463503 minutes\n",
      "Epoch [11/35]\t\n",
      "\tTraining Loss: 62.79342069476843 Test Loss: 31.21401345729828\n",
      "\tTraining Acc: 90.76552815508374 Test Acc: 80.29102667744543\n",
      "\tTime Taken: 0.3399666309356689 minutes\n",
      "Epoch [12/35]\t\n",
      "\tTraining Loss: 51.287404112517834 Test Loss: 31.10808539390564\n",
      "\tTraining Acc: 92.56560727944085 Test Acc: 80.88924818108326\n",
      "\tTime Taken: 0.3398333589235942 minutes\n",
      "Epoch [13/35]\t\n",
      "\tTraining Loss: 48.64680963754654 Test Loss: 37.7984773516655\n",
      "\tTraining Acc: 92.96452591322695 Test Acc: 78.39935327405013\n",
      "\tTime Taken: 0.3399856487909953 minutes\n",
      "Epoch [14/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 41.249087266623974 Test Loss: 33.61673730611801\n",
      "\tTraining Acc: 93.99643940392984 Test Acc: 81.00242522231204\n",
      "\tTime Taken: 0.34182083209355674 minutes\n",
      "Epoch [15/35]\t\n",
      "\tTraining Loss: 38.44105672836304 Test Loss: 36.56756234169006\n",
      "\tTraining Acc: 94.52063826981406 Test Acc: 79.90299110751819\n",
      "\tTime Taken: 0.3401299357414246 minutes\n",
      "Epoch [16/35]\t\n",
      "\tTraining Loss: 34.51268167048693 Test Loss: 38.453451454639435\n",
      "\tTraining Acc: 94.9360411446657 Test Acc: 79.1269199676637\n",
      "\tTime Taken: 0.3400161663691203 minutes\n",
      "Epoch [17/35]\t\n",
      "\tTraining Loss: 35.52992891520262 Test Loss: 36.378985941410065\n",
      "\tTraining Acc: 94.6986680733219 Test Acc: 80.51738075990299\n",
      "\tTime Taken: 0.34008333683013914 minutes\n",
      "Epoch [18/35]\t\n",
      "\tTraining Loss: 32.49303612858057 Test Loss: 41.73003926873207\n",
      "\tTraining Acc: 95.28880390346828 Test Acc: 79.0784155214228\n",
      "\tTime Taken: 0.33994342883427936 minutes\n",
      "Epoch [19/35]\t\n",
      "\tTraining Loss: 29.164028894156218 Test Loss: 47.48675549030304\n",
      "\tTraining Acc: 95.78662798364763 Test Acc: 78.17299919159257\n",
      "\tTime Taken: 0.33985056082407633 minutes\n",
      "Epoch [20/35]\t\n",
      "\tTraining Loss: 30.466564513742924 Test Loss: 41.93439158797264\n",
      "\tTraining Acc: 95.52947382302519 Test Acc: 77.34842360549717\n",
      "\tTime Taken: 0.3399666031201681 minutes\n",
      "Epoch [21/35]\t\n",
      "\tSaved model, optimizer, scheduler and epoch info to ./saved_models/NEW-Resnet-18.pt\n",
      "\tTraining Loss: 29.858394149690866 Test Loss: 27.01072785258293\n",
      "\tTraining Acc: 95.67123829618885 Test Acc: 84.93128536782538\n",
      "\tTime Taken: 0.3418328324953715 minutes\n",
      "Epoch [22/35]\t\n",
      "\tTraining Loss: 25.523827208206058 Test Loss: 36.33805751800537\n",
      "\tTraining Acc: 96.43281023341686 Test Acc: 81.71382376717865\n",
      "\tTime Taken: 0.3397965510686239 minutes\n",
      "Epoch [23/35]\t\n",
      "\tTraining Loss: 28.371235601603985 Test Loss: 30.132628738880157\n",
      "\tTraining Acc: 95.85586179612291 Test Acc: 83.18512530315279\n",
      "\tTime Taken: 0.3399578928947449 minutes\n",
      "Epoch [24/35]\t\n",
      "\tTraining Loss: 27.639301389455795 Test Loss: 44.18243333697319\n",
      "\tTraining Acc: 95.88553343004088 Test Acc: 78.73888439773646\n",
      "\tTime Taken: 0.3399122675259908 minutes\n",
      "Epoch [25/35]\t\n",
      "\tTraining Loss: 26.07241313904524 Test Loss: 36.85620152950287\n",
      "\tTraining Acc: 96.254780429909 Test Acc: 81.51980598221503\n",
      "\tTime Taken: 0.33980000019073486 minutes\n",
      "Epoch [26/35]\t\n",
      "\tTraining Loss: 23.512044090777636 Test Loss: 40.71756225824356\n",
      "\tTraining Acc: 96.62732427799024 Test Acc: 80.90541632983023\n",
      "\tTime Taken: 0.33983641465504966 minutes\n",
      "Epoch [27/35]\t\n",
      "\tTraining Loss: 22.23111618682742 Test Loss: 37.40494182705879\n",
      "\tTraining Acc: 96.79216668864565 Test Acc: 80.97008892481811\n",
      "\tTime Taken: 0.33992170890172324 minutes\n",
      "Epoch [28/35]\t\n",
      "\tTraining Loss: 25.898222025483847 Test Loss: 34.24041390419006\n",
      "\tTraining Acc: 96.26796782276143 Test Acc: 82.29587712206953\n",
      "\tTime Taken: 0.3399505893389384 minutes\n",
      "Epoch [29/35]\t\n",
      "\tTraining Loss: 22.617406928911805 Test Loss: 54.19404464960098\n",
      "\tTraining Acc: 96.68337069761309 Test Acc: 75.5375909458367\n",
      "\tTime Taken: 0.33988333543141686 minutes\n",
      "Epoch [30/35]\t\n",
      "\tTraining Loss: 26.586370626464486 Test Loss: 34.28400573134422\n",
      "\tTraining Acc: 96.09982856389291 Test Acc: 80.14551333872272\n",
      "\tTime Taken: 0.3397834062576294 minutes\n",
      "Epoch [31/35]\t\n",
      "\tTraining Loss: 22.106178537011147 Test Loss: 37.97695517539978\n",
      "\tTraining Acc: 96.87129104576026 Test Acc: 80.95392077607114\n",
      "\tTime Taken: 0.3398332675298055 minutes\n",
      "Epoch [32/35]\t\n",
      "\tTraining Loss: 21.75952952541411 Test Loss: 45.67918914556503\n",
      "\tTraining Acc: 96.94711855466174 Test Acc: 78.94907033144705\n",
      "\tTime Taken: 0.34394999742507937 minutes\n",
      "Epoch [33/35]\t\n",
      "\tTraining Loss: 21.812951501458883 Test Loss: 44.10298818349838\n",
      "\tTraining Acc: 96.9701964921535 Test Acc: 78.70654810024253\n",
      "\tTime Taken: 0.3651666283607483 minutes\n",
      "Epoch [34/35]\t\n",
      "\tTraining Loss: 22.499078571796417 Test Loss: 36.65424990653992\n",
      "\tTraining Acc: 96.86140050112093 Test Acc: 82.44139046079223\n",
      "\tTime Taken: 0.35837470293045043 minutes\n",
      "Epoch [35/35]\t\n",
      "\tTraining Loss: 21.500716308131814 Test Loss: 35.449563682079315\n",
      "\tTraining Acc: 96.91085322431755 Test Acc: 81.05092966855295\n",
      "\tTime Taken: 0.3549226959546407 minutes\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "        stime = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, trainloader, criterion)\n",
    "        test_loss, test_acc = test(epoch, model, testloader, criterion, optimizer, save=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tTraining Loss: {train_loss} Test Loss: {test_loss}\")\n",
    "        print(f\"\\tTraining Acc: {train_acc} Test Acc: {test_acc}\")\n",
    "        time_taken = (time.time()-stime)/60\n",
    "        print(f\"\\tTime Taken: {time_taken} minutes\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # loading the best model checkpoint from training before testing\n",
    "    out = os.path.join('./saved_models/', save_name)\n",
    "    checkpoint = torch.load(out, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_poison = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "clean_test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False, transform=transform_clean)\n",
    "testloader_clean = DataLoader(clean_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, c_acc = test(0, model, testloader_clean, criterion, optimizer, save=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='C:/Datasets', train=False, download=False)\n",
    "test_dataset = SkipLabelDataset(test_dataset, target_class)\n",
    "if DATASET == \"badnets\":\n",
    "    full_poisoned_test_dataset = BadNetsDataset(test_dataset, target_class, \"triggers/trigger_10.png\", seed=1, transform=transform_poison, poisoning_rate=1.0, return_original_label=False)\n",
    "elif DATASET == \"wanet\":\n",
    "    full_poisoned_test_dataset = WaNetDataset(test_dataset, target_class, seed=1, transform=transform_poison, poisoning_rate=1.0, noise_rate=0.0, return_original_label=False)\n",
    "elif DATASET == \"sig\":\n",
    "    full_poisoned_test_dataset = SIGDataset(test_dataset, target_class, 20, 6, seed=1, transform=transform_poison, return_original_label=False, attack_test=True)\n",
    "else:\n",
    "    raise Exception(\"Invalid dataset\")\n",
    "\n",
    "\n",
    "testloader_full_poison = DataLoader(full_poisoned_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "_, asr = test(0, model, testloader_full_poison, criterion, optimizer, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy (C-Acc): 72.78\n",
      "Attack Success Rate (ASR): 7.355555555555555\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clean Accuracy (C-Acc): {c_acc}\")\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No attack\n",
    "\n",
    "    Clean Accuracy (C-Acc): 90.49\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "BadNets\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.88\n",
    "    Attack Success Rate (ASR): 99.66\n",
    "\n",
    "BadNets \\\n",
    "kNN \n",
    "\n",
    "    Clean Accuracy (C-Acc): 80.82\n",
    "    Attack Success Rate (ASR): 1.2\n",
    "\n",
    "BadNets \\\n",
    "Energy\n",
    "\n",
    "    Clean Accuracy (C-Acc): 71.44\n",
    "    Attack Success Rate (ASR): 8.82\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "WaNet\n",
    "\n",
    "    Clean Accuracy (C-Acc): 80.93\n",
    "    Attack Success Rate (ASR): 79.58\n",
    "\n",
    "WaNet \\\n",
    "kNN\n",
    "\n",
    "    Clean Accuracy (C-Acc): 74.84\n",
    "    Attack Success Rate (ASR): 29.07\n",
    "\n",
    "WaNet \\\n",
    "Energy\n",
    "\n",
    "    Clean Accuracy (C-Acc): 70.97\n",
    "    Attack Success Rate (ASR): 6.28\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "SIG\n",
    "\n",
    "    Clean Accuracy (C-Acc): 85.38\n",
    "    Attack Success Rate (ASR): 99.5\n",
    "\n",
    "-----------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
